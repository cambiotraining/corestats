{
  "hash": "37a01926bfa37d7014f6ff5a41a9256d",
  "result": {
    "markdown": "---\ntitle: \"Multiple linear regression\"\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n::: callout-tip\n## Learning outcomes\n\n**Questions**\n\n- How do I use the linear model framework with three predictor variables?\n\n**Objectives**\n\n- Be able to expand the linear model framework with three predictor variables\n- Define the equation for the line of best fit for each categorical variable\n- Be able to construct and analyse any possible combination of predictor variables in the data\n:::\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n### Libraries\n### Functions\n\n## R\n\n### Libraries\n### Functions\n\n## Python\n\n### Libraries\n### Functions\n:::\n:::\n\n## Purpose and aim\nRevisiting the linear model framework and expanding to systems with three predictor variables.\n\n## Data and hypotheses\n\nThe data set we'll be using is located in `data/CS5-pm2_5.csv`. It contains data on air pollution levels measured in London, in 2019. It also contains several meteorological measurements. Each variable was recorded on a daily basis.\n\nNote: some of the variables are based on simulations.\n\nIt contains the following variables:\n\n| variable | explanation |\n|:--|:--|\n| `avg_temp`| average daily temperature ($^\\circ C$)|\n| `date`| date of record|\n| `location`| location in London (`inner` or `outer`) |\n| `pm2_5`| concentration of PM2.5 ($\\mu g / m^3$)|\n| `rain_mm`| daily rainfall in mm (same across both locations)|\n| `wind_m_s`| wind speed in $m/s$|\n\n## Summarise and visualise\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\nLet's first load the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npm2_5 <- read_csv(\"data/CS5-pm2_5.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 730 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): date, location\ndbl (4): avg_temp, pm2_5, rain_mm, wind_m_s\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nhead(pm2_5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 6\n  avg_temp date       location pm2_5 rain_mm wind_m_s\n     <dbl> <chr>      <chr>    <dbl>   <dbl>    <dbl>\n1      4.5 01/01/2019 inner     17.1     2.3     3.87\n2      4.9 01/01/2019 outer     10.8     2.3     5.84\n3      4.3 02/01/2019 inner     14.9     2.3     3.76\n4      4.8 02/01/2019 outer     11.4     2.3     6   \n5      4   03/01/2019 inner     18.5     1.4     2.13\n6      4.5 03/01/2019 outer     15.0     1.4     2.57\n```\n:::\n:::\n\n\nIt's the `pm2_5` response variable we're interested in here. Let's start by checking if there might be a difference between PM2.5 level in inner and outer London:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pm2_5, aes(x = location, y = pm2_5)) +\n    geom_boxplot() +\n    geom_jitter(width = 0.1, alpha = 0.7)\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nI've added the (jittered) data to the plot, with some transparency (`alpha = 0.7`). It's always good to look at the actual data and not just summary statistics (which is what the box plot is).\n\nThere seems to be quite a difference between the PM2.5 levels in the two London areas, with the levels in inner London being markedly higher. I'm not surprised by this! So when we do our statistical testing, I would expect to find a clear difference between the locations.\n\nApart from the location, there are quite a few numerical descriptor variables. We could plot them one-by-one, but that's a bit tedious. So instead we use the `pairs()` function again. This only works on numerical data, so we select all the columns that are numeric with `select_if(is.numeric)`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npm2_5 %>% \n    select_if(is.numeric) %>% \n    pairs(lower.panel = NULL)\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nWe can see that there is not much of a correlation between `pm2_5` and `avg_temp` or `rain_mm`, whereas there might be something going on in relation to `wind_m_s`.\n\nOther notable things include that rainfall seems completely independent of wind speed (rain fall seems pretty constant). Nor does the average temperature seem in any way related to wind speed (it looks like a random collection of data points!).\n\nWe can visualise the relationship between `pm2_5` and `wind_m_s` in a bit more detail, by plotting the data against each other and colouring by `location`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pm2_5, aes(x = wind_m_s, y = pm2_5,\n                  colour = location)) +\n    geom_point()\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nThis seems to show that there might be some linear relationship between PM2.5 levels and wind speed.\n\nAnother way of looking at this would be to create a correlation matrix, like we did before in the [correlations chapter](#correlation-coefficients):\n\n\n::: {.cell}\n\n```{.r .cell-code}\npm2_5 %>% \n    select_if(is.numeric) %>% \n    cor()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            avg_temp       pm2_5     rain_mm    wind_m_s\navg_temp  1.00000000  0.03349457  0.03149221 -0.01107855\npm2_5     0.03349457  1.00000000 -0.02184951 -0.41733945\nrain_mm   0.03149221 -0.02184951  1.00000000  0.04882097\nwind_m_s -0.01107855 -0.41733945  0.04882097  1.00000000\n```\n:::\n:::\n\n\nThis confirms what we saw in the plots, there aren't any very strong correlations between the different (numerical) variables, apart from a negative correlation between `pm2_5` and `wind_m_s`, which has a Pearson's r of $r$ = -0.42.\n\n## R\n\nLet's first load the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npm2_5_r <- read.csv(\"data/CS5-pm2_5.csv\")\n\nhead(pm2_5_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  avg_temp       date location  pm2_5 rain_mm wind_m_s\n1      4.5 01/01/2019    inner 17.126     2.3     3.87\n2      4.9 01/01/2019    outer 10.821     2.3     5.84\n3      4.3 02/01/2019    inner 14.884     2.3     3.76\n4      4.8 02/01/2019    outer 11.416     2.3     6.00\n5      4.0 03/01/2019    inner 18.471     1.4     2.13\n6      4.5 03/01/2019    outer 15.008     1.4     2.57\n```\n:::\n:::\n\n\nIt's the `pm2_5` response variable we're interested in here. Let's start by checking if there might be a difference between PM2.5 level in inner and outer London:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(pm2_5 ~ location, col = \"white\", data = pm2_5_r)\n\nstripchart(pm2_5 ~ location,\n           data = pm2_5_r,\n           method = \"jitter\",\n           pch = 19,\n           col = alpha(\"black\", 0.4),\n           vertical = TRUE,\n           add = TRUE)\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nI've added the (jittered) data to the plot. To do this I've used the `stripchart()` function and added some transparency (`col = alpha(\"black\", 0.4)`). It's always good to look at the actual data and not just summary statistics (which is what the box plot is).\n\nThere seems to be quite a difference between the PM2.5 levels in the two London areas, with the levels in inner London being markedly higher. I'm not surprised by this! So when we do our statistical testing, I would expect to find a clear difference between the locations.\n\nApart from the location, there are quite a few numerical descriptor variables. We could plot them one-by-one, but that's a bit tedious. So instead we use the `pairs()` function again. This only works on numerical data, so we select all the columns that are numeric with the base R `Filter()` function (not to be confused with the `dplyr::filter()` function). Note that there are many different ways to select numeric-only columns and a quick Google search will lead you to [Stackoverflow](https://stackoverflow.com/questions/5863097/selecting-only-numeric-columns-from-a-data-frame).\n\nWe could save the output of the `Filter()` operation into a new variable and use that with the `pairs()` function. However, since version 4.1 R has had a native pipe, using the `|>` symbol. There are some  [differences](https://towardsdatascience.com/understanding-the-native-r-pipe-98dea6d8b61b) between tidyverse's `%>%` and base R's `|>`, but we won't delve into this here.\n\nSuffice to say, we can do the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nFilter(is.numeric, pm2_5_r) |> pairs(lower.panel = NULL)\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nWe can see that there is not much of a correlation between `pm2_5` and `avg_temp` or `rain_mm`, whereas there might be something going on in relation to `wind_m_s`.\n\nOther notable things include that rainfall seems completely independent of wind speed (rain fall seems pretty constant). Nor does the average temperature seem in any way related to wind speed (it looks like a random collection of data points!).\n\nWe can visualise the relationship between `pm2_5` and `wind_m_s` in a bit more detail, by plotting the data against each other and colouring by `location`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(pm2_5 ~ wind_m_s,\n     col = factor(location),\n     data = pm2_5_r)\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThis seems to show that there might be some linear relationship between PM2.5 levels and wind speed.\n\nAnother way of looking at this would be to create a correlation matrix, like we did before in the [correlations chapter](materials/cs3_practical_correlations.qmd#correlation-coefficients). Again, this only works on numerical values, so we get all the numerical columns and send this to the `cor()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nFilter(is.numeric, pm2_5_r) |> cor()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            avg_temp       pm2_5     rain_mm    wind_m_s\navg_temp  1.00000000  0.03349457  0.03149221 -0.01107855\npm2_5     0.03349457  1.00000000 -0.02184951 -0.41733945\nrain_mm   0.03149221 -0.02184951  1.00000000  0.04882097\nwind_m_s -0.01107855 -0.41733945  0.04882097  1.00000000\n```\n:::\n:::\n\n\nThis confirms what we saw in the plots, there aren't any very strong correlations between the different (numerical) variables, apart from a negative correlation between `pm2_5` and `wind_m_s`, which has a Pearson's r of $r$ = -0.42.\n\n## Python\n\nLet's first load the data:\n\n\n::: {.cell}\n\n```{.python .cell-code}\npm2_5_py = pd.read_csv(\"data/CS5-pm2_5.csv\")\n\npm2_5_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   avg_temp        date location   pm2_5  rain_mm  wind_m_s\n0       4.5  01/01/2019    inner  17.126      2.3      3.87\n1       4.9  01/01/2019    outer  10.821      2.3      5.84\n2       4.3  02/01/2019    inner  14.884      2.3      3.76\n3       4.8  02/01/2019    outer  11.416      2.3      6.00\n4       4.0  03/01/2019    inner  18.471      1.4      2.13\n```\n:::\n:::\n\n\nIt's the `pm2_5` response variable we're interested in here. Let's start by checking if there might be a difference between PM2.5 level in inner and outer London:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(pm2_5_py, aes(x = \"location\", y = \"pm2_5\")) +\n    geom_boxplot() +\n    geom_jitter(width = 0.1, alpha = 0.7))\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-14-1.png){width=614}\n:::\n:::\n\n\nI've added the (jittered) data to the plot, with some transparency (`alpha = 0.7`). It's always good to look at the actual data and not just summary statistics (which is what the box plot is).\n\nThere seems to be quite a difference between the PM2.5 levels in the two London areas, with the levels in inner London being markedly higher. I'm not surprised by this! So when we do our statistical testing, I would expect to find a clear difference between the locations.\n\nApart from the location, there are quite a few numerical descriptor variables. At this point I should probably bite the bullet and install `seaborn`, so I can use the [pairplot()](https://seaborn.pydata.org/generated/seaborn.pairplot.html) function.\n\nBut I'm not going to ;-)\n\nI'll just tell you that there is not much of a correlation between `pm2_5` and `avg_temp` or `rain_mm`, whereas there might be something going on in relation to `wind_m_s`. So I plot that instead and colour it by location:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(pm2_5_py, aes(x = \"wind_m_s\",\n                      y = \"pm2_5\",\n                      colour = \"location\")) +\n     geom_point())\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-15-3.png){width=614}\n:::\n:::\n\n\nThis seems to show that there might be some linear relationship between PM2.5 levels and wind speed.\n\nIf I would plot all the other variables against each other, then I would spot that rainfall seems completely independent of wind speed (rain fall seems pretty constant). Nor does the average temperature seem in any way related to wind speed (it looks like a random collection of data points!). You can check this yourself!\n\nAnother way of looking at this would be to create a correlation matrix, like we did before in the [correlations chapter](materials/cs3_practical_correlations.qmd#correlation-coefficients):\n\n\n::: {.cell}\n\n```{.python .cell-code}\npm2_5_py.corr()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          avg_temp     pm2_5   rain_mm  wind_m_s\navg_temp  1.000000  0.033495  0.031492 -0.011079\npm2_5     0.033495  1.000000 -0.021850 -0.417339\nrain_mm   0.031492 -0.021850  1.000000  0.048821\nwind_m_s -0.011079 -0.417339  0.048821  1.000000\n```\n:::\n:::\n\n\nThis confirms what we saw in the plots, there aren't any very strong correlations between the different (numerical) variables, apart from a negative correlation between `pm2_5` and `wind_m_s`, which has a Pearson's r of $r$ = -0.42.\n:::\n\n## Implement and interpret the test\n\nFrom our initial observations we derived that there might be some relationship between PM2.5 levels and wind speed. We also noticed that this is likely to be different between inner and outer London.\n\nIf we would want to test for _every_ variable and interaction, then we would end up with a rather huge model, which would even include 3-way and a 4-way interaction! To illustrate the point that the process of model testing applies to as many variables as you like, we're adding the `avg_temp` and `rain_mm` variables to our model.\n\nSo in this case we create a model that takes into account all of the main effects (`avg_temp`, `location`, `rain_mm`, `wind_m_s`). We also include a potential two-way interaction (`location:wind_m_s`). The two-way interaction may be of interest since the PM2.5 levels in response to wind speed seem to differ between the two locations.\n\nOur model is then as follows:\n\n`pm2_5 ~ avg_temp, location + rain_mm + wind_m_s + wind_m_s:location`\n\nSo let's define and explore it!\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\nWe write the model as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_full <- lm(pm2_5 ~ avg_temp + location +\n                            rain_mm + wind_m_s +\n                            wind_m_s:location,\n                    data = pm2_5)\n```\n:::\n\n\nThis will give us quite a few coefficients, so instead of just calling the `lm` object, I'm restructuring the output using the `tidy()` function from the `broom` package. It's installed with `tidyverse` but you have to load it separately using `library(broom)`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_full %>%\n    tidy() %>% \n    select(term, estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  term                   estimate\n  <chr>                     <dbl>\n1 (Intercept)             18.2   \n2 avg_temp                 0.0105\n3 locationouter           -2.07  \n4 rain_mm                 -0.0279\n5 wind_m_s                -0.285 \n6 locationouter:wind_m_s  -0.429 \n```\n:::\n:::\n\n\nThe question is, are all of these terms statistically significant? To find out we perform an ANOVA:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lm_pm2_5_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: pm2_5\n                   Df  Sum Sq Mean Sq   F value  Pr(>F)    \navg_temp            1    5.29    5.29    5.0422 0.02504 *  \nlocation            1 3085.37 3085.37 2940.5300 < 2e-16 ***\nrain_mm             1    2.48    2.48    2.3644 0.12457    \nwind_m_s            1  728.13  728.13  693.9481 < 2e-16 ***\nlocation:wind_m_s   1  134.82  134.82  128.4912 < 2e-16 ***\nResiduals         724  759.66    1.05                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nFrom this we can see that the interaction between `location` and `wind_m_s` is statistically significant. Which means that we can't just talk about the effect of `location` or `wind_m_s` on PM2.5 levels, without taking the other variable into account!\n\nThe p-values for the `avg_temp` and `rain_mm` main effects are not significant. This means that they're not contributing much to model's ability to explain our data. This matches what we already saw when we visualised the data.\n\nWhat to do? We'll explore this in more detail in the chapter on model comparisons, but for now the most sensible option would be to redefine the model, but exclude those two variables. Here I have rewritten the model, using the shorthand `*` notation (`pm2_5 ~ location * wind_m_s` is equal to `pm2_5 ~ location + wind_m_s + wind_m_s:location`). I've named it `lm_pm2_5_red` to indicate it is a reduced model (with fewer variables than our original full model):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_red <- lm(pm2_5 ~ location * wind_m_s, data = pm2_5)\n```\n:::\n\n\nLet's look at the new model coefficients:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_red %>%\n    tidy() %>% \n    select(term, estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 2\n  term                   estimate\n  <chr>                     <dbl>\n1 (Intercept)              18.2  \n2 locationouter            -2.06 \n3 wind_m_s                 -0.285\n4 locationouter:wind_m_s   -0.432\n```\n:::\n:::\n\n\nAs we did in the linear regression on grouped data, we end up with two linear equations, one for `inner` London and one for `outer` London.\n\nOur reference group is `inner` (remember, it takes a reference group in alphabetical order and we can see `outer` in the output).\n\nSo we end up with:\n\n$PM2.5_{inner} = 18.24 - 0.29 \\times wind\\_m\\_s$\n\n$PM2.5_{outer} = (18.24 - 2.06) + (-0.29 - 0.43) \\times wind\\_m\\_s$\n\nwhich gives\n\n$PM2.5_{outer} = 16.18 - 0.72 \\times wind\\_m\\_s$\n\n\nWe still need to check the assumptions of the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_red %>% \n    resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nThey all look pretty good, with the only weird thing being a small empty zone of predicted values just under 16. Nothing that is getting me worried though.\n\nIt'd be useful to visualise the model. We can take the model and use the `augment()` function to extract the fitted values (`.fitted`). These are the values for `pm2_5` that the model is predicting. We can then plot these against the `wind_m_s` measurements, colouring by `location`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_red %>% \n    augment() %>% \n    ggplot(aes(x = wind_m_s, y = .fitted, colour = location)) +\n    geom_line(size = 1) +\n    # add the original data\n    geom_point(data = pm2_5, aes(wind_m_s, pm2_5, colour = location))\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nNote: in this case we're actually visualising the model with both the main effects and the interaction between them. This is also the default behaviour for the `lm` method of `geom_smooth()`, so we could have also plotted it with:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = pm2_5, aes(x = wind_m_s, y = pm2_5,\n                         colour = location)) +\n    geom_smooth(method = \"lm\", se = FALSE)\n```\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_full_r <- lm(pm2_5 ~ avg_temp + location +\n                              rain_mm + wind_m_s +\n                              wind_m_s:location,\n                      data = pm2_5_r)\n```\n:::\n\n\nLet's look at the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_full_r\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = pm2_5 ~ avg_temp + location + rain_mm + wind_m_s + \n    wind_m_s:location, data = pm2_5_r)\n\nCoefficients:\n           (Intercept)                avg_temp           locationouter  \n              18.18286                 0.01045                -2.07084  \n               rain_mm                wind_m_s  locationouter:wind_m_s  \n              -0.02788                -0.28545                -0.42945  \n```\n:::\n:::\n\n\nQuite a few coefficients! The question is, are all of these terms statistically significant? To find out we perform an ANOVA:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lm_pm2_5_full_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: pm2_5\n                   Df  Sum Sq Mean Sq   F value  Pr(>F)    \navg_temp            1    5.29    5.29    5.0422 0.02504 *  \nlocation            1 3085.37 3085.37 2940.5300 < 2e-16 ***\nrain_mm             1    2.48    2.48    2.3644 0.12457    \nwind_m_s            1  728.13  728.13  693.9481 < 2e-16 ***\nlocation:wind_m_s   1  134.82  134.82  128.4912 < 2e-16 ***\nResiduals         724  759.66    1.05                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nFrom this we can see that the interaction between `location` and `wind_m_s` is statistically significant. Which means that we can't just talk about the effect of `location` or `wind_m_s` on PM2.5 levels, without taking the other variable into account!\n\nThe p-values for the `avg_temp` and `rain_mm` main effects are not significant. This means that they're not contributing much to model's ability to explain our data. This matches what we already saw when we visualised the data.\n\nWhat to do? We'll explore this in more detail in the chapter on model comparisons, but for now the most sensible option would be to redefine the model, but exclude those two variables. Here I have rewritten the model, using the shorthand `*` notation (`pm2_5 ~ location * wind_m_s` is equal to `pm2_5 ~ location + wind_m_s + wind_m_s:location`). I've named it `lm_pm2_5_red` to indicate it is a reduced model (with fewer variables than our original full model):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_red_r <- lm(pm2_5 ~ location * wind_m_s, data = pm2_5_r)\n```\n:::\n\n\nLet's look at the new model coefficients:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_red_r\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = pm2_5 ~ location * wind_m_s, data = pm2_5_r)\n\nCoefficients:\n           (Intercept)           locationouter                wind_m_s  \n               18.2422                 -2.0597                 -0.2851  \nlocationouter:wind_m_s  \n               -0.4318  \n```\n:::\n:::\n\n\nAs we did in the linear regression on grouped data, we end up with two linear equations, one for `inner` London and one for `outer` London.\n\nOur reference group is `inner` (remember, it takes a reference group in alphabetical order and we can see `outer` in the output).\n\nSo we end up with:\n\n$PM2.5_{inner} = 18.24 - 0.29 \\times wind\\_m\\_s$\n\n$PM2.5_{outer} = (18.24 - 2.06) + (-0.29 - 0.43) \\times wind\\_m\\_s$\n\nwhich gives\n\n$PM2.5_{outer} = 16.18 - 0.72 \\times wind\\_m\\_s$\n\n\nWe still need to check the assumptions of the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2,2))\nplot(lm_pm2_5_red_r)\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\nThey all look pretty good, with the only weird thing being a small empty zone of predicted values just under 16. Nothing that is getting me worried though.\n\nIt'd be useful to visualise the model. To do this, we need to split the data by `location`, create a linear model for each subset and then plot this with the original data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninner_r <- subset(pm2_5_r, location == \"inner\")\nouter_r <- subset(pm2_5_r, location == \"outer\")\n\nlm_inner_red <- lm(pm2_5 ~ wind_m_s, data = inner_r)\nlm_outer_red <- lm(pm2_5 ~ wind_m_s, data = outer_r)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(pm2_5 ~ wind_m_s,\n     col = factor(location),\n     data = pm2_5_r)\n\nabline(lm_inner_red, col = 1)\nabline(lm_outer_red, col = 2)\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n## Python\n\nWe write the model as follows:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula= \"pm2_5 ~ avg_temp + C(location) + rain_mm + wind_m_s + wind_m_s:location\", data = pm2_5_py)\n# and get the fitted parameters of the model\nlm_pm2_5_full_py = model.fit()\n```\n:::\n\n\nThis will give us quite a few coefficients, so instead of just printing the entire summary table, we're extracting the parameters with `.params`:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlm_pm2_5_full_py.params\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercept                     18.182858\nC(location)[T.outer]          -2.070843\navg_temp                       0.010451\nrain_mm                       -0.027880\nwind_m_s                      -0.285450\nwind_m_s:location[T.outer]    -0.429455\ndtype: float64\n```\n:::\n:::\n\n\nThe question is, are all of these terms statistically significant? To find out we perform an ANOVA:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsm.stats.anova_lm(lm_pm2_5_full_py, typ = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                       sum_sq     df           F         PR(>F)\nC(location)        123.803830    1.0  117.991919   1.441933e-25\navg_temp             1.803685    1.0    1.719012   1.902360e-01\nrain_mm              0.350639    1.0    0.334179   5.633886e-01\nwind_m_s           728.129799    1.0  693.948101  8.928636e-108\nwind_m_s:location  134.820234    1.0  128.491164   1.567268e-27\nResidual           759.661960  724.0         NaN            NaN\n```\n:::\n:::\n\n\nFrom this we can see that the interaction between `location` and `wind_m_s` is statistically significant. Which means that we can't just talk about the effect of `location` or `wind_m_s` on PM2.5 levels, without taking the other variable into account!\n\nThe p-values for the `avg_temp` and `rain_mm` main effects are not significant. This means that they're not contributing much to model's ability to explain our data. This matches what we already saw when we visualised the data.\n\nWhat to do? We'll explore this in more detail in the chapter on model comparisons, but for now the most sensible option would be to redefine the model, but exclude those two variables. Here I have rewritten the model, using the shorthand `*` notation (`pm2_5 ~ location * wind_m_s` is equal to `pm2_5 ~ location + wind_m_s + wind_m_s:location`). I've named it `lm_pm2_5_red` to indicate it is a reduced model (with fewer variables than our original full model):\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula= \"pm2_5 ~ C(location) * wind_m_s\", data = pm2_5_py)\n# and get the fitted parameters of the model\nlm_pm2_5_red_py = model.fit()\n```\n:::\n\n\nLet's look at the new model coefficients:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlm_pm2_5_red_py.params\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercept                        18.242210\nC(location)[T.outer]             -2.059747\nwind_m_s                         -0.285088\nC(location)[T.outer]:wind_m_s    -0.431821\ndtype: float64\n```\n:::\n:::\n\n\nAs we did in the linear regression on grouped data, we end up with two linear equations, one for `inner` London and one for `outer` London.\n\nOur reference group is `inner` (remember, it takes a reference group in alphabetical order and we can see `outer` in the output).\n\nSo we end up with:\n\n$PM2.5_{inner} = 18.24 - 0.29 \\times wind\\_m\\_s$\n\n$PM2.5_{outer} = (18.24 - 2.06) + (-0.29 - 0.43) \\times wind\\_m\\_s$\n\nwhich gives\n\n$PM2.5_{outer} = 16.18 - 0.72 \\times wind\\_m\\_s$\n\n\nWe still need to check the assumptions of the model:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndgplots(lm_pm2_5_red_py)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/dgplots/2022_11_15-02:39:53_pm_dgplots.png){width=787}\n:::\n:::\n\n\nThey all look pretty good, with the only weird thing being a small empty zone of predicted values just under 16. Nothing that is getting me worried though.\n\nIt'd be useful to visualise the model. We can take the model and extract the fitted values (`.fittedvalues`). These are the values for `pm2_5` that the model is predicting. We can then plot these against the `wind_m_s` measurements, colouring by `location`. We're also adding the original values to the plot with `geom_point(aes(y = \"pm2_5\"))`:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(pm2_5_py, aes(x = \"wind_m_s\",\n                     y = lm_pm2_5_red_py.fittedvalues,\n                     colour = \"location\")) +\n    geom_line(size = 1) +\n    geom_point(aes(y = \"pm2_5\")))\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-41-1.png){width=614}\n:::\n:::\n\n\nNote: in this case we're actually visualising the model with both the main effects and the interaction between them. This is also the default behaviour for the `lm` method of `geom_smooth()`, so we could have also plotted it with:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(pm2_5_py, aes(x = \"wind_m_s\",\n                      y = \"pm2_5\",\n                      colour = \"location\")) +\n     geom_smooth(method = \"lm\", se = False) +\n     geom_point())\n```\n:::\n\n\n\n:::\n\n## Exploring models\n\nRather than stop here however, we will use the concept of the linear model to its full potential and show that we can construct and analyse any possible combination of predictor variables for this data set. Namely we will consider the following four extra models, where reduce the complexity to the model, step-by-step:\n\n| Model| Description|\n|:- |:- |\n|1. `pm2_5 ~ wind_m_s + location`| An additive model |\n|2. `pm2_5 ~ wind_m_s` | Equivalent to a simple linear regression |\n|3. `pm2_5 ~ location` | Equivalent to a one-way ANOVA |\n|4. `pm2_5 ~ 1` | The null model, where we have no predictors |\n\n### Additive model\n\nTo create the additive model, we drop the interaction term (keep in mind, this is to demonstrate the process - we would normally not do this because the interaction term is significant!).\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\nFirst, we define the model:\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_add <- lm(pm2_5 ~ location + wind_m_s,\n                   data = pm2_5)\n```\n:::\n\n\nWe can visualise this as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_add %>% \n    augment() %>% \n    ggplot(aes(x = wind_m_s, y = .fitted,\n               colour = location)) +\n    geom_line(size = 1) +\n    geom_point(data = pm2_5,\n               aes(x = wind_m_s, y = pm2_5))\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-44-1.png){width=672}\n:::\n:::\n\n\nNext, we extract the coefficient estimates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_add %>%\n    tidy() %>% \n    select(term, estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n  term          estimate\n  <chr>            <dbl>\n1 (Intercept)     19.2  \n2 locationouter   -4.05 \n3 wind_m_s        -0.499\n```\n:::\n:::\n\n\n## R\n\nFirst, we define the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_add_r <- lm(pm2_5 ~ location + wind_m_s,\n                   data = pm2_5_r)\n```\n:::\n\n\nVisualising this is not exactly trivial. There is no easy way to extract the fitted/predicted values by `location`.\n\nSo we'll have to do it manually. We can get the coefficients of the model as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get the coefficients\nlm_pm2_5_add_r$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept) locationouter      wind_m_s \n   19.2175729    -4.0532228    -0.4991019 \n```\n:::\n:::\n\n\nWe can, for example, get the first coefficient with:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_add_r$coefficients[1]\n```\n:::\n\n\nThe linear model will be in the format of:\n\n$predicted\\_value = \\beta_0 + \\beta_1 \\times wind\\_m\\_s$\n\nwith the $\\beta_0$ for the `outer` location needing correcting, since it is relative to the `inner` location (this correction is given as the second coefficient).\n\nThankfully we can create the linear model using the `abline()` function. This has a `coef =` argument that takes model coefficients. This then gives us:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the original data\nplot(pm2_5 ~ wind_m_s,\n     col = factor(location),\n     data = pm2_5_r)\n\n# add the regression line for the first (inner) location\nabline(coef = c(lm_pm2_5_add_r$coefficients[1],\n                lm_pm2_5_add_r$coefficients[3]), col = 1)\n\n# add the regression line for the second (outer) location\nabline(coef = c((lm_pm2_5_add_r$coefficients[1] +\n                 lm_pm2_5_add_r$coefficients[2]),\n                 lm_pm2_5_add_r$coefficients[3]), col = 2)\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-49-1.png){width=672}\n:::\n:::\n\n\n## Python\n\nFirst, we define the model\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula= \"pm2_5 ~ C(location) + wind_m_s\",\n                data = pm2_5_py)\n                \n# and get the fitted parameters of the model\nlm_pm2_5_add_py = model.fit()\n```\n:::\n\n\nWe can visualise this as follows:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(pm2_5_py, aes(x = \"wind_m_s\",\n                     y = lm_pm2_5_add_py.fittedvalues,\n                     colour = \"location\")) +\n    geom_line(size = 1) +\n    geom_point(aes(y = \"pm2_5\")))\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-51-1.png){width=614}\n:::\n:::\n\n\nNext, we extract the coefficient estimates:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlm_pm2_5_add_py.params\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercept               19.217573\nC(location)[T.outer]    -4.053223\nwind_m_s                -0.499102\ndtype: float64\n```\n:::\n:::\n\n\n:::\n\nSo our two equations would be as follows:\n\n$PM2.5_{inner} = 19.22 - 0.50 \\times wind\\_m\\_s$\n\n$PM2.5_{outer} = (19.22 - 4.05) - 0.50 \\times wind\\_m\\_s$\n\ngives\n\n$PM2.5_{outer} = 15.17 - 0.50 \\times wind\\_m\\_s$\n\n### Revisiting linear regression\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\nFirst, we define the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_wind <- lm(pm2_5 ~ wind_m_s,\n                   data = pm2_5)\n```\n:::\n\n\nWe can visualise this as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_wind %>% \n    augment() %>% \n    ggplot(aes(x = wind_m_s, y = .fitted)) +\n    geom_line(colour = \"blue\", size = 1) +\n    geom_point(data = pm2_5,\n               aes(x = wind_m_s, y = pm2_5))\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n:::\n\n\n::: {.callout-tip collapse=\"true\"}\n## Alternative using `geom_smooth()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pm2_5, aes(x = wind_m_s, y = pm2_5)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-55-1.png){width=672}\n:::\n:::\n\n:::\n\nNext, we extract the coefficient estimates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_wind %>%\n    tidy() %>% \n    select(term, estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   17.3  \n2 wind_m_s      -0.529\n```\n:::\n:::\n\n\n## R\n\nFirst, we define the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_wind_r <- lm(pm2_5 ~ wind_m_s,\n                   data = pm2_5_r)\n```\n:::\n\n\nExtract the model coefficients:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_wind_r$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)    wind_m_s \n 17.3267081  -0.5285102 \n```\n:::\n:::\n\n\nWe can visualise this as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the original data\nplot(pm2_5 ~ wind_m_s,\n     col = factor(location),\n     data = pm2_5_r)\n\n# add the regression line for the first (inner) location\nabline(coef = c(lm_pm2_5_wind_r$coefficients[1],\n                lm_pm2_5_wind_r$coefficients[2]), col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-59-1.png){width=672}\n:::\n:::\n\n\n## Python\n\nFirst, we define the model\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula= \"pm2_5 ~ wind_m_s\",\n                data = pm2_5_py)\n                \n# and get the fitted parameters of the model\nlm_pm2_5_wind_py = model.fit()\n```\n:::\n\n\nWe can visualise this as follows:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(pm2_5_py, aes(x = \"wind_m_s\",\n                      y = lm_pm2_5_wind_py.fittedvalues)) +\n    geom_line(colour = \"blue\", size = 1) +\n    geom_point(aes(y = \"pm2_5\")))\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-61-1.png){width=614}\n:::\n:::\n\n\n::: {.callout-tip collapse=\"true\"}\n## Alternative using `geom_smooth()`\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(pm2_5_py, aes(x = \"wind_m_s\", y = \"pm2_5\")) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = False, colour = \"blue\"))\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-62-3.png){width=614}\n:::\n:::\n\n:::\n\nNext, we extract the coefficient estimates:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlm_pm2_5_wind_py.params\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercept    17.326708\nwind_m_s     -0.528510\ndtype: float64\n```\n:::\n:::\n\n\n:::\n\nThis gives us the following equation:\n\n$PM2.5 = 17.33 - 0.53 \\times wind\\_m\\_s$\n\n### Revisiting ANOVA\n\nIf we're just looking at the effect of `location`, then we're essentially doing a one-way ANOVA.\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\nFirst, we define the model:\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_loc <- lm(pm2_5 ~ location,\n                   data = pm2_5)\n```\n:::\n\n\nWe can visualise this as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_loc %>% \n    augment() %>% \n    ggplot(aes(x = location, y = .fitted)) +\n    geom_point(colour = \"blue\", size = 3) +\n    geom_jitter(data = pm2_5,\n                aes(x = location, y = pm2_5),\n                width = 0.1,\n                alpha = 0.2)\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-65-1.png){width=672}\n:::\n:::\n\n\nOK, what's going on here? I've plotted the `.fitted` values (the values predicted by the model) in blue and overlaid the original (with a little bit of jitter to avoid overplotting). However, there are only two predicted values!\n\nWe can check this and see that each unique fitted value occurs 365 times:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_loc %>% \n    augment() %>% \n    count(location, .fitted)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  location .fitted     n\n  <chr>      <dbl> <int>\n1 inner       16.9   365\n2 outer       12.8   365\n```\n:::\n:::\n\n\nThis makes sense if we think back to our original ANOVA exploration. There we established that an ANOVA is just a special case of a linear model, where the fitted values are equal to the mean of each group.\n\nWe could even check this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npm2_5 %>% \n    group_by(location) %>% \n    summarise(mean_pm2_5 = mean(pm2_5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  location mean_pm2_5\n  <chr>         <dbl>\n1 inner          16.9\n2 outer          12.8\n```\n:::\n:::\n\n\nSo, that matches. We move on and extract the coefficient estimates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_loc %>%\n    tidy() %>% \n    select(term, estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  term          estimate\n  <chr>            <dbl>\n1 (Intercept)      16.9 \n2 locationouter    -4.11\n```\n:::\n:::\n\n\nThese values match up exactly with the predicted values for each individual location.\n\n## R\n\nFirst, we define the model:\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_loc_r <- lm(pm2_5 ~ location,\n                   data = pm2_5_r)\n```\n:::\n\n\nExtract the model coefficients:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_loc_r$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept) locationouter \n     16.94293      -4.11157 \n```\n:::\n:::\n\n\nWe can visualise this as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get the location data (inner/outer)\nlocation <- pm2_5_r$location\n\n# get the PM2.5 levels\npm2_5_levels <- pm2_5_r$pm2_5\n\n# and get the PM2.5 levels predicted by the model\nfitted.values <- lm_pm2_5_loc_r$fitted.values\n\n# plot the (jittered) data by location\nstripchart(pm2_5_levels ~ location,\n           method = \"jitter\",\n           pch = 19,\n           col = alpha(\"black\", 0.4),\n           vertical = TRUE)\n\n# add the fitted values\npoints(fitted.values, col = \"blue\", pch = 19)\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-71-1.png){width=672}\n:::\n:::\n\n\nOK, what's going on here? I've plotted the `fitted.values` (the values predicted by the model) in blue and overlaid the original (with a little bit of jitter to avoid overplotting). However, there are only two predicted values!\n\nWe can check this and see that each unique fitted value occurs multiple times:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(lm_pm2_5_loc_r$fitted.values)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n12.8313561643836 12.8313561643837 16.9429260273967 16.9429260273973 \n             364                1                1              364 \n```\n:::\n:::\n\n\nThe `table()` function gives us an overview of each unique value in the data and counts the number of occurrences.\n\nIf you look closely you'll see that each value actually occurs 365 times, with a tiny, tiny rounding difference on the last digit between 364 values and the one remaining one.\n\nAltogether, there are two values: one for the `inner` location (16.94) and one for the `outer` location (12.83).\n\nThis makes sense if we think back to our original ANOVA exploration. There we established that an ANOVA is just a special case of a linear model, where the fitted values are equal to the mean of each group.\n\nWe could even check this, since we stored the data per location in the `inner_r` and `outer_r` objects previously:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(inner_r$pm2_5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 16.94293\n```\n:::\n\n```{.r .cell-code}\nmean(outer_r$pm2_5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 12.83136\n```\n:::\n:::\n\n\nSo, that matches. We move on and extract the coefficient estimates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_loc_r$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept) locationouter \n     16.94293      -4.11157 \n```\n:::\n:::\n\n\nThese values match up exactly with the predicted values for each individual location.\n\n## Python\n\nFirst, we define the model:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula= \"pm2_5 ~ C(location)\", data = pm2_5_py)\n# and get the fitted parameters of the model\nlm_pm2_5_loc_py = model.fit()\n```\n:::\n\n\nWe can visualise this as follows:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(pm2_5_py, aes(x = \"location\",\n                     y = lm_pm2_5_loc_py.fittedvalues)) +\n    geom_point(colour = \"blue\", size = 3) +\n    geom_jitter(pm2_5_py, aes(x = \"location\", y = \"pm2_5\"),\n                       width = 0.1,\n                       alpha = 0.2))\n```\n\n::: {.cell-output-display}\n![](cs5_practical_multiple-linear-regression_files/figure-html/unnamed-chunk-76-1.png){width=614}\n:::\n:::\n\n\nOK, what's going on here? I've plotted the `fittedvalues` (the values predicted by the model) in blue and overlaid the original (with a little bit of jitter to avoid overplotting). However, there are only two predicted values!\n\nWe can check this and see that each unique fitted value occurs 365 times, using the `value_counts()` function on the fitted values:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlm_pm2_5_loc_py.fittedvalues.value_counts()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n16.942926    365\n12.831356    365\ndtype: int64\n```\n:::\n:::\n\n\nThis makes sense if we think back to our original ANOVA exploration. There we established that an ANOVA is just a special case of a linear model, where the fitted values are equal to the mean of each group.\n\nWe could even check this:\n\n\n::: {.cell}\n\n```{.python .cell-code}\npm2_5_py.groupby(\"location\")[\"pm2_5\"].mean()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlocation\ninner    16.942926\nouter    12.831356\nName: pm2_5, dtype: float64\n```\n:::\n:::\n\n\nSo, that matches. We move on and extract the coefficient estimates:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlm_pm2_5_loc_py.params\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercept               16.942926\nC(location)[T.outer]    -4.111570\ndtype: float64\n```\n:::\n:::\n\n\nThese values match up exactly with the predicted values for each individual location.\n\n:::\n\nThis gives us the following equation:\n\n$\\bar{PM2.5_{inner}} = 16.94$\n\n$\\bar{PM2.5_{outer}} = 16.94 - 4.11 = 12.83$\n\n### The null model\n\nThe null model by itself is rarely analysed for its own sake but is instead used a reference point for more sophisticated model selection techniques. It represents your data as an overal average value.\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\nWe define the null model as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_null <- lm(pm2_5 ~ 1, data = pm2_5)\n```\n:::\n\n\nWe can just view the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_null\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = pm2_5 ~ 1, data = pm2_5)\n\nCoefficients:\n(Intercept)  \n      14.89  \n```\n:::\n:::\n\n\n\n## R\n\nWe define the null model as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_null_r <- lm(pm2_5 ~ 1,\n                      data = pm2_5_r)\n```\n:::\n\n\nWe can just view the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_pm2_5_null_r\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = pm2_5 ~ 1, data = pm2_5_r)\n\nCoefficients:\n(Intercept)  \n      14.89  \n```\n:::\n:::\n\n\n## Python\n\nWe define the null model as follows:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula= \"pm2_5 ~ 1\", data = pm2_5_py)\n# and get the fitted parameters of the model\nlm_pm2_5_null_py = model.fit()\n```\n:::\n\n\nWe can just view the model parameters:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlm_pm2_5_null_py.params\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercept    14.887141\ndtype: float64\n```\n:::\n:::\n\n\n:::\n\nThis shows us that there is just one value: 14.89. This is the average across all the PM2.5 values in the data set.\n\nHere we'd predict the PM2.5 values as follows:\n\n$PM2.5 = 14.89$ \n\n## Exercise\n\n::: {.callout-tip collapse=\"true\"}\n## Answer\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n## R\n## Python\n:::\n:::\n\n## Key points\n\n::: callout-note\n-\n-\n-\n:::\n",
    "supporting": [
      "cs5_practical_multiple-linear-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}