{
  "hash": "4651fdd898ae45f559428664dffc0597",
  "result": {
    "markdown": "---\ntitle: \"Correlations\"\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n::: callout-tip\n## Learning outcomes\n\n**Questions**\n\n-   What are correlation coefficients?\n-   What kind of correlation coefficients are there and when do I use them?\n\n**Objectives**\n\n-   Be able to calculate correlation coefficients in R or Python\n-   Use visual tools to explore correlations between variables\n-   Know the limitations of correlation coefficients\n:::\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n### Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n# A package for exploring correlations in R\nlibrary(corrr)\n```\n:::\n\n\n### Functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creates diagnostic plots\nggResidpanel::resid_panel()\n```\n:::\n\n\n## R\n\n## Python\n:::\n:::\n\n## Purpose and aim\n\nCorrelation refers to the relationship of two variables (or data sets) to one another. Two data sets are said to be correlated if they are not independent from one another. Correlations can be useful because they can indicate if a predictive relationship may exist. However just because two data sets are correlated does not mean that they are causally related.\n\n## Data and hypotheses\n\nWe will use the `USArrests` data set for this example. This rather bleak data set contains statistics in arrests per 100,000 residents for assault, murder and robbery in each of the 50 US states in 1973, alongside the proportion of the population who lived in urban areas at that time. `USArrests` is a data frame with 50 observations of five variables: `state`, `murder`, `assault`, `urban_pop` and `robbery`.\n\nWe will be using these data to explore if there are correlations between these variables.\n\nThe data are stored in the file `data/CS3-usarrests.csv`.\n\n## Summarise and visualise\n\nFirst, we load the data:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the data\nUSArrests <- read_csv(\"data/CS3-usarrests.csv\")\n\n# have a look at the data\nUSArrests\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 5\n   state       murder assault urban_pop robbery\n   <chr>        <dbl>   <dbl>     <dbl>   <dbl>\n 1 Alabama       13.2     236        58    21.2\n 2 Alaska        10       263        48    44.5\n 3 Arizona        8.1     294        80    31  \n 4 Arkansas       8.8     190        50    19.5\n 5 California     9       276        91    40.6\n 6 Colorado       7.9     204        78    38.7\n 7 Connecticut    3.3     110        77    11.1\n 8 Delaware       5.9     238        72    15.8\n 9 Florida       15.4     335        80    31.9\n10 Georgia       17.4     211        60    25.8\n# … with 40 more rows\n```\n:::\n:::\n\n\nWe can create a visual overview of the potential correlations that might exist between the variables. For this, we use the `corrr` package.\n\nMake sure to install the package, if you haven't done so already:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"corrr\")\n```\n:::\n\n\nand then load it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(corrr)\n```\n:::\n\n\nWe can only calculate correlations between numerical variables, so we have to deselect the `state` variable. Next, we calculate the correlations with `correlate()`. We `shave()` off the redundant top results (`murder` vs `assault` is the same as `assault` vs `murder`) and plot the result using `rplot()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create correlation graph\nUSArrests %>% \n    select(-state) %>% \n    correlate() %>% \n    shave() %>% \n    rplot()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nCorrelation method: 'pearson'\nMissing treated using: 'pairwise.complete.obs'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nDon't know how to automatically pick scale for object of type noquote. Defaulting to continuous.\n```\n:::\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThe output tells us that the correlation method used is `pearson` (see below) and that only paired observations without missing values are taken into account.\n\nOn the right there is a scale of the strength of the correlation.\n\n## R\n\nFirst, we load the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the data\nUSArrests_r <- read.csv(\"data/CS3-usarrests.csv\")\n\n# and have a look at the data\nhead(USArrests_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       state murder assault urban_pop robbery\n1    Alabama   13.2     236        58    21.2\n2     Alaska   10.0     263        48    44.5\n3    Arizona    8.1     294        80    31.0\n4   Arkansas    8.8     190        50    19.5\n5 California    9.0     276        91    40.6\n6   Colorado    7.9     204        78    38.7\n```\n:::\n:::\n\n\nWe can only calculate correlations between numerical variables, so we have to deselect the `state` variable.\n\nWe can load the data with an extra argument, `row.names = 1`. This will instruct R to load the data but use first column (`state`) as row names:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the data\nUSArrests_r <- read.csv(\"data/CS3-usarrests.csv\", row.names = 1)\n\n# have a look at the data\nhead(USArrests_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           murder assault urban_pop robbery\nAlabama      13.2     236        58    21.2\nAlaska       10.0     263        48    44.5\nArizona       8.1     294        80    31.0\nArkansas      8.8     190        50    19.5\nCalifornia    9.0     276        91    40.6\nColorado      7.9     204        78    38.7\n```\n:::\n:::\n\n\nWe can visualise the data with the `pairs()` function. This function creates a matrix of scatter plots that we can use to look for correlations. Every combination of variables appears twice (e.g. `murder` vs `assault` is the same as `assault` vs `murder`), so we use the `lower.panel = NULL` argument to only visualise the unique combinations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create matrix of scatter plots\npairs(USArrests_r, lower.panel = NULL)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n## Python\n:::\n\nFrom the visual inspection we can see that there appears to be a slight positive correlation between all pairs of variables, although this may be very weak in some cases (`murder` and `urban_pop` for example).\n\n::: callout-note\n## Correlation coefficients\n\nThe method we used above is **Pearson's r**. This is a measure of the linear correlation between two variables. It has a value between -1 and +1, where +1 means a perfect positive correlation, -1 means a perfect negative correlation and 0 means no correlation at all.\n\nThere are other correlation coefficients, most notably the **Spearman's rank correlation coefficient**, a non-parametric measure of rank correlation and is generally less sensitive to outliers.\n:::\n\n## Implement and interpret test\n\nA bit earlier we created a graph that visualised the possible correlations between the different variables. Underlying that were the values of Pearson's r. If we want to get the actual values, we can do the following:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate correlation matrix\nUSArrests %>% \n    select(-state) %>% \n    correlate() %>% \n    shave()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nCorrelation method: 'pearson'\nMissing treated using: 'pairwise.complete.obs'\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 5\n  term       murder assault urban_pop robbery\n  <chr>       <dbl>   <dbl>     <dbl>   <dbl>\n1 murder    NA       NA        NA          NA\n2 assault    0.802   NA        NA          NA\n3 urban_pop  0.0696   0.259    NA          NA\n4 robbery    0.564    0.665     0.411      NA\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(USArrests_r, method = \"pearson\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              murder   assault  urban_pop   robbery\nmurder    1.00000000 0.8018733 0.06957262 0.5635788\nassault   0.80187331 1.0000000 0.25887170 0.6652412\nurban_pop 0.06957262 0.2588717 1.00000000 0.4113412\nrobbery   0.56357883 0.6652412 0.41134124 1.0000000\n```\n:::\n:::\n\n\n-   The first argument is a matrix or a data frame\n-   The argument `method` tells R which correlation coefficient to use (`pearson` (default), `kendall`, or `spearman`)\n\n## Python\n:::\n\nThe table gives the correlation coefficient between each pair of variables in the data frame. The most correlated variables are murder and assault with an $r$ value of 0.80. This appears to agree well with the set plots that we produced earlier.\n\n## Exercise: Pearson's r for USA state data\n\nPearson's correlation for USA state data\n\nWe will use the data from the file `data/CS3-statedata.csv` data set for this exercise. This rather more benign data set contains information on more general properties of each US state, such as population (1975), per capita income (1974), illiteracy proportion (1970), life expectancy (1969), murder rate per 100,000 people (there's no getting away from it), percentage of the population who are high-school graduates, average number of days where the minimum temperature is below freezing between 1931 and 1960, and the state area in square miles. The data set contains 50 rows and 8 columns, with column names: `population`, `income`, `illiteracy`, `life_exp`, `murder`, `hs_grad`, `frost` and `area`.\n\nVisually identify 3 different pairs of variables that appear to be\n\n1.  the most positively correlated\n2.  the most negatively correlated\n3.  not correlated at all\n\nCalculate Pearson's r for all variable pairs and see how well you were able to identify correlation visually.\n\n::: {.callout-tip collapse=\"true\"}\n## Hint\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\nLook at the help page of the `stretch()` function from the `corrr` package to help extract the correct values from the correlation matrix.\n\n## R\n\nLook at the help page of the `as.table()` function to help extract the correct values from the correlation matrix. Combine this with the `as.data.frame()` function.\n\n## Python\n:::\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Answer\n\nVisually determining the most negative/positively and uncorrelated pairs of variables:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSAstate <- read_csv(\"data/CS3-statedata.csv\")\n\n# have a look at the data\nUSAstate\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 9\n   state       population income illiteracy life_exp murder hs_grad frost   area\n   <chr>            <dbl>  <dbl>      <dbl>    <dbl>  <dbl>   <dbl> <dbl>  <dbl>\n 1 Alabama           3615   3624        2.1     69.0   15.1    41.3    20  50708\n 2 Alaska             365   6315        1.5     69.3   11.3    66.7   152 566432\n 3 Arizona           2212   4530        1.8     70.6    7.8    58.1    15 113417\n 4 Arkansas          2110   3378        1.9     70.7   10.1    39.9    65  51945\n 5 California       21198   5114        1.1     71.7   10.3    62.6    20 156361\n 6 Colorado          2541   4884        0.7     72.1    6.8    63.9   166 103766\n 7 Connecticut       3100   5348        1.1     72.5    3.1    56     139   4862\n 8 Delaware           579   4809        0.9     70.1    6.2    54.6   103   1982\n 9 Florida           8277   4815        1.3     70.7   10.7    52.6    11  54090\n10 Georgia           4931   4091        2       68.5   13.9    40.6    60  58073\n# … with 40 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# visualise the correlations\nUSAstate %>% \n    select(-state) %>% \n    correlate() %>% \n    shave() %>% \n    rplot()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nCorrelation method: 'pearson'\nMissing treated using: 'pairwise.complete.obs'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nDon't know how to automatically pick scale for object of type noquote. Defaulting to continuous.\n```\n:::\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSAstate_r <- read.csv(\"data/CS3-statedata.csv\",\n                     row.names = 1)\n\n# have a look at the data\nhead(USAstate_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           population income illiteracy life_exp murder hs_grad frost   area\nAlabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\nAlaska            365   6315        1.5    69.31   11.3    66.7   152 566432\nArizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\nArkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\nCalifornia      21198   5114        1.1    71.71   10.3    62.6    20 156361\nColorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(USAstate_r, lower.panel = NULL)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n## Python\n:::\n\nIt looks like:\n\n1.  `illiteracy` and `murder` are the most positively correlated pair\n2.  `life_exp` and `murder` are the most negatively correlated pair\n3.  `population` and `area` are the least correlated pair\n\nWe can explore that numerically, by doing the following:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\nWe can use the `corrr::stretch()` function. This converts the correlation matrix into a long format. If we use the `remove.dups = TRUE` argument (it is `FALSE` by default) then the duplicate correlations are removed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate the correlation matrix\n# convert into long format, omitting\n# missing values and duplicates\nUSAstate_cor <- USAstate %>% \n    select(-state) %>% \n    correlate() %>% \n    stretch(remove.dups = TRUE) %>% \n    drop_na()\n```\n:::\n\n\nNow we can extract the values we're interested in:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# most positively correlated pair\nUSAstate_cor %>% \n    filter(r == max(r))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  x          y          r\n  <chr>      <chr>  <dbl>\n1 illiteracy murder 0.703\n```\n:::\n\n```{.r .cell-code}\n# most negatively correlated pair\nUSAstate_cor %>% \n    filter(r == min(r))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  x        y           r\n  <chr>    <chr>   <dbl>\n1 life_exp murder -0.781\n```\n:::\n\n```{.r .cell-code}\n# least correlated pair\nUSAstate_cor %>% \n    filter(r == min(abs(r)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  x          y          r\n  <chr>      <chr>  <dbl>\n1 population area  0.0225\n```\n:::\n:::\n\n\nNote that we use the minimum *absolute* value (with the `abs()` function) to find the least correlated pair.\n\n## R\n\nFirst, we need to create the pairwise comparisons, with the relevant Pearson's $r$ values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSAstate_r_cor <- USAstate_r |>\n    # create the correlation matrix\n    cor(method = \"pearson\") |>\n    # build a contingency table\n    as.table() |>\n    # and create a dataframe\n    as.data.frame()\n\n# and have a look\nUSAstate_r_cor\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Var1       Var2        Freq\n1  population population  1.00000000\n2      income population  0.20822756\n3  illiteracy population  0.10762237\n4    life_exp population -0.06805195\n5      murder population  0.34364275\n6     hs_grad population -0.09848975\n7       frost population -0.33215245\n8        area population  0.02254384\n9  population     income  0.20822756\n10     income     income  1.00000000\n11 illiteracy     income -0.43707519\n12   life_exp     income  0.34025534\n13     murder     income -0.23007761\n14    hs_grad     income  0.61993232\n15      frost     income  0.22628218\n16       area     income  0.36331544\n17 population illiteracy  0.10762237\n18     income illiteracy -0.43707519\n19 illiteracy illiteracy  1.00000000\n20   life_exp illiteracy -0.58847793\n21     murder illiteracy  0.70297520\n22    hs_grad illiteracy -0.65718861\n23      frost illiteracy -0.67194697\n24       area illiteracy  0.07726113\n25 population   life_exp -0.06805195\n26     income   life_exp  0.34025534\n27 illiteracy   life_exp -0.58847793\n28   life_exp   life_exp  1.00000000\n29     murder   life_exp -0.78084575\n30    hs_grad   life_exp  0.58221620\n31      frost   life_exp  0.26206801\n32       area   life_exp -0.10733194\n33 population     murder  0.34364275\n34     income     murder -0.23007761\n35 illiteracy     murder  0.70297520\n36   life_exp     murder -0.78084575\n37     murder     murder  1.00000000\n38    hs_grad     murder -0.48797102\n39      frost     murder -0.53888344\n40       area     murder  0.22839021\n41 population    hs_grad -0.09848975\n42     income    hs_grad  0.61993232\n43 illiteracy    hs_grad -0.65718861\n44   life_exp    hs_grad  0.58221620\n45     murder    hs_grad -0.48797102\n46    hs_grad    hs_grad  1.00000000\n47      frost    hs_grad  0.36677970\n48       area    hs_grad  0.33354187\n49 population      frost -0.33215245\n50     income      frost  0.22628218\n51 illiteracy      frost -0.67194697\n52   life_exp      frost  0.26206801\n53     murder      frost -0.53888344\n54    hs_grad      frost  0.36677970\n55      frost      frost  1.00000000\n56       area      frost  0.05922910\n57 population       area  0.02254384\n58     income       area  0.36331544\n59 illiteracy       area  0.07726113\n60   life_exp       area -0.10733194\n61     murder       area  0.22839021\n62    hs_grad       area  0.33354187\n63      frost       area  0.05922910\n64       area       area  1.00000000\n```\n:::\n:::\n\n\nIs this method obvious? No! Some create Googling led to [Stackoverflow](https://stackoverflow.com/questions/7074246/show-correlations-as-an-ordered-list-not-as-a-large-matrix) and here we are. But, it does give us what we need.\n\nNow that we have the paired comparisons, we can extract the relevant data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# first we remove the same-pair correlations\nUSAstate_r_cor <- USAstate_r_cor |>\n    # remove the same-pair correlations\n    subset(Freq != 1)\n\nUSAstate_r_cor[, max(USAstate_r_cor$Freq)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndata frame with 0 columns and 56 rows\n```\n:::\n\n```{.r .cell-code}\n# most positively correlated pair\nUSAstate_r_cor[which.max(USAstate_r_cor$Freq), ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Var1       Var2      Freq\n21 murder illiteracy 0.7029752\n```\n:::\n\n```{.r .cell-code}\n# most negatively correlated pair\nUSAstate_r_cor[which.min(USAstate_r_cor$Freq), ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Var1     Var2       Freq\n29 murder life_exp -0.7808458\n```\n:::\n\n```{.r .cell-code}\n# least correlated pair\nUSAstate_r_cor[which.min(abs(USAstate_r_cor$Freq)), ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Var1       Var2       Freq\n8 area population 0.02254384\n```\n:::\n:::\n\n\nNote that we use the minimum *absolute* value (with the `abs()` function) to find the least correlated pair.\n\n## Python\n:::\n:::\n\n## Spearman's rank correlation coefficient\n\nThis test first calculates the rank of the numerical data (i.e. their position from smallest (or most negative) to the largest (or most positive)) in the two variables and then calculates Pearson's product moment correlation coefficient using the ranks. As a consequence, this test is less sensitive to outliers in the distribution.\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate correlation matrix\nUSArrests %>% \n    select(-state) %>% \n    correlate(method = \"spearman\") %>% \n    shave()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nCorrelation method: 'spearman'\nMissing treated using: 'pairwise.complete.obs'\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 5\n  term      murder assault urban_pop robbery\n  <chr>      <dbl>   <dbl>     <dbl>   <dbl>\n1 murder    NA      NA        NA          NA\n2 assault    0.817  NA        NA          NA\n3 urban_pop  0.107   0.275    NA          NA\n4 robbery    0.679   0.714     0.438      NA\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(USArrests_r, method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             murder   assault urban_pop   robbery\nmurder    1.0000000 0.8172735 0.1067163 0.6794265\nassault   0.8172735 1.0000000 0.2752133 0.7143681\nurban_pop 0.1067163 0.2752133 1.0000000 0.4381068\nrobbery   0.6794265 0.7143681 0.4381068 1.0000000\n```\n:::\n:::\n\n\n## Python\n:::\n\n## Exercise: State data (Spearman)\n\nSpearman's correlation for USA state data\n\nCalculate Spearman's correlation coefficient for the `data/CS3-statedata.csv` data set.\n\nWhich variable's correlations are affected most by the use of the Spearman's rank compared with Pearson's r? Hint: think of a way to address this question programmatically.\n\nThinking about the variables, can you explain why this might this be?\n\n::: {.callout-tip collapse=\"true\"}\n## Answer\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate correlation matrix\nUSAstate %>% \n    select(-state) %>% \n    correlate(method = \"spearman\") %>% \n    shave()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nCorrelation method: 'spearman'\nMissing treated using: 'pairwise.complete.obs'\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 × 9\n  term       population  income illiteracy life_exp murder hs_grad  frost  area\n  <chr>           <dbl>   <dbl>      <dbl>    <dbl>  <dbl>   <dbl>  <dbl> <dbl>\n1 population     NA     NA          NA       NA     NA      NA     NA        NA\n2 income          0.125 NA          NA       NA     NA      NA     NA        NA\n3 illiteracy      0.313 -0.315      NA       NA     NA      NA     NA        NA\n4 life_exp       -0.104  0.324      -0.555   NA     NA      NA     NA        NA\n5 murder          0.346 -0.217       0.672   -0.780 NA      NA     NA        NA\n6 hs_grad        -0.383  0.510      -0.655    0.524 -0.437  NA     NA        NA\n7 frost          -0.459  0.197      -0.683    0.298 -0.544   0.399 NA        NA\n8 area           -0.121  0.0571     -0.250    0.128  0.106   0.439  0.112    NA\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(USAstate_r, method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           population      income illiteracy   life_exp     murder    hs_grad\npopulation  1.0000000  0.12460984  0.3130496 -0.1040171  0.3457401 -0.3833649\nincome      0.1246098  1.00000000 -0.3145948  0.3241050 -0.2174623  0.5104809\nilliteracy  0.3130496 -0.31459482  1.0000000 -0.5553735  0.6723592 -0.6545396\nlife_exp   -0.1040171  0.32410498 -0.5553735  1.0000000 -0.7802406  0.5239410\nmurder      0.3457401 -0.21746230  0.6723592 -0.7802406  1.0000000 -0.4367330\nhs_grad    -0.3833649  0.51048095 -0.6545396  0.5239410 -0.4367330  1.0000000\nfrost      -0.4588526  0.19686382 -0.6831936  0.2983910 -0.5438432  0.3985351\narea       -0.1206723  0.05709484 -0.2503721  0.1275002  0.1064259  0.4389752\n                frost        area\npopulation -0.4588526 -0.12067227\nincome      0.1968638  0.05709484\nilliteracy -0.6831936 -0.25037208\nlife_exp    0.2983910  0.12750018\nmurder     -0.5438432  0.10642590\nhs_grad     0.3985351  0.43897520\nfrost       1.0000000  0.11228778\narea        0.1122878  1.00000000\n```\n:::\n:::\n\n\n## Python\n:::\n\nIn order to determine which variables are most affected by the choice of Spearman vs Pearson you could just plot both matrices out side by side and try to spot what was going on, but one of the reasons we're using programming languages is that we can be a bit more **programmatic** about these things. Also, our eyes aren't that good at processing and parsing this sort of information display. A better way would be to somehow visualise the data.\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\nWe're going to subtract two correlation matrices from one another. This only works on numerical data, so we do the following:\n\n1.  calculate the correlations\n2.  remove the variable names\n3.  subtract the two matrices\n4.  find the absolute differences (we do not care about the direction of change)\n5.  put back the variable names\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create Pearson's r correlation matrix\nUSAstate_pearson <- USAstate %>% \n    select(-state) %>% \n    correlate(method = \"pearson\") %>% \n    shave() %>%\n    select(-term)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nCorrelation method: 'pearson'\nMissing treated using: 'pairwise.complete.obs'\n```\n:::\n\n```{.r .cell-code}\n# create Spearmans correlation matrix\nUSAstate_spearman <- USAstate %>% \n    select(-state) %>% \n    correlate(method = \"spearman\") %>% \n    shave() %>% \n    select(-term)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nCorrelation method: 'spearman'\nMissing treated using: 'pairwise.complete.obs'\n```\n:::\n\n```{.r .cell-code}\nUSAstate_diff <- USAstate_pearson - USAstate_spearman\n\n# get the row names of the correlation matrix\nUSAstate_colnames <- USAstate %>%\n    select(-state) %>% \n    correlate() %>% \n    select(term)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nCorrelation method: 'pearson'\nMissing treated using: 'pairwise.complete.obs'\n```\n:::\n\n```{.r .cell-code}\n# combine the two tables\n# taking the absolute values of the differences\n# and plot\nbind_cols(USAstate_colnames, abs(USAstate_diff)) %>% \n    rplot()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nDon't know how to automatically pick scale for object of type noquote. Defaulting to continuous.\n```\n:::\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorPear <- cor(USAstate_r, method = \"pearson\")\ncorSpea <- cor(USAstate_r, method = \"spearman\")\ncorDiff <- corPear - corSpea\n```\n:::\n\n\nAgain, we could now just look at a grid of 64 numbers and see if we can spot the biggest differences, but our eyes aren't that good at processing and parsing this sort of information display. A better way would be to somehow visualise the data. We can do that using some R plotting functions, `heatmap()` to be exact. The `heatmap()` function has a lot of features that we don't need and so I'm not going to go into it in detail here. The main reason I'm using it is that it displays matrices the right way round (other plotting functions display matrices rotated by 90 degrees) and automatically labels the rows and columns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nheatmap(abs(corDiff), symm = TRUE, Rowv = NA)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\nThe `abs()` function calculates the absolute value (i.e. just the magnitude) of the matrix values. This is just because I only care about situations where the two correlation coefficients are different from each other but I don't care which is the larger. The `symm` argument tells the function that we have a symmetric matrix and in conjunction with the `Rowv = NA` argument stops the plot from reordering the rows and columns. The `Rowv = NA` argument also stops the function from adding dendrograms to the margins of the plot.\n\nThe plot itself is coloured from yellow, indicating the smallest values (which in this case correspond to no difference in correlation coefficients), through orange to dark red, indicating the biggest values (which in this case correspond to the variables with the biggest difference in correlation coefficients).\n\nThe plot is symmetric along the leading diagonal (hopefully for obvious reasons) and we can see that the majority of squares are light yellow in colour, which means that there isn't much difference between Spearman and Pearson for the vast majority of variables. The squares appear darkest when we look along the `area` row/column suggesting that there's a big difference in the correlation coefficients there.\n\n## Python\n:::\n\nAll in all there is not a huge difference in correlation coefficients, since the values are all quite small. Most of the changes occur along the `area` variable. One possible explanation could be that certain states with a large area have a relatively large effect on the Pearson's r coefficient. For example, Alaska has an area that is over twice as big as the next state - Texas.\n\nIf we'd look a bit closer then we would find for `area` and `income` that Pearson gives a value of 0.36, a slight positive correlation, whereas Spearman gives a value of 0.057, basically uncorrelated.\n\nThis means that this is basically ignored by Spearman.\n\nWell done, [Mr. Spearman](https://en.wikipedia.org/wiki/Charles_Spearman).\n:::\n\n## Key points\n\n::: callout-note\n-   Correlation is the degree to which two variables are linearly related\n-   Correlation does not imply causation\n-   We can visualise correlations by plotting variables against each other or creating heatmap-type plots of the correlation coefficients\n-   Two main correlation coefficients are Pearson's r and Spearman's rank, with Spearman's rank being less sensitive to outliers\n:::\n",
    "supporting": [
      "cs3_practical_correlations_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}