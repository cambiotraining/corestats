{
  "hash": "c101be93216d36e2905ba44fa670cf1c",
  "result": {
    "markdown": "---\ntitle: \"Correlations\"\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-tip}\n#### Learning outcomes\n\n**Questions**\n\n-   What are correlation coefficients?\n-   What kind of correlation coefficients are there and when do I use them?\n\n**Objectives**\n\n-   Be able to calculate correlation coefficients in R or Python\n-   Use visual tools to explore correlations between variables\n-   Know the limitations of correlation coefficients\n:::\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n### Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n```\n:::\n\n\n### Functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Computes the absolute value\nbase::abs()\n\n# Creates a matrix of scatter plots\ngraphics::pairs()\n\n# Computes a correlation matrix\nstats::cor()\n\n# Creates a heat map\nstats::heatmap()\n\n# Turns object into tibble\ntibble::as.tibble()\n\n# Lengthens the data\ntidyr::pivot_longer()\n```\n:::\n\n\n## Python\n\n### Libraries\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n```\n:::\n\n\n### Functions\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Compute pairwise correlation of columns\npandas.DataFrame.corr()\n\n# Plots the first few rows of a DataFrame\npandas.DataFrame.head()\n\n# Query the columns of a DataFrame with a boolean expression\npandas.DataFrame.query()\n\n# Set the name of the axis for the index or columns\npandas.DataFrame.rename_axis()\n\n# Unpivot a DataFrame from wide to long format\npandas.melt()\n\n# Reads in a .csv file\npandas.read_csv()\n```\n:::\n\n:::\n:::\n\n## Purpose and aim\n\nCorrelation refers to the relationship of two variables (or data sets) to one another. Two data sets are said to be correlated if they are not independent from one another. Correlations can be useful because they can indicate if a predictive relationship may exist. However just because two data sets are correlated does not mean that they are causally related.\n\n## Data and hypotheses\n\nWe will use the `USArrests` data set for this example. This rather bleak data set contains statistics in arrests per 100,000 residents for assault, murder and robbery in each of the 50 US states in 1973, alongside the proportion of the population who lived in urban areas at that time. `USArrests` is a data frame with 50 observations of five variables: `state`, `murder`, `assault`, `urban_pop` and `robbery`.\n\nWe will be using these data to explore if there are correlations between these variables.\n\nThe data are stored in the file `data/CS3-usarrests.csv`.\n\n## Summarise and visualise\n\nFirst, we load the data:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the data\nUSArrests <- read_csv(\"data/CS3-usarrests.csv\")\n\n# have a look at the data\nUSArrests\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 5\n   state       murder assault urban_pop robbery\n   <chr>        <dbl>   <dbl>     <dbl>   <dbl>\n 1 Alabama       13.2     236        58    21.2\n 2 Alaska        10       263        48    44.5\n 3 Arizona        8.1     294        80    31  \n 4 Arkansas       8.8     190        50    19.5\n 5 California     9       276        91    40.6\n 6 Colorado       7.9     204        78    38.7\n 7 Connecticut    3.3     110        77    11.1\n 8 Delaware       5.9     238        72    15.8\n 9 Florida       15.4     335        80    31.9\n10 Georgia       17.4     211        60    25.8\n# ℹ 40 more rows\n```\n:::\n:::\n\n\nWe can create a visual overview of the potential correlations that might exist between the variables. There are different ways of doing this, for example by creating scatter plots between variable pairs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# murder vs robbery\nggplot(USArrests,\n       aes(x = murder, y = robbery)) +\n    geom_point()\n```\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# assault vs urban_pop\nggplot(USArrests,\n       aes(x = assault, y = urban_pop)) +\n    geom_point()\n```\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n:::\n\n\nThis gets a bit tedious if there are many unique variable pairs. Unfortunately `ggplot()` does not have a pairwise function, but we can borrow the one from base R. The `pairs()` function only wants numerical data, so we need to remove the `state` column for this. The `pairs()` function has a `lower.panel` argument that allows you to remove duplicate combinations (after all `murder` vs `assault` is the same as `assault` vs `murder`):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSArrests %>% \n    select(-state) %>% \n    pairs(lower.panel = NULL)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## Python\nFirst, we load the data:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nUSArrests_py = pd.read_csv(\"data/CS3-usarrests.csv\")\n\nUSArrests_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        state  murder  assault  urban_pop  robbery\n0     Alabama    13.2      236         58     21.2\n1      Alaska    10.0      263         48     44.5\n2     Arizona     8.1      294         80     31.0\n3    Arkansas     8.8      190         50     19.5\n4  California     9.0      276         91     40.6\n```\n:::\n:::\n\n\nWe can create a visual overview of the potential correlations that might exist between the variables. There are different ways of doing this, for example by creating scatter plots between variable pairs:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# murder vs robbery\n(ggplot(USArrests_py,\n       aes(x = \"murder\",\n           y = \"robbery\")) +\n     geom_point())\n```\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-11-1.png){width=614}\n:::\n\n```{.python .cell-code}\n# assault vs urban_pop\n(ggplot(USArrests_py,\n       aes(x = \"assault\",\n           y = \"urban_pop\")) +\n     geom_point())\n```\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-11-2.png){width=614}\n:::\n:::\n\n\nThis gets a bit tedious if there are many unique variable pairs. There is an option to automatically create a matrix of scatter plots, using Seaborn. But that would involve installing the `seaborn` package just for this. And frankly, I don't want to - not least because staring at tons of scatter plots is probably not the best way forward anyway!\n\nIf you have your heart set on creating a pairplot, then have a look [at the seaborn documentation](https://seaborn.pydata.org/generated/seaborn.pairplot.html).\n\n:::\n\nFrom the visual inspection we can see that there appears to be a slight positive correlation between all pairs of variables, although this may be very weak in some cases (`murder` and `urban_pop` for example).\n\n## Correlation coefficients {#correlation-coefficients}\n\nInstead of visualising the variables against each other in a scatter plot, we can also calculate correlation coefficients for each variable pair. There are different types of correlation coefficients, but the most well-known one is probably **Pearson's r**. This is a measure of the linear correlation between two variables. It has a value between -1 and +1, where +1 means a perfect positive correlation, -1 means a perfect negative correlation and 0 means no correlation at all.\n\nThere are other correlation coefficients, most notably the **Spearman's rank correlation coefficient**, a non-parametric measure of rank correlation and is generally less sensitive to outliers.\n\nSo, let's calculate **Pearson's r** for our data:\n\n::: {.panel-tabset group=\"language\"}\n## R\nWe can do this using the `cor()` function. Since we can only calculate correlations between numbers, we have to remove the `state` column from our data before calculating the correlations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSArrests %>% \n    select(-state) %>% \n    cor()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              murder   assault  urban_pop   robbery\nmurder    1.00000000 0.8018733 0.06957262 0.5635788\nassault   0.80187331 1.0000000 0.25887170 0.6652412\nurban_pop 0.06957262 0.2588717 1.00000000 0.4113412\nrobbery   0.56357883 0.6652412 0.41134124 1.0000000\n```\n:::\n:::\n\n\nThis gives us a numerical overview of the Pearson's r correlation coefficients between each variable pair. Note that across the diagonal the correlation coefficients are `1` - this should make sense since, for example, `murder` is perfectly correlated with itself.\n\nAs before, the values are mirrored across the diagonal, since the correlation between, for example, `murder` and `assault` is the same as the one between `assault` and `murder`.\n\n### Visualise the correlation matrix\n\nJust staring at a matrix of numbers might not be very useful. It would be good to create some sort of heatmap of the values, so we can visually inspect the data a bit better. There are dedicated packages that allow you to do this (for example the [corrr](https://corrr.tidymodels.org)) package). \n\nHere we'll just use the standard `stats::heatmap()` function. The `symm` argument tells the function that we have a symmetric matrix and in conjunction with the `Rowv = NA` argument stops the plot from reordering the rows and columns. The `Rowv = NA` argument also stops the function from adding dendrograms to the margins of the plot.\n\nThe plot itself is coloured from yellow, indicating the smallest values (which in this case correspond to no difference in correlation coefficients), through orange to dark red, indicating the biggest values (which in this case correspond to the variables with the biggest difference in correlation coefficients).\n\nThe plot is symmetric along the leading diagonal (hopefully for obvious reasons).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSArrests %>% \n  select(-state) %>% \n  cor() %>% \n  heatmap(symm = TRUE, Rowv = NA)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n::: {.callout-note collapse=true}\n## Alternative method 1: ggplot\nBefore we can plot the data we need to reformat the data. We're taking the following steps:\n\n1. we calculate the correlation matrix with `cor()` using the (default) method of `method = \"pearson\"`\n2. convert the output to a tibble so we can use\n3. `pivot_longer()` to reformat the data into pairwise variables and a column with the Pearson's r value\n4. use the `mutate()` and `round()` functions to round the Pearson's r values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSArrests_pear <- USArrests %>% \n    select(-state) %>% \n    cor(method = \"pearson\") %>% \n    as_tibble(rownames = \"var1\") %>% \n    pivot_longer(cols = -var1,\n                 names_to = \"var2\",\n                 values_to = \"pearson_cor\") %>% \n    mutate(pearson_cor = round(pearson_cor, digits = 3))\n```\n:::\n\n\nThe output of that looks like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(USArrests_pear)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  var1    var2      pearson_cor\n  <chr>   <chr>           <dbl>\n1 murder  murder          1    \n2 murder  assault         0.802\n3 murder  urban_pop       0.07 \n4 murder  robbery         0.564\n5 assault murder          0.802\n6 assault assault         1    \n```\n:::\n:::\n\n\nAfter all that, we can visualise the data with `geom_tile()`, adding the Pearson's r values as text labels:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(USArrests_pear,\n       aes(x = var1, y = var2, fill = pearson_cor)) +\n    geom_tile() +\n    geom_text(aes(label = pearson_cor),\n              color = \"white\",\n              size = 4)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n:::\n\n::: {.callout-note collapse=true}\n## Alternative method 2: rstatix\nAs always, there are multiple ways to skin a proverbial cat. If you'd rather use a function from the `rstatix` package (which we've loaded before), then you can run the following code, which uses the `rstatix::cor_test()` function:\n\n::: {.cell}\n\n```{.r .cell-code}\nUSArrests %>% \n    select(-state) %>% \n    cor_test() %>%\n    select(var1, var2, cor) %>% \n    ggplot(aes(x = var1, y = var2, fill = cor)) +\n    geom_tile() +\n    geom_text(aes(label = cor),\n              color = \"white\",\n              size = 4)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n:::\n\n## Python\n\nWe can do this using the `pandas.DataFrame.corr()` function. This function takes the default `method = \"pearson\"` and ignores any non-numerical columns (such as the `state` column in our data set).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nUSArrests_py.corr()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             murder   assault  urban_pop   robbery\nmurder     1.000000  0.801873   0.069573  0.563579\nassault    0.801873  1.000000   0.258872  0.665241\nurban_pop  0.069573  0.258872   1.000000  0.411341\nrobbery    0.563579  0.665241   0.411341  1.000000\n```\n:::\n:::\n\nThis gives us a numerical overview of the Pearson's r correlation coefficients between each variable pair. Note that across the diagonal the correlation coefficients are `1` - this should make sense since, for example, `murder` is perfectly correlated with itself.\n\nAs before, the values are mirrored across the diagonal, since the correlation between, for example, `murder` and `assault` is the same as the one between `assault` and `murder`.\n\n### Visualise the correlation matrix\n\nJust staring at a matrix of numbers might not be very useful. It would be good to create some sort of heatmap of the values, so we can visually inspect the data a bit better.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create correlation matrix\nUSArrests_cor_py = USArrests_py.corr()\n# put the row names into a column\nUSArrests_cor_py = USArrests_cor_py.rename_axis(\"var1\").reset_index()\n\nUSArrests_cor_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        var1    murder   assault  urban_pop   robbery\n0     murder  1.000000  0.801873   0.069573  0.563579\n1    assault  0.801873  1.000000   0.258872  0.665241\n2  urban_pop  0.069573  0.258872   1.000000  0.411341\n3    robbery  0.563579  0.665241   0.411341  1.000000\n```\n:::\n:::\n\n\nNow that we have the correlation matrix in a workable format, we need to restructure it so that we can plot the data. For this, we need to create a \"long\" format, using the `melt()` function.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nUSArrests_pear_py = pd.melt(USArrests_cor_py,\n        id_vars=['var1'],\n        value_vars=['murder', 'assault', 'urban_pop', 'robbery'],\n        var_name='var2',\n        value_name='cor').round(3)\n```\n:::\n\n\nHave a look at the structure:\n\n::: {.cell}\n\n```{.python .cell-code}\nUSArrests_pear_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        var1     var2    cor\n0     murder   murder  1.000\n1    assault   murder  0.802\n2  urban_pop   murder  0.070\n3    robbery   murder  0.564\n4     murder  assault  0.802\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(USArrests_pear_py,\n        aes(x = \"var1\", y = \"var2\", fill = \"cor\")) +\n     geom_tile() +\n     geom_text(aes(label = \"cor\"),\n               colour = \"white\",\n               size = 10))\n```\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-22-1.png){width=614}\n:::\n:::\n\n:::\n\nThe correlation matrix and visualisations give us the insight that we need. The most correlated variables are `murder` and `assault` with an $r$ value of 0.80. This appears to agree well with the set plots that we produced earlier.\n\n## Spearman's rank correlation coefficient\n\nThis test first calculates the rank of the numerical data (i.e. their position from smallest (or most negative) to the largest (or most positive)) in the two variables and then calculates Pearson's product moment correlation coefficient using the ranks. As a consequence, this test is less sensitive to outliers in the distribution.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSArrests %>% \n    select(-state) %>% \n    cor(method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             murder   assault urban_pop   robbery\nmurder    1.0000000 0.8172735 0.1067163 0.6794265\nassault   0.8172735 1.0000000 0.2752133 0.7143681\nurban_pop 0.1067163 0.2752133 1.0000000 0.4381068\nrobbery   0.6794265 0.7143681 0.4381068 1.0000000\n```\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nUSArrests_py.corr(method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             murder   assault  urban_pop   robbery\nmurder     1.000000  0.817274   0.106716  0.679427\nassault    0.817274  1.000000   0.275213  0.714368\nurban_pop  0.106716  0.275213   1.000000  0.438107\nrobbery    0.679427  0.714368   0.438107  1.000000\n```\n:::\n:::\n\n:::\n\n## Exercises\n\n### Pearson's r {#sec-exr_pearsonstate}\n\n:::{.callout-exercise}\n\n\n{{< level 2 >}}\n\n\n\nPearson's correlation for USA state data\n\nWe will again use the data from the file `data/CS3-statedata.csv` data set for this exercise. The data set contains 50 rows and 8 columns, with column names: `population`, `income`, `illiteracy`, `life_exp`, `murder`, `hs_grad`, `frost` and `area`.\n\nVisually identify 3 different pairs of variables that appear to be\n\n1.  the most positively correlated\n2.  the most negatively correlated\n3.  not correlated at all\n\nCalculate Pearson's r for all variable pairs and see how well you were able to identify correlation visually.\n\n::: {.callout-answer collapse=\"true\"}\n## Answer\n\nVisually determining the most negative/positively and uncorrelated pairs of variables:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSAstate <- read_csv(\"data/CS3-statedata.csv\")\n\n# have a look at the data\nUSAstate\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 9\n   state       population income illiteracy life_exp murder hs_grad frost   area\n   <chr>            <dbl>  <dbl>      <dbl>    <dbl>  <dbl>   <dbl> <dbl>  <dbl>\n 1 Alabama           3615   3624        2.1     69.0   15.1    41.3    20  50708\n 2 Alaska             365   6315        1.5     69.3   11.3    66.7   152 566432\n 3 Arizona           2212   4530        1.8     70.6    7.8    58.1    15 113417\n 4 Arkansas          2110   3378        1.9     70.7   10.1    39.9    65  51945\n 5 California       21198   5114        1.1     71.7   10.3    62.6    20 156361\n 6 Colorado          2541   4884        0.7     72.1    6.8    63.9   166 103766\n 7 Connecticut       3100   5348        1.1     72.5    3.1    56     139   4862\n 8 Delaware           579   4809        0.9     70.1    6.2    54.6   103   1982\n 9 Florida           8277   4815        1.3     70.7   10.7    52.6    11  54090\n10 Georgia           4931   4091        2       68.5   13.9    40.6    60  58073\n# ℹ 40 more rows\n```\n:::\n:::\n\n\nWe basically repeat what we've done previously:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSAstate_pear <-USAstate %>% \n  select(-state) %>% \n  cor(method = \"pearson\")\n```\n:::\n\n\nNext, we can plot the data:\n\n::: {.cell}\n\n```{.r .cell-code}\nheatmap(USAstate_pear, symm = TRUE, Rowv = NA)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n## Python\nFirst, we load the data:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nUSAstate_py = pd.read_csv(\"data/CS3-statedata.csv\")\n\nUSAstate_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        state  population  income  illiteracy  ...  murder  hs_grad  frost    area\n0     Alabama        3615    3624         2.1  ...    15.1     41.3     20   50708\n1      Alaska         365    6315         1.5  ...    11.3     66.7    152  566432\n2     Arizona        2212    4530         1.8  ...     7.8     58.1     15  113417\n3    Arkansas        2110    3378         1.9  ...    10.1     39.9     65   51945\n4  California       21198    5114         1.1  ...    10.3     62.6     20  156361\n\n[5 rows x 9 columns]\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# create correlation matrix\nUSAstate_cor_py = USAstate_py.corr()\n# put the row names into a column\nUSAstate_cor_py = USAstate_cor_py.rename_axis(\"var1\").reset_index()\n\nUSAstate_cor_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         var1  population    income  ...   hs_grad     frost      area\n0  population    1.000000  0.208228  ... -0.098490 -0.332152  0.022544\n1      income    0.208228  1.000000  ...  0.619932  0.226282  0.363315\n2  illiteracy    0.107622 -0.437075  ... -0.657189 -0.671947  0.077261\n3    life_exp   -0.068052  0.340255  ...  0.582216  0.262068 -0.107332\n4      murder    0.343643 -0.230078  ... -0.487971 -0.538883  0.228390\n\n[5 rows x 9 columns]\n```\n:::\n:::\n\n\nNow that we have the correlation matrix in a workable format, we need to restructure it so that we can plot the data. For this, we need to create a \"long\" format, using the `melt()` function. Note that we're not setting the `values_var` argument. If not set, then it uses all but the `id_vars` column (which in our case is a good thing, since we don't want to manually specify lots of column names).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nUSAstate_pear_py = pd.melt(USAstate_cor_py,\n        id_vars=['var1'],\n        var_name='var2',\n        value_name='cor').round(3)\n```\n:::\n\n\nHave a look at the structure:\n\n::: {.cell}\n\n```{.python .cell-code}\nUSArrests_pear_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        var1     var2    cor\n0     murder   murder  1.000\n1    assault   murder  0.802\n2  urban_pop   murder  0.070\n3    robbery   murder  0.564\n4     murder  assault  0.802\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(USAstate_pear_py,\n        aes(x = \"var1\", y = \"var2\", fill = \"cor\")) +\n     geom_tile() +\n     geom_text(aes(label = \"cor\"),\n               colour = \"white\",\n               size = 10))\n```\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-32-1.png){width=614}\n:::\n:::\n\n:::\n\nIt looks like:\n\n1.  `illiteracy` and `murder` are the most positively correlated pair\n2.  `life_exp` and `murder` are the most negatively correlated pair\n3.  `population` and `area` are the least correlated pair\n\nWe can explore that numerically, by doing the following:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nFirst, we need to create the pairwise comparisons, with the relevant Pearson's $r$ values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# build a contingency table with as.table()\n# and create a dataframe with as.data.frame()\nUSAstate_pear_cont <- as.data.frame(as.table(USAstate_pear))\n    \n# and have a look\nhead(USAstate_pear_cont)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Var1       Var2        Freq\n1 population population  1.00000000\n2     income population  0.20822756\n3 illiteracy population  0.10762237\n4   life_exp population -0.06805195\n5     murder population  0.34364275\n6    hs_grad population -0.09848975\n```\n:::\n:::\n\n\nIs this method obvious? No! Some creative Googling led to [Stackoverflow](https://stackoverflow.com/questions/7074246/show-correlations-as-an-ordered-list-not-as-a-large-matrix) and here we are. But, it does give us what we need.\n\nNow that we have the paired comparisons, we can extract the relevant data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# first we remove the same-pair correlations\nUSAstate_pear_cont <- USAstate_pear_cont %>% \n  filter(Freq != 1)\n\n# most positively correlated pair\nUSAstate_pear_cont %>% \n  filter(Freq == max(Freq))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Var1       Var2      Freq\n1     murder illiteracy 0.7029752\n2 illiteracy     murder 0.7029752\n```\n:::\n\n```{.r .cell-code}\n# most negatively correlated pair\nUSAstate_pear_cont %>% \n  filter(Freq == min(Freq))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Var1     Var2       Freq\n1   murder life_exp -0.7808458\n2 life_exp   murder -0.7808458\n```\n:::\n\n```{.r .cell-code}\n# least correlated pair\nUSAstate_pear_cont %>% \n  filter(Freq == min(abs(Freq)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Var1       Var2       Freq\n1       area population 0.02254384\n2 population       area 0.02254384\n```\n:::\n:::\n\n\nNote that we use the minimum *absolute* value (with the `abs()` function) to find the least correlated pair.\n\n## Python\nWe take the correlation matrix in the long format:\n\n::: {.cell}\n\n```{.python .cell-code}\nUSAstate_pear_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         var1        var2    cor\n0  population  population  1.000\n1      income  population  0.208\n2  illiteracy  population  0.108\n3    life_exp  population -0.068\n4      murder  population  0.344\n```\n:::\n:::\n\n\nand use it to extract the relevant values:\n\n::: {.cell}\n\n```{.python .cell-code}\n# filter out self-pairs\ndf_cor = USAstate_pear_py.query(\"cor != 1\")\n\n# filter for the maximum correlation value\ndf_cor[df_cor.cor == df_cor.cor.max()]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          var1        var2    cor\n20      murder  illiteracy  0.703\n34  illiteracy      murder  0.703\n```\n:::\n\n```{.python .cell-code}\n# filter for the minimum correlation value\ndf_cor[df_cor.cor == df_cor.cor.min()]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        var1      var2    cor\n28    murder  life_exp -0.781\n35  life_exp    murder -0.781\n```\n:::\n\n```{.python .cell-code}\n# filter for the least correlated value\n# create a column containing absolute values\ndf_cor[\"abs_cor\"] = df_cor[\"cor\"].abs()\ndf_cor[df_cor.abs_cor == df_cor.abs_cor.min()]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          var1        var2    cor  abs_cor\n7         area  population  0.023    0.023\n56  population        area  0.023    0.023\n```\n:::\n:::\n\n:::\n:::\n:::\n\n### Spearman's correlation {#sec-exr_spearman}\n\n:::{.callout-exercise}\n\n\n{{< level 2 >}}\n\n\n\nCalculate Spearman's correlation coefficient for the `data/CS3-statedata.csv` data set.\n\nWhich variable's correlations are affected most by the use of the Spearman's rank compared with Pearson's r? Hint: think of a way to address this question programmatically.\n\nThinking about the variables, can you explain why this might this be?\n\n::: {.callout-answer collapse=\"true\"}\n\n## Answer\n\nIn order to determine which variables are most affected by the choice of Spearman vs Pearson you could just plot both matrices out side by side and try to spot what was going on, but one of the reasons we're using programming languages is that we can be a bit more **programmatic** about these things. Also, our eyes aren't that good at processing and parsing this sort of information display. A better way would be to somehow visualise the data.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nFirst, calculate the Pearson and Spearman correlation matrices (technically, we've done the Pearson one already, but we're doing it again for clarity here).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor_pear <- USAstate %>% \n    select(-state) %>% \n    cor(method = \"pearson\")\n\ncor_spear <- USAstate %>% \n    select(-state) %>% \n    cor(method = \"spearman\")\n```\n:::\n\n\nWe can calculate the difference between two matrices by subtracting them.\n\n::: {.cell}\n\n```{.r .cell-code}\ncor_diff <- cor_pear - cor_spear\n```\n:::\n\n\nAgain, we could now just look at a grid of 64 numbers and see if we can spot the biggest differences, but our eyes aren't that good at processing and parsing this sort of information display. A better way would be to visualise the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nheatmap(abs(cor_diff), symm = TRUE, Rowv = NA)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\nThe `abs()` function calculates the absolute value (i.e. just the magnitude) of the matrix values. This is just because I only care about situations where the two correlation coefficients are different from each other but I don't care which is the larger. The `symm` argument tells the function that we have a symmetric matrix and in conjunction with the `Rowv = NA` argument stops the plot from reordering the rows and columns. The `Rowv = NA` argument also stops the function from adding dendrograms to the margins of the plot.\n\n## Python\n\nFirst, calculate the Pearson and Spearman correlation matrices (technically, we've done the Pearson one already, but we're doing it again for clarity here).\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncor_pear_py = USAstate_py.corr(method = \"pearson\")\ncor_spea_py = USAstate_py.corr(method = \"spearman\")\n```\n:::\n\n\nWe can calculate the difference between two matrices by subtracting them.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncor_dif_py = cor_pear_py - cor_spea_py\n```\n:::\n\n\nAgain, we could now just look at a grid of 64 numbers and see if we can spot the biggest differences, but our eyes aren't that good at processing and parsing this sort of information display. A better way would be to visualise the data.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# get the row names in a column\ncor_dif_py = cor_dif_py.rename_axis(\"var1\").reset_index()\n\n# reformat the data into a long format\n# and round the values\ncor_dif_py = pd.melt(cor_dif_py,\n        id_vars=['var1'],\n        var_name='var2',\n        value_name='cor').round(3)\n        \n# create a column with absolute correlation difference values\ncor_dif_py[\"abs_cor\"] = cor_dif_py[\"cor\"].abs()\n\n# have a look at the final data frame\ncor_dif_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         var1        var2    cor  abs_cor\n0  population  population  0.000    0.000\n1      income  population  0.084    0.084\n2  illiteracy  population -0.205    0.205\n3    life_exp  population  0.036    0.036\n4      murder  population -0.002    0.002\n```\n:::\n:::\n\n\nNow we can plot the data:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(cor_dif_py,\n        aes(x = \"var1\", y = \"var2\", fill = \"abs_cor\")) +\n     geom_tile() +\n     geom_text(aes(label = \"abs_cor\"),\n               colour = \"white\",\n               size = 10))\n```\n\n::: {.cell-output-display}\n![](cs3_practical_correlations_files/figure-html/unnamed-chunk-43-1.png){width=614}\n:::\n:::\n\n:::\n\nAll in all there is not a huge difference in correlation coefficients, since the values are all quite small. Most of the changes occur along the `area` variable. One possible explanation could be that certain states with a large area have a relatively large effect on the Pearson's r coefficient. For example, Alaska has an area that is over twice as big as the next state - Texas.\n\nIf, for example, we'd look a bit closer then we would find for `area` and `income` that Pearson gives a value of 0.36, a slight positive correlation, whereas Spearman gives a value of 0.057, basically uncorrelated.\n\nThis means that this is basically ignored by Spearman.\n\nWell done, [Mr. Spearman](https://en.wikipedia.org/wiki/Charles_Spearman).\n:::\n:::\n\n## Summary\n\n::: {.callout-tip}\n#### Key points\n\n-   Correlation is the degree to which two variables are linearly related\n-   Correlation does not imply causation\n-   We can visualise correlations by plotting variables against each other or creating heatmap-type plots of the correlation coefficients\n-   Two main correlation coefficients are Pearson's r and Spearman's rank, with Spearman's rank being less sensitive to outliers\n:::\n",
    "supporting": [
      "cs3_practical_correlations_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}