{
  "hash": "e554fdd08b53b1f00d43c903913d1217",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Power analysis\"\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-tip}\n#### Learning outcomes\n\n**Questions**\n\n- What is power analysis?\n- How can I use power analysis to design better experiments?\n\n**Objectives**\n\n- Be able to perform power analysis programmatically\n- Understand the importance of effect size\n- Use power, significance level and effect size to optimise your experimental design\n\n:::\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n### Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A library for power analysis\nlibrary(pwr)\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n```\n:::\n\n\n### Functions\n\n## Python\n\n### Libraries\n### Functions\n:::\n:::\n\n## Background\nAll hypothesis tests can be wrong in two ways:\n\n* we can appear to have found a significant result when there really isnâ€™t anything there: a false positive (or Type I error), or\n* we can fail to spot a significant result when there really is something interesting going on: a false negative (or Type II error).\n\nThe probability of getting a false positive in our analysis is precisely the significance level we use in our analysis. So, in order to reduce the likelihood of getting a false positive we simply reduce the significance level of our test (from 0.05 down to 0.01 say). Easy as that.\n\nUnfortunately, this has unintended consequences (doesnâ€™t everything?). It turns out that reducing the significance level means that we increase the chance of getting false negatives. This should make sense; if weâ€™re increasing the barrier to entry in terms of acceptance then weâ€™ll also accidentally miss out on some of the good stuff.\n\n**Power** is the capacity of a test to detect significant different results. It is affected by three things:\n\n1. the effect size: i.e. how big of a difference do you want to be able to detect, or alternatively what do you consider a meaningful effect/difference to be?\n2. sample size\n3. the significance level\n\nIn an ideal world we would want to be carrying out highly powerful tests using low significance levels, to both reduce our chance of getting a false positive and maximise our chances of finding a true effect.\n\n**Power analysis** allows us to design experiments to do just that. Given:\n\n* a desired power (0.8 or 80% is considered pretty good)\n* a significance level (0.05 or 5% is our trusty yet arbitrary steed once again)\n* an effect size that we would like to detect\n\nWe can calculate the amount of data that we need to collect in our experiments. (Woohoo! it looks as if statistics will actually give us an answer at last rather than these perpetual shades-of-grey \"maybes\").\n\nThe reality is that most of the easily usable power analysis functions all operate under the assumption that the data that you will collect will meet all of the assumptions of your chosen statistical test perfectly. So, for example, if you want to design an experiment investigating the effectiveness of a single drug compared to a placebo (so a simple t-test) and you want to know how many patients to have in each group in order for the test to work, then the standard power analysis techniques will still assume that all of the data that you end up collecting will meet the assumptions of the t-test that you have to carry out (sorry to have raised your hopes ever so slightly ðŸ˜‰).\n\n### Effect size\n\nAs we shall see the commands for carrying out power analyses are very simple to implement apart from the concept of effect size. This is a tricky issue for most people to get to grips with for two reasons:\n\n1. Effect size is related to biological significance rather than statistical significance\n2. The way in which we specify effect sizes\n\n::: {.callout-note}\n\nWith respect to the first point a common conversation goes a bit like this:\n\nme: \"So youâ€™ve been told to carry out a power analysis, eh? Lucky you. What sort of effect size are you looking for?\"\n\nyou: \"I have no idea what youâ€™re talking about. I want to know if my drug is any better than a placebo. How many patients do I need?\"\n\nme:\t\"It depends on how big a difference you think your drug will have compared to the placebo.\"\n\nyou: \"I havenâ€™t carried out my experiment yet, so I have absolutely no idea how big the effect will be!\"\n\nme:\t<sigh>\n\n(To be honest this would be a relatively well-informed conversation: [this is much closer](https://www.youtube.com/watch?v=Hz1fyhVOjr4) to how things actually go)\n:::\n\nThe key point about effect sizes and power analyses is that you need to specify an effect size that you would be interested in observing, or one that would be biologically relevant to see. There may well actually be a 0.1% difference in effectiveness of your drug over a placebo but designing an experiment to detect that would require markedly more individuals than an experiment that was trying to detect a 50% difference in effectiveness. In reality there are three places we can get a sense of effect sizes from:\n\n1. A pilot study\n2. Previous literature or theory\n3. Jacob Cohen\n\n[Jacob Cohen](https://en.wikipedia.org/wiki/Jacob_Cohen_(statistician)) was an American statistician who developed a large set of measures for effect sizes (which we will use today). He came up with a rough set of numerical measures for \"small\", \"medium\" and \"large\" effect sizes that are still in use today. These do come with some caveats though; Jacob was a psychologist and so his assessment of what was a large effect may be somewhat different from yours. They do form a useful starting point however.\n\nThere a lot of different ways of specifying effects sizes, but we can split them up into three distinct families of estimates:\n\n1. **Correlation estimates**: these use $R^2$ as a measure of variance explained by a model (for linear models, anova etc. A large $R^2$ value would indicate that a lot of variance has been explained by our model and we would expect to see a lot of difference between groups, or a tight cluster of points around a line of best fit. The argument goes that we would need fewer data points to observe such a relationship with confidence. Trying to find a relationship with a low $R^2$ value would be trickier and would therefore require more data points for an equivalent power.\n2. **Difference between means**: these look at how far apart the means of two groups are, measured in units of standard deviations (for t-tests). An effect size of 2 in this case would be interpreted as the two groups having means that were two standard deviations away from each other (quite a big difference), whereas an effect size of 0.2 would be harder to detect and would require more data to pick it up.\n3. **Difference between count data**: these I freely admit I have no idea how to intuitively explain them (shock, horror). Mathematically they are based on the chi-squared statistic but thatâ€™s as good as I can tell you Iâ€™m afraid. They are, however, pretty easy to calculate.\n\nFor reference here are some of Cohenâ€™s suggested values for effect sizes for different tests. Youâ€™ll probably be surprised by how small some of these are.\n\n| Test| Small | Medium | Large |\n|:- |:- |:- |:- |\n|t-tests| 0.2 | 0.5 | 0.8 |\n|anova | 0.1 | 0.25 | 0.4 |\n|linear models | 0.02 | 0.15 | 0.35 |\n|chi-squared | 0.1 | 0.3 | 0.5 |\n\nWe will look at how to carry out power analyses and estimate effect sizes in this section.\n\n## Power analysis t-test\n\nThe first example we'll look at is how to perform a power analysis on two groups of data.\n\nLetâ€™s assume that we want to design an experiment to determine whether there is a difference in the mean price of what male and female students pay at a cafe. How many male and female students would we need to observe in order to detect a \"medium\" effect size with 80% power and a significance level of 0.05?\n\nWe first need to think about which test we would use to analyse the data. Here we would have two groups of continuous response. Clearly a t-test.\n\n### Determine effect size\n\nThe first thing we need to do is figure out what a \"medium\" effect size is. In absence of any further information we refer back to Cohen's effect sizes.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nWe're using the `pwr` library, so make sure that you have installed and loaded it with the following commands:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install pwr package if needed\ninstall.packages(\"pwr\")\n\n# load the pwr package\nlibrary(pwr)\n```\n:::\n\n\nWe can get Cohen's effect size using the `cohen.ES()` function (`ES` stands for Effect Size):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncohen.ES(test = \"t\", size = \"medium\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Conventional effect size from Cohen (1982) \n\n           test = t\n           size = medium\n    effect.size = 0.5\n```\n\n\n:::\n:::\n\n\nThis function just returns the default conventional values for effect sizes as determined by Jacob Cohen back in the day. It just saves us scrolling back up the page to look at the table I provided. It only takes two arguments:\n\n*\ttest which is one of\n    * \"t\", for t-tests,\n    * \"anova\" for anova,\n    * \"f2\" for linear models\n    * \"chisq\" for chi-squared test\n* size, which is just one of \"small\", \"medium\" or \"large\".\n\nThe bit we want is on the bottom line; we apparently want an effect size of 0.5.\n\n## Python\n\nUnlike in R, Cohen's effect sizes are not available through a package (that I am aware of). So in this case we're referring back to the effect size table we saw earlier and define \"medium\" as 0.5.\n:::\n\nFor this sort of study effect size is measured in terms of Cohenâ€™s d statistic. This is simply a measure of how different the means of the two groups are expressed in terms of the number of standard deviations they are apart from each other. So, in this case weâ€™re looking to detect two means that are 0.5 standard deviations away from each other. In a minute weâ€™ll look at what this means for real data.\n\n### Calculating sample sizes\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nWe do this as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.t.test(d = 0.5, sig.level = 0.05, power = 0.8,\n           type = \"two.sample\", alternative = \"two.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 63.76561\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\nThe first line is what weâ€™re looking for `n = 63.76` tells that we need 64 (rounding up) students in each group (so 128 in total) in order to carry out this study with sufficient power. The other lines should be self-explanatory (well they should be by this stage; if you need me to tell you that the function is just returning the values that youâ€™ve just typed in then you have bigger problems to worry about).\n\nThe `pwr.t.test()` function has six arguments. Two of them specify what sort of t-test youâ€™ll be carrying out\n* `type`; which describes the type of t-test you will eventually be carrying out (one of `two.sample`, `one.sample` or `paired`), and\n* `alternative`; which describes the type of alternative hypothesis you want to test (one of `two.sided`, `less` or `greater`)\n\nThe other four arguments are what is used in the power analysis:\n\n* `d`; this is the effect size, a single number calculated using Cohenâ€™s d statistic.\n* `sig.level`; this is the significance level\n* `power`; is the power\n* `n`; this is the number of observations per sample.\n\n## Python\n\nWe do this with the `power_ttest()` function from `pingouin`:\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.power_ttest(d = 0.5,\n               alpha = 0.05,\n               power = 0.80)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n63.76561137745676\n```\n\n\n:::\n:::\n\n\nThe output `n = 63.76` tells that we need 64 (rounding up) students in each group (so 128 in total) in order to carry out this study with sufficient power.\n\nThe `power_ttest()` function has six arguments. Two of them specify what sort of t-test youâ€™ll be carrying out\n* `contrast`; which describes the type of t-test you will eventually be carrying out (one of `one-sample`, `two-samples` or `paired`), and\n* `alternative`; which describes the type of alternative hypothesis you want to test. These can be `\"two-sided\"` (default), `\"greater\"` or `\"less\"`\n\nThe other four arguments are what is used in the power analysis:\n\n* `d`; this is the effect size, a single number calculated using Cohenâ€™s d statistic.\n* `alpha`; this is the significance level (default is `0.05`)\n* `power`; is the power\n* `n`; this is the number of observations per sample.\n:::\n\nThe function works by allowing you to specify any three of these four arguments and the function works out the fourth. In the example above we have used the test in the standard fashion by specifying power, significance and desired effect size and getting the function to tell us the necessary sample size.\n\n### Calculating effect sizes\n\nWe can use the function to answer a different question:\n\n> If I know in advance that I can only observe 30 students per group, what is the effect size that I should be able to observe with 80% power at a 5% significance level?\n\nLet's see how we do this:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.t.test(n = 30, sig.level = 0.05, power = 0.8,\n           type = \"two.sample\", alternative = \"two.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 30\n              d = 0.7356292\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.power_ttest(n = 30,\n               alpha = 0.05,\n               power = 0.80,\n               contrast = \"two-samples\",\n               alternative = \"two-sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.73562107868047\n```\n\n\n:::\n:::\n\n\n:::\n\nThis time we want to see what the effect size is so we look at the second line and we can see that an experiment with this many people would only be expected to detect a difference in means of `d = 0.74` standard deviations. Is this good or bad? Well, it depends on the natural variation of your data; if your data is really noisy then it will have a large variation and a large standard deviation which will mean that 0.74 standard deviations might actually be quite a big difference between your groups. If on the other hand your data doesnâ€™t vary very much, then 0.74 standard deviations might actually be a really small number and this test could pick up even quite small differences in mean.\n\n## Power analysis on data\n\nIn both of the previous two examples we were a little bit context-free in terms of effect size. Letâ€™s look at how we can use a pilot study with real data to calculate effect sizes and perform a power analysis to inform a future study.\n\nLetâ€™s look again at the `fishlength` data we saw in the first practical relating to the lengths of fish from two separate rivers. \nThis is saved as `data/CS1-twosample.csv`.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read in the data\nfishlength <- read_csv(\"data/CS1-twosample.csv\")\n\n# visualise the data\nfishlength %>% \n  ggplot(aes(x = river, y = length)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](cs6_practical_power-analysis_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nFrom the plot we can see that the groups appear to have different means. This difference is significant, as per a two-sample t-test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# perform t-test\nt.test(length ~ river,\n       data = fishlength,\n       var.equal = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  length by river\nt = 3.8433, df = 66, p-value = 0.0002754\nalternative hypothesis: true difference in means between group Aripo and group Guanapo is not equal to 0\n95 percent confidence interval:\n 0.9774482 3.0909868\nsample estimates:\n  mean in group Aripo mean in group Guanapo \n             20.33077              18.29655 \n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# read in the data\nfishlength_py = pd.read_csv(\"data/CS1-twosample.csv\")\n\n# visualise the data\n(ggplot(fishlength_py, aes(x = \"river\", y = \"length\")) +\n     geom_boxplot())\n```\n\n::: {.cell-output-display}\n![](cs6_practical_power-analysis_files/figure-html/unnamed-chunk-12-1.png){width=614}\n:::\n:::\n\n\nFrom the plot we can see that the groups appear to have different means. This difference is significant, as per a two-sample t-test.\n\nThe `ttest()` function in `pingouin` needs two vectors as input, so we split the data as follows:\n\n\n::: {.cell}\n\n```{.python .cell-code}\naripo = fishlength_py.query('river == \"Aripo\"')[\"length\"]\nguanapo = fishlength_py.query('river == \"Guanapo\"')[\"length\"]\n```\n:::\n\n\nNext, we perform the t-test. We specify that the variance are equal by setting `correction = False`. We also `transpose()` the data, so we can actually see the entire output.\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.ttest(aripo, guanapo,\n         correction = False).transpose()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   T-test\nT                3.843267\ndof                    66\nalternative     two-sided\np-val            0.000275\nCI95%        [0.98, 3.09]\ncohen-d          0.942375\nBF10               92.191\npower            0.966135\n```\n\n\n:::\n:::\n\n\n:::\n\nCan we use this information to design a more efficient experiment? One that we would be confident was powerful enough to pick up a difference in means as big as was observed in this study but with fewer observations?\n\nLetâ€™s first work out exactly what the effect size of this previous study really was by estimating Cohenâ€™s d using this data.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nTo this, we use the `cohens_d` function from the `rstatix` package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncohens_d(length ~ river,\n         var.equal = TRUE,\n         data = fishlength)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 7\n  .y.    group1 group2  effsize    n1    n2 magnitude\n* <chr>  <chr>  <chr>     <dbl> <int> <int> <ord>    \n1 length Aripo  Guanapo   0.942    39    29 large    \n```\n\n\n:::\n:::\n\n\nThe `cohens_d()` function calculates the effect size using the formula of the test. The `effsize` column contains the information that we want, in this case 0.94 .\n\nWe can now actually answer our question and see how many fish we really need to catch in the future:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.t.test(d = 0.94, power = 0.8, sig.level = 0.05,\n           type = \"two.sample\", alternative = \"two.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 18.77618\n              d = 0.94\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n## Python\n\nTo do this, we use the `compute_effsize()` function from `pingouin`. This takes two vectors as input, so we use the `aripo` and `guanapo` objects we created earlier:\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.compute_effsize(aripo, guanapo,\n                   paired = False,\n                   eftype = \"cohen\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.9423748389254938\n```\n\n\n:::\n:::\n\n\nNote: the `compute_effsize()` function is able to compute various effect sizes, but we're specifying Cohen's d here.\n\nSo, the Cohen's d value for these data are d = 0.94 .\n\nWe can now actually answer our question and see how many fish we really need to catch in the future:\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.power_ttest(d = 0.94,\n               alpha = 0.05,\n               power = 0.80,\n               contrast = \"two-samples\",\n               alternative = \"two-sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n18.776177961938313\n```\n\n\n:::\n:::\n\n\n:::\n\nFrom this we can see that any future experiments would really only need to use 19 fish for each group (we always round this number up, so no fish will be harmed during the experiment...) if we wanted to be confident of detecting the difference we observed in the previous study.\n\nThis approach can also be used when the pilot study showed a smaller effect size that wasnâ€™t observed to be significant (indeed arguably, a pilot study shouldnâ€™t really concern itself with significance but should only really be used as a way of assessing potential effect sizes which can then be used in a follow-up study).\n\n## Linear model power calculations\n\nThankfully the ideas weâ€™ve covered in the t-test section should see us in good stead going forward and I can stop writing everything out in such excruciating detail (I do have other things to do you know...).\n\nLet's read in `data/CS2-lobsters.csv`. This data set was used in an earlier practical and describes the effect of three different food sources on lobster weight .\n\nAs a quick reminder we'll also plot the data and perform an ANOVA:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read in the data\nlobsters <- read_csv(\"data/CS2-lobsters.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# visualise the data\nggplot(lobsters,\n       aes(x = diet, y = weight)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](cs6_practical_power-analysis_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# define the linear model\nlm_lobster <- lm(weight ~ diet,\n                 data = lobsters)\n\n# perform ANOVA on model\nanova(lm_lobster)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: weight\n          Df Sum Sq Mean Sq F value Pr(>F)\ndiet       2 1567.2  783.61  1.6432 0.2263\nResiduals 15 7153.1  476.87               \n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# read in the data\nlobsters_py = pd.read_csv(\"data/CS2-lobsters.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# visualise the data\n(ggplot(lobsters_py, aes(x = \"diet\",\n                         y = \"weight\")) +\n     geom_boxplot())\n```\n\n::: {.cell-output-display}\n![](cs6_practical_power-analysis_files/figure-html/unnamed-chunk-23-1.png){width=614}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula = \"weight ~ C(diet)\", data = lobsters_py)\n# and get the fitted parameters of the model\nlm_lobsters_py = model.fit()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# perform the anova on the fitted model\nsm.stats.anova_lm(lm_lobsters_py)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            df       sum_sq     mean_sq        F    PR(>F)\nC(diet)    2.0  1567.229381  783.614690  1.64324  0.226313\nResidual  15.0  7153.075619  476.871708      NaN       NaN\n```\n\n\n:::\n:::\n\n\n:::\n\n* the box plot shows us that there might well be some differences between groups\n* the ANOVA analysis though shows that there isnâ€™t sufficient evidence to support that claim given the insignificant p-value we observe.\n\nSo the question we can ask is:\n\n> If there really is a difference between the different food sources as big as appears here, how big a sample would we need in order to be able to detect it statistically?\n\nFirst letâ€™s calculate the observed effect size from this study.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nFor linear models the effect size is called Cohenâ€™s $f^2$. We can calculate it easily by using the $R^2$ value from the model fit and shoving it in the following formula:\n\n\\begin{equation}\nf^2 = \\frac{R^2}{1-R^2}\n\\end{equation}\n\nWe find $R^2$ from the `lm_lobster` summary. We can either just look at the results (spoiler alert, the $R^2$ is 0.1797) and add it manually or extract the value with the `broom::glance()` function.\n\nEither way, we can calculate Cohen's $f^2$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get the effect size for ANOVA\nR2 <- summary(lm_lobster) %>%\n    glance() %>% \n    pull(r.squared)\n\n# calculate Cohen's f2\nR2 / (1 - R2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2190987\n```\n\n\n:::\n:::\n\n\nSo now we've got Cohen's $f^2$. \n\n## Python\n\nFor linear models the effect size metric we use is called $\\eta^2$, or eta-squared.\n\nThe eta-squared value measures the contribution of the individual model terms. This is closely linked to the $R^2$ value, which measures the total amount of variation that is explained by the entire model.\n\nSince we only have one model term here (`diet`), the $R^2$ and $\\eta^2$ values are the same.\n\nWe can get the $R^2$  (0.1797) value from the model as follows:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# get the R2 value\nR2 = lm_lobsters_py.rsquared\n```\n:::\n\n:::\n\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nThereâ€™s one more thing that we need for the power calculation for a linear model; the degrees of freedom.\n\nWe have two different degrees of freedom: the **numerator degrees of freedom** and the **denominator degrees of freedom**. Here the numerator degrees of freedom is 2. This is the number that we want. It is simply the number of parameters in the model minus 1. In this model there are three parameters for the three groups, so 3 - 1 = 2 (see the math isnâ€™t too bad). The other number is called the denominator degrees of freedom, which in this case is 15. This is actually the number we want the power analysis to calculate as itâ€™s a proxy for the number of observations used in the model, and weâ€™ll see how in a minute.\n\nThe degrees of freedom are mentioned at the bottom of the model summary:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm_lobster)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = weight ~ diet, data = lobsters)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.129 -16.155  -4.279  15.195  46.720 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  114.433      8.915  12.836 1.71e-09 ***\ndietMussels   21.895     12.149   1.802   0.0916 .  \ndietPellets   14.047     13.223   1.062   0.3049    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.84 on 15 degrees of freedom\nMultiple R-squared:  0.1797,\tAdjusted R-squared:  0.07035 \nF-statistic: 1.643 on 2 and 15 DF,  p-value: 0.2263\n```\n\n\n:::\n:::\n\n\nSo, we now want to run a power analysis for this linear model, using the following information:\n\n* power = 0.8\n* significance = 0.05\n* effect size = 0.219\n* numerator DF = 2\n\nWe can feed this into the `pwr.f2.test()` function, where we use\n\n* `u` to represent the numerator DF value\n* `f2` to represent Cohenâ€™s $f^2$ effect size value\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.f2.test(u = 2, f2 = 0.219,\n            sig.level = 0.05, power = 0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Multiple regression power calculation \n\n              u = 2\n              v = 44.12292\n             f2 = 0.219\n      sig.level = 0.05\n          power = 0.8\n```\n\n\n:::\n:::\n\n\nAs before most of these numbers are just what youâ€™ve put into the function yourself. The new number is `v`. This is the denominator degrees of freedom required for the analysis to have sufficient power. Thankfully this number is related to the number of observations that we should use in a straightforward manner:\n\n$number\\:of\\:observations = u + v + 1$\n\nSo in our case we would ideally have 48 observations (45 + 2 + 1, remembering to round up) in our experiment.\n\nThe most challenging part for using power analyses for linear models is working out what the numerator degrees of freedom should be. The easiest way of thinking about it is to say that itâ€™s the number of parameters in your model, excluding the intercept. If you look back at how we wrote out the linear model equations, then you should be able to see how many non-zero parameters would be expected. For some of the simple cases the table below will help you, but for complex linear models you will need to write out the linear model equation and count parameters (sorry!).\n\n| Test| u|\n|:- |:- |\n|one-way ANOVA| no. of groups - 1 |\n|simple linear regression| 1 |\n|two-way ANOVA with interaction| no. of groups (`v1`) x no. of groups (`v2`) - 1 |\n|two-way ANOVA without interaction| no. of groups (`v1`) + no. of groups (`v2`) - 2 |\n|ANCOVA with interaction| 2 x no. of groups â€“ 1 |\n|ANCOVA without interaction| no. of groups |\n\n## Python\n\nWe can use the eta-squared value in the `power_anova()` function from `pingouin`.\n\nIf we're trying to figure out the sample size, we need to give it the following information:\n\n- `eta_squared`, the effect size we're after (we saved this as `R2`)\n- `k`, the number of groups (three, in our case)\n- `power`, the statistical power we're after, in this case 80%\n- `alpha`, the significance threshold at which we want to detect it\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.power_anova(eta_squared = R2, k = 3, power = 0.80, alpha = 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n15.701046156535305\n```\n\n\n:::\n:::\n\n\nWhen we fill in all that information, then we find that we need 15.701 samples _per group_ - rounding up this gives 16. This means that we need a total of $16 \\times 3 = 48$ samples altogether.\n:::\n\nThere are two questions you might now ask (if youâ€™re still following all of this that is â€“ youâ€™re quite possibly definitely in need of a coffee by now):\n\n1. how many observations should go into each group?\n    * ideally they should be equally distributed (so in this case 16 per group).\n2. why is this so complicated, why isnâ€™t there just a single function that just does this, and just tells me how many observations I need?\n    * Very good question â€“ I have no answer to that sorry â€“ sometimes life is just hard.\n\n\n::: .{callout-note}\n## R-squared and eta-squared\n\nLike we've seen before in the previous sessions, the $R^2$ value can give us an indication of how much of the variance is explained by our model.\n\nSometimes you also come across $\\eta^2$. What that does is that it partitions $R^2$ across the predictors. This means that $\\eta^2$ represents how much variance is explained by each of the predictors. If you have multiple predictors, then you would get multiple values.\n\nIn the case where there is *one* predictor, $R^2 = \\eta^2$.\n:::\n\n## Exercises\n\n### Power: one-sample {#sec-exr_pwronesample}\n\n:::{.callout-exercise}\n\n\n{{< level 2 >}}\n\n\n\nPerforming a power analysis on a one-sample data set\n\nLoad in `data/CS1-onesample.csv` (this is the same data we looked at in the earlier practical containing information on fish lengths from the Guanapo river).\n\na. Assume this was a pilot study and analyse the data using a one-sample t-test to see if there is any evidence that the mean length of fish differs from 20 mm.\nb. Use the results of this analysis to estimate the effect size.\nc. Work out how big a sample size would be required to detect an effect this big with power 0.8 and significance 0.05.\nd. How would the sample size change if we wanted 0.9 power and significance 0.01?\n\n::: {.callout-answer collapse=\"true\"}\n## Answer\n::: {.panel-tabset group=\"language\"}\n## R\n\nFirst, read in the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfish_data <- read_csv(\"data/CS1-onesample.csv\")\n```\n:::\n\n\nLet's run the one-sample t-test as we did before:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(length ~ 1,\n       mu = 20,\n       alternative = \"two.sided\",\n       data = fish_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  length\nt = -3.5492, df = 28, p-value = 0.001387\nalternative hypothesis: true mean is not equal to 20\n95 percent confidence interval:\n 17.31341 19.27969\nsample estimates:\nmean of x \n 18.29655 \n```\n\n\n:::\n:::\n\n\n## Python\n\nFirst, read in the data:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfish_data_py = pd.read_csv(\"data/CS1-onesample.csv\")\n```\n:::\n\n\nLet's run the one-sample t-test as we did before:\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.ttest(x = fish_data_py.length,\n         y = 20,\n         alternative = \"two-sided\").transpose()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                     T-test\nT                 -3.549184\ndof                      28\nalternative       two-sided\np-val              0.001387\nCI95%        [17.31, 19.28]\ncohen-d            0.659067\nBF10                 25.071\npower               0.92855\n```\n\n\n:::\n:::\n\n\n:::\n\nThere does appear to be a statistically significant result here; the mean length of the fish appears to be different from 20 mm.\n\nLet's calculate the effect size using these data. This gives us the following output for the effect size in terms of the Cohen's d metric.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncohens_d(length ~ 1,\n         mu = 20,\n         data = fish_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 6\n  .y.    group1 group2     effsize     n magnitude\n* <chr>  <chr>  <chr>        <dbl> <int> <ord>    \n1 length 1      null model  -0.659    29 moderate \n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.compute_effsize(x = fish_data_py.length,\n                   y = 20,\n                   paired = False,\n                   eftype = \"cohen\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n-0.6590669150482831\n```\n\n\n:::\n:::\n\n:::\n\nOur effect size is -0.66 which is a moderate effect size. This is pretty good and it means we might have been able to detect this effect with fewer samples.\n\n::: {.callout-important}\nAlthough the effect size here is negative, it does not matter in terms of the power calculations whether it's negative or positive.\n:::\n\nSo, let's do the power analysis to actually calculate the minimum sample size required:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.t.test(d = -0.6590669, sig.level = 0.05, power = 0.8,\n           type = \"one.sample\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     One-sample t test power calculation \n\n              n = 20.07483\n              d = 0.6590669\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.power_ttest(d = -0.6590669,\n               alpha = 0.05,\n               power = 0.80,\n               contrast = \"one-sample\",\n               alternative = \"two-sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n20.074833996884752\n```\n\n\n:::\n:::\n\n:::\n\nWe would need 21 (you round up the n value) observations in our experimental protocol in order to be able to detect an effect size this big (small?) at a 5% significance level and 80% power. Let's see what would happen if we wanted to be even more stringent and calculate this at a significance level of 1%:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.t.test(d = -0.6590669, sig.level = 0.01, power = 0.9,\n           type = \"one.sample\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     One-sample t test power calculation \n\n              n = 37.62974\n              d = 0.6590669\n      sig.level = 0.01\n          power = 0.9\n    alternative = two.sided\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.power_ttest(d = -0.6590669,\n               alpha = 0.01,\n               power = 0.80,\n               contrast = \"one-sample\",\n               alternative = \"two-sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n30.25402339849718\n```\n\n\n:::\n:::\n\n:::\n\nThen we'd need quite a few more observations! We would need to do a bit more work if we wanted to work to this level of significance and power. Are such small differences in fish length biologically meaningful?\n:::\n:::\n\n### Power: two-sample paired {#sec-exr_pwrtwosamplepaired}\n\n:::{.callout-exercise}\n\n\n{{< level 2 >}}\n\n\n\nPower analysis on a paired two-sample data set\n\nLoad in `data/CS1-twopaired.csv` (again this is the same data that we used in an earlier practical and relates to cortisol levels measured on 20 participants in the morning and evening).\n\na. first carry out a power analysis to work out how big of an effect size this experiment should be able to detect at a power of 0.8 and significance level of 0.05. Donâ€™t look at the data just yet!\nb. Now calculate the actual observed effect size from the study.\nc. If you were to repeat the study in the future, how many observations would be necessary to detect the observed effect with 80% power and significance level 0.01?\n\n::: {.callout-answer collapse=\"true\"}\n## Answer\n::: {.panel-tabset group=\"language\"}\n## R\n\nFirst, read in the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncortisol <- read_csv(\"data/CS1-twopaired.csv\")\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncortisol_py = pd.read_csv(\"data/CS1-twopaired.csv\")\n```\n:::\n\n:::\n\nWe have a paired data set with 20 pairs of observations, what sort of effect size could we detect at a significance level of 0.05 and power of 0.8?\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.t.test(n = 20, sig.level = 0.05, power = 0.8,\n           type = \"paired\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Paired t test power calculation \n\n              n = 20\n              d = 0.6604413\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.power_ttest(n = 20,\n               alpha = 0.05,\n               power = 0.80,\n               contrast = \"paired\",\n               alternative = \"two-sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.660441660152974\n```\n\n\n:::\n:::\n\n:::\n\nRemember that we get effect size measured in Cohen's d metric. So here this experimental design would be able to detect a d value of 0.66, which is a medium to large effect size.\n\nNow let's look at the actual data and work out what the effect size actually is.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncohens_d(cortisol ~ time,\n         paired = TRUE,\n         data = cortisol)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 7\n  .y.      group1  group2  effsize    n1    n2 magnitude\n* <chr>    <chr>   <chr>     <dbl> <int> <int> <ord>    \n1 cortisol evening morning   -1.16    20    20 large    \n```\n\n\n:::\n:::\n\n\n## Python\n\nTo do this, we need reformat our data a bit:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncortisol_wide_py = pd.pivot(cortisol_py, index = \"patient_id\", columns = \"time\", values = \"cortisol\")\n\ncortisol_wide_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntime        evening  morning\npatient_id                  \n1             273.2    310.6\n2              65.7    146.1\n3             256.6    297.0\n4             321.0    270.9\n5              80.3    267.5\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npg.compute_effsize(x = cortisol_wide_py.evening,\n                   y = cortisol_wide_py.morning,\n                   paired = False,\n                   eftype = \"cohen\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n-1.434358623934538\n```\n\n\n:::\n:::\n\n\n:::\n\nThis value is a massive effect size. It's quite likely that we actually have more participants in this study than we actually need given such a large effect. Let calculate how many individuals we would actually need:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.t.test(d = -1.159019, sig.level = 0.01, power = 0.8,\n           type = \"paired\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Paired t test power calculation \n\n              n = 12.10628\n              d = 1.159019\n      sig.level = 0.01\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n```\n\n\n:::\n:::\n\n\nSo we would have only needed 13 pairs of participants in this study given the size of effect we were trying to detect.\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.power_ttest(d = -1.434359,\n               alpha = 0.01,\n               power = 0.80,\n               contrast = \"paired\",\n               alternative = \"two-sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n9.094695334046879\n```\n\n\n:::\n:::\n\n\nSo we would have only needed 10 pairs of participants in this study given the size of effect we were trying to detect.\n\n:::\n:::\n:::\n\n### Mussel muscles {#sec-exr_musslemuscles}\n\n:::{.callout-exercise}\n\n\n{{< level 2 >}}\n\n\n\nIn this exercise we're going to determine a required sample size, dependent on a calculated effect size. The file `data/CS6-shelllength.csv` contains information from a pilot study looking at whether the standardised length of the anterior adductor muscle scar in the mussel _Mytilus trossulus_ differs across five locations around the world (well it might be of interest to someone...).\n\nFind the effect size from this study and perform a power calculation (at 0.8 and 0.05 significance level) to determine how many mussel muscles need to be recorded in order to be confident that an effect really exists.\n\n::: {.callout-answer collapse=\"true\"}\n## Answer\n\nLet's first load in the data and have a look at them.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmussels <- read_csv(\"data/CS6-shelllength.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mussels,\n       aes(x = location,\n           y = length)) +\n    geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](cs6_practical_power-analysis_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmussels_py = pd.read_csv(\"data/CS6-shelllength.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(mussels_py,\n        aes(x = \"location\",\n            y = \"length\")) +\n    geom_boxplot())\n```\n\n::: {.cell-output-display}\n![](cs6_practical_power-analysis_files/figure-html/unnamed-chunk-53-1.png){width=614}\n:::\n:::\n\n\n:::\n\nSo we are effectively looking at a one-way ANOVA with five groups. This will be useful to know later.\n\nNow we fit a linear model, and perform our calculations:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define the model\nlm_mussels <- lm(length ~ location,\n                data = mussels)\n\n# summarise the model\nsummary(lm_mussels)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = length ~ location, data = mussels)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.025400 -0.007956  0.000100  0.007000  0.031757 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         0.078012   0.004454  17.517  < 2e-16 ***\nlocationNewport    -0.003213   0.006298  -0.510  0.61331    \nlocationPetersburg  0.025430   0.006519   3.901  0.00043 ***\nlocationTillamook   0.002187   0.005975   0.366  0.71656    \nlocationTvarminne   0.017687   0.006803   2.600  0.01370 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0126 on 34 degrees of freedom\nMultiple R-squared:  0.4559,\tAdjusted R-squared:  0.3918 \nF-statistic: 7.121 on 4 and 34 DF,  p-value: 0.0002812\n```\n\n\n:::\n:::\n\n\nFrom this we can see that the R-squared value is 0.4559. We can extract that as follows and then use it to calculate Cohen's $f^2$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get the effect size for ANOVA\nR2 <- summary(lm_mussels) %>%\n    glance() %>% \n    pull(r.squared)\n\n# calculate Cohen's f2\nf2 <- R2 / (1 - R2)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nf2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.837767\n```\n\n\n:::\n:::\n\n\nNow, our model has 5 parameters (because we have 5 groups) and so the numerator degrees of freedom will be 4 $(5 - 1 = 4)$. This means that we can now carry our our power analysis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.f2.test(u = 4, f2 = 0.837767,\n            sig.level = 0.05 , power = 0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Multiple regression power calculation \n\n              u = 4\n              v = 14.62396\n             f2 = 0.837767\n      sig.level = 0.05\n          power = 0.8\n```\n\n\n:::\n:::\n\n\nThis tells us that the denominator degrees of freedom should be 15 (14.62 rounded up), and this means that we would only need 20 observations in total across all five groups to detect this effect size (Remember: number of observations = numerator d.f. + denominator d.f. + 1)\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula = \"length ~ C(location)\", data = mussels_py)\n# and get the fitted parameters of the model\nlm_mussels_py = model.fit()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# perform the anova on the fitted model\nsm.stats.anova_lm(lm_mussels_py)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               df    sum_sq   mean_sq         F    PR(>F)\nC(location)   4.0  0.004520  0.001130  7.121019  0.000281\nResidual     34.0  0.005395  0.000159       NaN       NaN\n```\n\n\n:::\n:::\n\n\nSince we only have one model term here (`location`), the $R^2$ and $\\eta^2$ values are the same.\n\nWe can get the $R^2$  (0.4559) value from the model as follows:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# get the R2 value\nR2 = lm_mussels_py.rsquared\n```\n:::\n\n\nWe can now calculate the number of required samples.\n\nWe use the following values:\n\n* `eta_squared` = `R2`\n* `k` = 5 (we have five groups)\n* `power` = 0.80\n* `alpha` = 0.05 (our significance threshold)\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.power_anova(eta_squared = R2,\n               k = 5, power = 0.80, alpha = 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3.9247946339568163\n```\n\n\n:::\n:::\n\n\nThis means we need 4 samples per group, so 20 in total ($4 \\times 5$).\n\n:::\n:::\n:::\n\n### Power and effect {#sec-exr_pwreffect}\n\n:::{.callout-exercise}\n\n\n{{< level 2 >}}\n\n\n\nThe file `/data/CS6-epilepsy1.csv` contains information on the ages and rates of seizures of 236 patients undertaking a clinical trial.\n\na. Analyse the data using a linear model and calculate the effect size.\nb. If there would be a relationship that large between age and seizure rate how big a study would be needed to observe the effect with a 90% power?\n\n\n::: {.callout-answer collapse=\"true\"}\n## Answer\n\nLet's first load in the data and have a look at them.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nLet's load in the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nepilepsy <- read_csv(\"data/CS6-epilepsy1.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 236 Columns: 2\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\ndbl (2): age, seizure\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\nLet's just have a quick look at the data to see what we're dealing with:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = epilepsy,\n       mapping = aes(x = age,\n                     y = seizure)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](cs6_practical_power-analysis_files/figure-html/unnamed-chunk-63-1.png){width=672}\n:::\n:::\n\n\n## Python\n\nLet's load in the data:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nepilepsy_py = pd.read_csv(\"data/CS6-epilepsy1.csv\")\n```\n:::\n\n\nLet's just have a quick look at the data to see what we're dealing with:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(epilepsy_py, aes(x = \"age\",\n                         y = \"seizure\")) +\n  geom_point())\n```\n\n::: {.cell-output-display}\n![](cs6_practical_power-analysis_files/figure-html/unnamed-chunk-65-1.png){width=614}\n:::\n:::\n\n\n:::\n\nSo we are effectively looking at a simple linear regression here.\n\nNow we fit a linear model and determine the number of samples for the observed effect size at 90% power:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define the model\nlm_epilepsy <- lm(seizure ~ age,\n                  data = epilepsy)\n\n# summarise the model\nsummary(lm_epilepsy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = seizure ~ age, data = epilepsy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.77513 -0.19585 -0.04333  0.22288  1.24168 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.814935   0.124857   6.527 4.12e-10 ***\nage         -0.001990   0.004303  -0.463    0.644    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.413 on 234 degrees of freedom\nMultiple R-squared:  0.0009134,\tAdjusted R-squared:  -0.003356 \nF-statistic: 0.2139 on 1 and 234 DF,  p-value: 0.6441\n```\n\n\n:::\n:::\n\n\nFrom this we get that the $R^2$ value is 9.134\\times 10^{-4} (which is tiny!) and we can use this to calculate Cohen's $f^2$ value using the formula in the notes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2 <- lm_epilepsy %>% \n    glance() %>% \n    pull(r.squared)\n\nf2 <- R2 / (1 - R2)\nf2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0009142378\n```\n\n\n:::\n:::\n\n\nThis effect size is absolutely tiny. If we really wanted to design an experiment to pick up an effect size this small then we would expect that we'll need 1000s of participants.\n\nNow, our model has 2 parameters (an intercept and a slope) and so the numerator degrees of freedom (`u`) will be 1 (2 - 1 = 1!). This means that we can now carry our our power analysis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.f2.test(u = 1, f2 = f2,\n            sig.level = 0.05 , power = 0.9)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 11493.01\n             f2 = 0.0009142378\n      sig.level = 0.05\n          power = 0.9\n```\n\n\n:::\n:::\n\n\nThis tells us that the denominator degrees of freedom (`v`) should be 1.1494\\times 10^{4} (rounded up to the nearest number), and this means that we would need 11496 participants to detect this effect size (Remember: number of observations = numerator d.f. (`u`) + denominator d.f. (`v`) + 1). \n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula = \"seizure ~ age\", data = epilepsy_py)\n# and get the fitted parameters of the model\nlm_epilepsy_py = model.fit()\n```\n:::\n\n\nSince we only have one model term here (`age`), the $R^2$ and $\\eta^2$ values are the same.\n\nWe can get the $R^2$ value from the model as follows:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# get the R2 value\nR2 = lm_epilepsy_py.rsquared\n\nR2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.0009134027561267244\n```\n\n\n:::\n:::\n\n\nThis is a tiny effect size! In order to detect this, we would need a hell of a lot of samples.\n\nThere is no straightforward way (that I currently know of) to calculate sample sizes in Python for a linear model with a continuous predictor. We can use an external script that is based on the `pwr.f2.test()` function in R - this uses a couple of different metrics to calculate the sample size (or effect size, power or significance threshold - depending on which values are given).\n\nThis takes the following arguments:\n\n* `f2`, Cohen's $f^2$ which is calculated as $f^2 = \\frac{R^2}{1-R^2}$\n* `u`, numerator degrees of freedom. This is the number of parameters in the model minus 1\n* `v`, denominator degrees of freedom. This is the number of observations - number of parameters\n* `sig_level`, significance level\n* `power`, desired power of the test\n\nIt has the following dependencies, so you'll need to load these:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom numpy import sqrt, ceil, abs\nfrom scipy.optimize import brenth\nfrom scipy.stats import f\nfrom scipy.special import ncfdtr\n```\n:::\n\n\nNext, we load the `pwr_f2_test()` function (here I've saved it in `scripts/pwr_f2_test.py`):\n\n\n::: {.cell}\n\n```{.python .cell-code}\nexec(open('scripts/pwr_f2_test.py').read())\n```\n:::\n\n\nNext, we calculated `f2`:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nf2 = R2 / (1 - R2)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npwr_f2_test(u = 1, f2 = f2, sig_level = 0.05 , power = 0.9)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPower analysis results: \n u is: 1,\n v is: 11492.99883145477,\n f2 is: 0.0009142378234744412,\n sig_level is: 0.05,\n power is: 0.9,\n num_obs is: 11495\n```\n\n\n:::\n:::\n\n\nSo we see that the number of observations we need is 11495!\n:::\n:::\n:::\n\n### Study size with multiple regression {#sec-exr_pwrmultreg}\n\n:::{.callout-exercise}\n\n\n{{< level 2 >}}\n\n\n\nWe wish to test the effectiveness of a new drug against a placebo. It is thought that the sex and age of the patients may have an effect on their response.\n\na. Write down a linear model equation that might describe the relationship between these variables including all possible two-way interactions.\nb. How big a study would we need to detect a medium effect size (according to Cohen, this is 0.15) at a power of 90%, with significance level 0.05?\n\n::: {.callout-answer collapse=\"true\"}\n## Answer\n\nHere we have a system with a single response variable, and three predictor variables. One of them is `gender`, a categorical predictor with two possible levels (M, F). One of them is `treatment`, again a categorical predictor with two possible levels (Drug, placebo) and one of them is continuous (`age`). The last one could even be viewed as a categorical predictor, where each year is a category. If we would like to model `age` as such, then we'd have to define it as a factor. We're not doing this here.\n\nA linear model with all possible two-way interactions would look something like this:\n\n`response ~ treatment + gender + age + treatment:gender + treatment:age + age:gender`\n\nIn order to do a power calculation for this set up, we'll need four things:\n\n1. the **effect size**. Here we're told it's a medium effect size according to Cohen so we can use his default values. Here the value is 0.15, see table further above. Alternatively, we could have looked this up online (which may give us different values, or values that are relevant to a specific discipline).\n2. The **desired power**. Here we're told it's 90%\n3. The **significance level** to work to. Again we're told this is going to be 0.05.\n4. The **numerator degrees of freedom**. This is the tricky bit. We can do this by adding up the degrees of freedom for each term separately.\n\nThe numerator degrees of freedom is best calculated by working out the degrees of freedom of each of the six terms separately and then adding these up.\n\nThere are three simple ideas here that you need:\n\n1. The degrees of freedom for a categorical variable is just the number of groups - 1\n2. The degrees of freedom for a continuous variable is always 1\n3. the degrees of freedom for any interaction is simple the product of the degrees of the main effects involved in the interaction.\n\nSo this means:\n\n* The `df` for `gender` is 1 (2 groups - 1)\n* The `df` for `treatment` is 1 (2 groups -1)\n* The `df` for `age` is 1 (continuous predictor)\n* The `df` for `gender:treatment` is 1 (1 x 1)\n* The `df` for `gender:age` is 1 (1 x 1)\n* The `df` for `age:treatment` is 1 (1 x 1)\n\nRather boring that all of them were 1 to be honest. Anyway, given that the denominator degrees of freedom is just the sum of all of these, we can see that $u = 6$.\n\nWe now have all of the information to carry out the power analysis.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.f2.test(u = 6, f2 = 0.15,\n            sig.level = 0.05, power = 0.9)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Multiple regression power calculation \n\n              u = 6\n              v = 115.5826\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.9\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\npwr_f2_test(u = 6, f2 = 0.15,\n            sig_level = 0.05, power = 0.9)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPower analysis results: \n u is: 6,\n v is: 115.58168763404052,\n f2 is: 0.15,\n sig_level is: 0.05,\n power is: 0.9,\n num_obs is: 123\n```\n\n\n:::\n:::\n\n:::\n\nWe get a denominator df of 116, which means that we would need at least 123 participants in our study (Remember: number of observations = numerator d.f. (u) + denominator d.f. (v) + 1). Given that we have four unique combinations of `gender` and `treatment`, it would be practically sensible to round this up to 124 participants so that we could have an equal number (31) in each combination of sex and treatment. It would also be sensible to aim for a similar distribution of age ranges in each group as well.\n\n:::\n:::\n\n## Summary\n\n::: {.callout-tip}\n#### Key points\n\n- Power is the capacity of a test to detect significant results and is affected by\n    1. the effect size\n    2. sample size\n    3. the significance level\n- Power analysis optimises the trade-off between power, significance level and the desired effect size that we would like to detect\n:::\n",
    "supporting": [
      "cs6_practical_power-analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}