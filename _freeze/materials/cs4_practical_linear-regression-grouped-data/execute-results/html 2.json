{
  "hash": "99195b9da39b0bd159b1c04ba21e5c49",
  "result": {
    "markdown": "---\ntitle: \"Linear regression with grouped data\"\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n::: callout-tip\n## Learning outcomes\n\n**Questions**\n\n- How do I perform a linear regression on grouped data?\n\n**Objectives**\n\n- Be able to perform a linear regression on grouped data in R\n- Calculate the linear regression for individual groups and visualise these with the data\n- Understand and be able to create equations of the regression line\n- Be able to deal with interactions in this context\n:::\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n### Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n# Helper functions for tidying data\nlibrary(broom)\n```\n:::\n\n\n### Functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gets underlying data out of model object\nbroom::augment()\n\n# Creates diagnostic plots\nggResidpanel::resid_panel()\n```\n:::\n\n\n## R\n\n### Functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creates a subset of the data\nsubset()\n```\n:::\n\n\n\n## Python\n| Libraries                                                             | Description                                                                                |\n|:-----------------------|:-----------------------------------------------|\n| [`numpy`](https://numpy.org)| A fundamental package for scientific computing with Python.|\n| [`pandas`](https://pandas.pydata.org/docs/getting_started/index.html) | A Python data analysis and manipulation tool.                                              |\n| [`pingouin`](https://pingouin-stats.org)                              | A Python module developed to have simple yet exhaustive stats functions.                   |\n| [`plotnine`](https://plotnine.readthedocs.io/en/stable/)              | The Python equivalent of `ggplot2`.|\n| [`statsmodels`](https://www.statsmodels.org/stable/index.html)        | A Python module for statistical models, conducting tests and statistical data exploration. |\n\n:::\n:::\n\n## Purpose and aim\nA linear regression analysis with grouped data is used when we have one categorical predictor variable (or factor), and one continuous predictor variable. The response variable must still be continuous however.\n\nFor example in an experiment that looks at light intensity in woodland, how is light intensity (continuous: lux) affected by the height at which the measurement is taken, recorded as depth measured from the top of the canopy (continuous: metres) and by the type of woodland (categorical: Conifer or Broad leaf).\n\n\n::: {.cell show_col_types='false'}\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nWhen analysing these type of data we want to know:\n\n1.\tIs there a difference between the groups?\n2.\tDoes the continuous predictor variable affect the continuous response variable (does canopy depth affect measured light intensity?)\n3.\tIs there any interaction between the two predictor variables? Here an interaction would display itself as a difference in the slopes of the regression lines for each group, so for example perhaps the conifer data set has a significantly steeper line than the broad leaf woodland data set.\n\nIn this case, no interaction means that the regression lines will have the same slope.\nEssentially the analysis is identical to two-way ANOVA.\n\n1. We will plot the data and visually inspect it.\n2. We will test for an interaction and if it doesn’t exist then:\n    a. We can test to see if either predictor variable has an effect (i.e. do the regression lines have different intercepts? and is the common gradient significantly different from zero?)\n    \nWe will first consider how to visualise the data before then carrying out an appropriate statistical test.\n\n## Data and hypotheses\nThe data are stored in `data/CS4-treelight.csv`. This is a data frame with four variables; `id`, `light`, `depth` and `species`. `light` is the continuous response variable, `depth` is the continuous predictor variable and `species` is the categorical predictor variable.\n\nRead in the data and inspect them:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read in the data\ntreelight <- read_csv(\"data/CS4-treelight.csv\")\n\n# inspect the data\ntreelight\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 23 × 4\n      id light depth species\n   <dbl> <dbl> <dbl> <chr>  \n 1     1 4106.  1    Conifer\n 2     2 4934.  1.75 Conifer\n 3     3 4417.  2.5  Conifer\n 4     4 4529.  3.25 Conifer\n 5     5 3443.  4    Conifer\n 6     6 4640.  4.75 Conifer\n 7     7 3082.  5.5  Conifer\n 8     8 2368.  6.25 Conifer\n 9     9 2777.  7    Conifer\n10    10 2419.  7.75 Conifer\n# … with 13 more rows\n```\n:::\n:::\n\n\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\n# read in the data\ntreelight_r <- read.csv(\"data/CS4-treelight.csv\")\n\n# inspect the data\nhead(treelight_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  id    light depth species\n1  1 4105.646  1.00 Conifer\n2  2 4933.925  1.75 Conifer\n3  3 4416.527  2.50 Conifer\n4  4 4528.618  3.25 Conifer\n5  5 3442.610  4.00 Conifer\n6  6 4640.297  4.75 Conifer\n```\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# load the data\ntreelight_py = pd.read_csv(\"data/CS4-treelight.csv\")\n\n# and have a look\ntreelight_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   id        light  depth  species\n0   1  4105.646110   1.00  Conifer\n1   2  4933.925144   1.75  Conifer\n2   3  4416.527443   2.50  Conifer\n3   4  4528.618186   3.25  Conifer\n4   5  3442.610306   4.00  Conifer\n```\n:::\n:::\n\n:::\n\n## Summarise and visualise\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the data\ntreelight %>% \n  ggplot(aes(x = depth, y = light, colour = species)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Depth (m)\",\n       y = \"Light intensity (lux)\")\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(light ~ depth,\n     data = treelight_r)\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThis plots all of the points in the data set on the same window. To visualise the data by `species`, we can use the `col` argument. This only takes data in the form of factors (categorical variables with inherit order), so we need to temporarily conver the `species` column to a factor with `col = factor(species)`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(light ~ depth,\n     data = treelight_r,\n     col = factor(species))\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# plot the data\n(\n  ggplot(treelight_py,\n    aes(x = \"depth\", y = \"light\", colour = \"species\")) +\n    geom_point() +\n    scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n    labs(x = \"Depth (m)\",\n         y = \"Light intensity (lux)\")\n)\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-13-1.png){width=614}\n:::\n:::\n\n\n:::\n\nIt looks like there is a slight negative correlation between depth and light intensity, with light intensity reducing as the canopy depth increases. It would be useful to plot the regression lines in this plot.\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the data\ntreelight %>% \n  ggplot(aes(x = depth, y = light, colour = species)) +\n  geom_point() +\n  # add regression lines\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Depth (m)\",\n       y = \"Light intensity (lux)\")\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-14-3.png){width=672}\n:::\n:::\n\n\n## R\n\nUnfortunately there is not an easy way in base R to add regression line to individual groups. To do this, we need to do three things:\n\n1. Split the data by `species`\n2. Create a linear model for each group\n3. Determine the regression line (or line of best fit) for each group and then add this to the plot\n\nThe `subset` function creates subsets of data frames. The first argument is the original data frame, and the subset argument is a logical expression that defines which observations (rows) should be extracted. The logical expression must be enclosed in parentheses. In the first case it says (`Species == \"Conifer\"`). This tells R to only extract the rows of the original data frame which have `Conifer` in the species variable column. Ditto for `Broadleaf`.\n\nFirst, we subset the data:\n\n::: {.cell}\n\n```{.r .cell-code}\n# subset the conifers\nconLight <- subset(treelight_r,\n                   subset = (species == \"Conifer\"))\n\n# subset the broad leaf\nbroLight <- subset(treelight_r,\n                   subset = (species == \"Broadleaf\"))\n```\n:::\n\n\nThen we create a linear model for each subgroup:\n\n::: {.cell}\n\n```{.r .cell-code}\n# linear regression for Broadleaf\nlm_broadleaf <- lm(light ~ depth,\n                   data = broLight)\n\n# linear regression for Conifer\nlm_conifer <- lm(light ~ depth,\n                 data = conLight)\n```\n:::\n\n\nAnd plot it all together:\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(light ~ depth,\n     data = treelight_r,\n     col = factor(species))\n\n# add the Broadleaf linear regression\nabline(lm_broadleaf, col = \"black\")\n\n# add the Conifer linear regression\nabline(lm_conifer, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n## Python\n\n::: {.cell}\n\n```{.python .cell-code}\n# plot the data\n(\nggplot(treelight_py,\n       aes(x = \"depth\", y = \"light\", colour = \"species\")) +\n  geom_point() +\n  # add regression lines\n  geom_smooth(method = \"lm\", se = False) +\n  scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n  labs(x = \"Depth (m)\",\n       y = \"Light intensity (lux)\")\n)\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-18-1.png){width=614}\n:::\n:::\n\n:::\n\nLooking at this plot, there doesn’t appear to be any significant interaction between the woodland type (`Broadleaf` and `Conifer`) and the depth at which light measurements were taken (`depth`) on the amount of light intensity getting through the canopy as the gradients of the two lines appear to be very similar. There does appear to be a noticeable slope to both lines and both lines look as though they have very different intercepts. All of this suggests that there isn’t any interaction but that both `depth` and `species` have a significant effect on `light` independently.\n\n## Implement and interpret the test\nIn this case we're going to implement the test before checking the assumptions (I know, let's live a little!). You'll find out why soon...\n\nWe can test for a possible interaction more formally:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lm(light ~ depth * species,\n         data = treelight))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: light\n              Df   Sum Sq  Mean Sq  F value    Pr(>F)    \ndepth          1 30812910 30812910 107.8154 2.861e-09 ***\nspecies        1 51029543 51029543 178.5541 4.128e-11 ***\ndepth:species  1   218138   218138   0.7633    0.3932    \nResiduals     19  5430069   285793                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nRemember that `depth * species` is a shorthand way of writing the full set of `depth + species + depth:species` terms in R _i.e._ both main effects and the interaction effect.\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lm(light ~ depth * species,\n         data = treelight))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: light\n              Df   Sum Sq  Mean Sq  F value    Pr(>F)    \ndepth          1 30812910 30812910 107.8154 2.861e-09 ***\nspecies        1 51029543 51029543 178.5541 4.128e-11 ***\ndepth:species  1   218138   218138   0.7633    0.3932    \nResiduals     19  5430069   285793                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nRemember that `depth * species` is a shorthand way of writing the full set of `depth + species + depth:species` terms in R _i.e._ both main effects and the interaction effect.\n\n## Python\n\nUnfortunately there is no clear way of defining interaction models in pingouin. So we're resorting back to `statsmodels`, just like we had to when we performed the Shapiro-Wilk test on the residuals.\n\nIf you haven't loaded `statsmodels` yet, run the following:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n```\n:::\n\n\nNext, we create a linear model and get the `.fit()`:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula = \"light ~ depth * C(species)\",\n                data = treelight_py)\n\n# and get the fitted parameters of the model\nlm_treelight_py = model.fit()\n```\n:::\n\n\nTo get the relevant values, we can print the summary of the model fit. This gives us a rather huge table. Don't be too daunted by it - there is a logic to the madness and for now we're mainly interested in the `P>|t|` column.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(lm_treelight_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  light   R-squared:                       0.938\nModel:                            OLS   Adj. R-squared:                  0.928\nMethod:                 Least Squares   F-statistic:                     95.71\nDate:                Wed, 26 Oct 2022   Prob (F-statistic):           1.19e-11\nTime:                        14:15:25   Log-Likelihood:                -174.91\nNo. Observations:                  23   AIC:                             357.8\nDf Residuals:                      19   BIC:                             362.4\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n===============================================================================================\n                                  coef    std err          t      P>|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------\nIntercept                    7798.5655    298.623     26.115      0.000    7173.541    8423.590\nC(species)[T.Conifer]       -2784.5833    442.274     -6.296      0.000   -3710.274   -1858.893\ndepth                        -221.1256     61.802     -3.578      0.002    -350.478     -91.773\ndepth:C(species)[T.Conifer]   -71.0357     81.309     -0.874      0.393    -241.217      99.145\n==============================================================================\nOmnibus:                        1.435   Durbin-Watson:                   2.176\nProb(Omnibus):                  0.488   Jarque-Bera (JB):                1.269\nSkew:                           0.444   Prob(JB):                        0.530\nKurtosis:                       2.267   Cond. No.                         31.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n:::\n\nAs with two-way ANOVA we have a row in the table for each of the different effects. At this point we are particularly interested in the p-values. We need to look at the interaction first.\n\nThe interaction term between `depth` and `species` has a p-value of 0.393 (which is bigger than 0.05) and so we can conclude that the interaction between `depth` and `species` isn’t significant. As such we can now consider whether each of the predictor variables independently has an effect.\n\nBoth `depth` and `species` have very small p-values (2.86x10<sup>-9</sup> and 4.13x10 <sup>-11</sup>) and so we can conclude that they do have a significant effect on `light`.\n\nThis means that the two regression lines should have the same non-zero slope, but different intercepts. We would now like to know what those values are.\n\n### Finding intercept values\n\nFinding the intercept values is not entirely straightforward and there is some deciphering required to get this right.\n\nFor a simple straight line such as the linear regression for the conifer data by itself, the output is relatively straightforward:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# filter the Conifer data and fit a linear model\ntreelight %>% \n  filter(species == \"Conifer\") %>% \n  lm(light ~ depth, data = .)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = light ~ depth, data = .)\n\nCoefficients:\n(Intercept)        depth  \n     5014.0       -292.2  \n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(light ~ depth,\n   data = conLight)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = light ~ depth, data = conLight)\n\nCoefficients:\n(Intercept)        depth  \n     5014.0       -292.2  \n```\n:::\n:::\n\n## Python\n\nWe have two options to obtain the intercept for conifers only. We could subset our data, keeping only the conifer values. We could then create a linear model of those data, and obtain the relevant intercept.\n\nHowever, since we already created a model for the entire data set (including the interaction term) and printed the summary of that, we can actually derive the intercept value with the information that we've got.\n\nIn the `coef` table of the summary there are several values:\n\n```\nIntercept                      7798.5655\nC(species)[T.Conifer]         -2784.5833\ndepth                         -221.1256\ndepth:C(species)[T.Conifer]   -71.0357\n```\n\nThis tells us that the _overall_ intercept value for the model with the interaction term is 7798.5655. The `C(species)[T.Conifer]` term means that, to go from this overall intercept value _to the intercept for conifer_, we need to add -2784.5833.\n\nDoing the maths gives us an intercept of $7798.5655 + (-2784.5833) = 5014$ if we round this.\n\nEqually, if we want to get the coefficient for `depth`, then we take the reference value of -221.1256 and add the value next to `depth:C(species)[T.Conifer]` to it. This gives us $-221.1256 + (-71.0357) = -292.2$ if we round it.\n\n:::\n\nWe can interpret this as meaning that the intercept of the regression line is 5014 and the coefficient of the depth variable (the number in front of it in the equation) is -292.2.\n\nSo, the equation of the regression line for the conifer data is given by:\n\n\\begin{equation}\nlight = 5014 + -292.2 \\times depth\n\\end{equation} \n\nThis means that for every extra 1 m of depth of forest canopy we lose 292.2 lux of light.\n\nWhen we looked at the full data set, we found that interaction wasn’t important. This means that we will have a model with two distinct intercepts but only a single slope (that’s what you get for a linear regression without any interaction), so we need to calculate that specific combination. We do this is as follows:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(light ~ depth + species,\n   data = treelight)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = light ~ depth + species, data = treelight)\n\nCoefficients:\n   (Intercept)           depth  speciesConifer  \n        7962.0          -262.2         -3113.0  \n```\n:::\n:::\n\n\nNotice the `+` symbol in the argument, as opposed to the `*` symbol used earlier. This means that we are explicitly _not_ including an interaction term in this fit, and consequently we are forcing R to calculate the equation of lines which have the same gradient.\n\nIdeally we would like R to give us two equations, one for each forest type, so four parameters in total.\nUnfortunately, R is parsimonious and doesn’t do that. Instead R gives you three coefficients, and these require a bit of interpretation.\n\nThe first two numbers that R returns (underneath `Intercept` and `depth`) are the exact intercept and slope coefficients for one of the lines (in this case they correspond to the data for `Broadleaf` woodlands).\n\nFor the coefficients belonging to the other line, R uses these first two coefficients as baseline values and expresses the other coefficients relative to these ones. R also doesn’t tell you explicitly which group it is using as its baseline reference group! (Did I mention that R can be very helpful at times 😉?)\n\nSo, how to decipher the above output?\n\nFirst, I need to work out which group has been used as the baseline.\n\n* It will be the group that comes first alphabetically, so it should be `Broadleaf`\n* The other way to check would be to look and see which group is not mentioned in the above table. `Conifer` is mentioned (in the `SpeciesConifer` heading) and so again the baseline group is `Broadleaf.`\n\nThis means that the intercept value and `depth` coefficient correspond to the `Broadleaf` group and as a result I know what the equation of one of my lines is:\n\nBroadleaf:\n\n\\begin{equation}\nlight = 7962 + -262.2 \\times depth\n\\end{equation} \n\nIn this example we know that the gradient is the same for both lines (because we explicitly asked to exclude an interaction), so all I need to do is find the intercept value for the `Conifer` group. Unfortunately, the final value given underneath `SpeciesConifer` does not give me the intercept for `Conifer`, instead it tells me the difference between the `Conifer` group intercept and the baseline intercept i.e. the equation for the regression line for conifer woodland is given by:\n\n\\begin{equation}\nlight = (7962 + -3113) + -262.2 \\times depth\n\\end{equation} \n\n\\begin{equation}\nlight = 4829 + -262.2 \\times depth\n\\end{equation} \n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(light ~ depth + species,\n   data = treelight_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = light ~ depth + species, data = treelight_r)\n\nCoefficients:\n   (Intercept)           depth  speciesConifer  \n        7962.0          -262.2         -3113.0  \n```\n:::\n:::\n\n\nNotice the `+` symbol in the argument, as opposed to the `*` symbol used earlier. This means that we are explicitly _not_ including an interaction term in this fit, and consequently we are forcing R to calculate the equation of lines which have the same gradient.\n\nIdeally we would like R to give us two equations, one for each forest type, so four parameters in total.\nUnfortunately, R is parsimonious and doesn’t do that. Instead R gives you three coefficients, and these require a bit of interpretation.\n\nThe first two numbers that R returns (underneath `Intercept` and `depth`) are the exact intercept and slope coefficients for one of the lines (in this case they correspond to the data for `Broadleaf` woodlands).\n\nFor the coefficients belonging to the other line, R uses these first two coefficients as baseline values and expresses the other coefficients relative to these ones. R also doesn’t tell you explicitly which group it is using as its baseline reference group! (Did I mention that R can be very helpful at times 😉?)\n\nSo, how to decipher the above output?\n\nFirst, I need to work out which group has been used as the baseline.\n\n* It will be the group that comes first alphabetically, so it should be `Broadleaf`\n* The other way to check would be to look and see which group is not mentioned in the above table. `Conifer` is mentioned (in the `SpeciesConifer` heading) and so again the baseline group is `Broadleaf.`\n\nThis means that the intercept value and `depth` coefficient correspond to the `Broadleaf` group and as a result I know what the equation of one of my lines is:\n\nBroadleaf:\n\n\\begin{equation}\nlight = 7962 + -262.2 \\times depth\n\\end{equation} \n\nIn this example we know that the gradient is the same for both lines (because we explicitly asked to exclude an interaction), so all I need to do is find the intercept value for the `Conifer` group. Unfortunately, the final value given underneath `SpeciesConifer` does not give me the intercept for `Conifer`, instead it tells me the difference between the `Conifer` group intercept and the baseline intercept i.e. the equation for the regression line for conifer woodland is given by:\n\n\\begin{equation}\nlight = (7962 + -3113) + -262.2 \\times depth\n\\end{equation} \n\n\\begin{equation}\nlight = 4829 + -262.2 \\times depth\n\\end{equation} \n\n## Python\n\nThe way we obtain the values for the model without the interaction is very similar to what we did for the conifer data. We need to update our model first, to remove the interaction:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula = \"light ~ depth + C(species)\",\n                data = treelight_py)\n\n# and get the fitted parameters of the model\nlm_treelight_add_py = model.fit()\n```\n:::\n\n\nNotice the `+` symbol in the argument, as opposed to the `*` symbol used earlier. This means that we are explicitly _not_ including an interaction term in this fit, and consequently we are forcing Python to calculate the equation of lines which have the same gradient.\n\nWe can get the relevant coefficients as follows:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(lm_treelight_add_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  light   R-squared:                       0.935\nModel:                            OLS   Adj. R-squared:                  0.929\nMethod:                 Least Squares   F-statistic:                     144.9\nDate:                Wed, 26 Oct 2022   Prob (F-statistic):           1.26e-12\nTime:                        14:15:26   Log-Likelihood:                -175.37\nNo. Observations:                  23   AIC:                             356.7\nDf Residuals:                      20   BIC:                             360.1\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=========================================================================================\n                            coef    std err          t      P>|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------\nIntercept              7962.0316    231.356     34.415      0.000    7479.431    8444.633\nC(species)[T.Conifer] -3113.0265    231.586    -13.442      0.000   -3596.106   -2629.947\ndepth                  -262.1656     39.922     -6.567      0.000    -345.441    -178.891\n==============================================================================\nOmnibus:                        2.068   Durbin-Watson:                   2.272\nProb(Omnibus):                  0.356   Jarque-Bera (JB):                1.677\nSkew:                           0.633   Prob(JB):                        0.432\nKurtosis:                       2.618   Cond. No.                         13.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nAgain, I need to work out which group has been used as the baseline.\n\n* It will be the group that comes first alphabetically, so it should be `Broadleaf`\n* The other way to check would be to look and see which group is not mentioned in the above table. `Conifer` is mentioned (in the `C(species)[T.Conifer]` heading) and so again the baseline group is `Broadleaf.`\n\nThis means that the intercept value and `depth` coefficient correspond to the `Broadleaf` group and as a result I know what the equation of one of my lines is:\n\nBroadleaf:\n\n\\begin{equation}\nlight = 7962 + -262.2 \\times depth\n\\end{equation} \n\nIn this example we know that the gradient is the same for both lines (because we explicitly asked to exclude an interaction), so all I need to do is find the intercept value for the `Conifer` group. Unfortunately, the final value given in `C(species)[T.Conifer]` does not give me the intercept for `Conifer`, instead it tells me the difference between the `Conifer` group intercept and the baseline intercept i.e. the equation for the regression line for conifer woodland is given by:\n\n\\begin{equation}\nlight = (7962 + -3113) + -262.2 \\times depth\n\\end{equation} \n\n\\begin{equation}\nlight = 4829 + -262.2 \\times depth\n\\end{equation} \n:::\n\n### Adding custom regression lines\nIn the example above we determined that the interaction term `species:depth` was not significant. It would be good to visualise the model without the interaction term.\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\nThis is relatively straightforward if we understand the output of the model a bit better.\n\nFirst of all, we load the `broom` library. This is part of tidyverse, so you don't have to install it. It is not loaded by default, hence us loading it. What broom does it changes the format of many common base R outputs into a more tidy format, so we can work with the output in our analyses more easily.\n\nThe function we use here is called `augment()`. What this does is take a model object and a dataset and adds information about each observation in the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define the model without interaction term\nlm_additive <- lm(light ~ species + depth,\n                  data = treelight)\n\n# load the broom package\nlibrary(broom)\n\n# augment the model\nlm_additive %>% augment()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 23 × 9\n   light species depth .fitted .resid   .hat .sigma .cooksd .std.resid\n   <dbl> <chr>   <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>      <dbl>\n 1 4106. Conifer  1      4587.  -481. 0.191    531. 0.0799      -1.01 \n 2 4934. Conifer  1.75   4390.   544. 0.156    528. 0.0766       1.11 \n 3 4417. Conifer  2.5    4194.   223. 0.128    542. 0.00985      0.449\n 4 4529. Conifer  3.25   3997.   532. 0.105    530. 0.0440       1.06 \n 5 3443. Conifer  4      3800.  -358. 0.0896   538. 0.0163      -0.706\n 6 4640. Conifer  4.75   3604.  1037. 0.0801   486. 0.120        2.03 \n 7 3082. Conifer  5.5    3407.  -325. 0.0769   540. 0.0113      -0.637\n 8 2368. Conifer  6.25   3210.  -842. 0.0801   507. 0.0793      -1.65 \n 9 2777. Conifer  7      3014.  -237. 0.0896   542. 0.00719     -0.468\n10 2419. Conifer  7.75   2817.  -398. 0.105    537. 0.0247      -0.792\n# … with 13 more rows\n```\n:::\n:::\n\n\nThe output shows us lots of data. Our original `light` values are in the `light` column and it's the same for `species` and `depth`. What has been added is information about the fitted (or predicted) values based on the `light ~ depth + species` model we defined.\n\nThe fitted or predicted values are in the `.fitted` column, with corresponding residuals in the `.resid` column. Remember, your data = predicted values + error, so if you would add `.fitted` + `resid` then you would end up with your original data again.\n\nUsing this information we can now plot the regression lines by `species`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the regression lines by species\nlm_additive %>%\n  augment() %>% \n  ggplot(aes(x = depth, y = .fitted, colour = species)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Dark2\")\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\nLastly, if we would want to plot the data and regression lines together, we could change the code as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the regression lines\nlm_additive %>%\n  augment() %>% \n  ggplot(aes(x = depth, y = .fitted, colour = species)) +\n  # add the original data points\n  geom_point(data = treelight,\n             aes(x = depth, y = light, colour = species)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Depth (m)\",\n       y = \"Light intensity (lux)\")\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n## R\nUnfortunately, base R doesn’t have a sensible way of automatically adding multiple regression lines to a plot and so if we want to do this, we will have to do it manually.\n\nFirst, we create a linear regression without the interaction term.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_treelight_r_add <- lm(light ~ depth + species,\n             data = treelight_r)\n```\n:::\n\n\nWe first need to extract the relative coefficient values from the `lm` object and then combine them manually to create separate vectors containing the intercept and slope coefficients for each line. This next set of command is a bit annoying but stick with it; it’ll pay dividends (no, really it will – you always secretly wanted to be a computer programmer didn’t you? This medic/biologist/life scientist thing is just a passing phase that you’ll grow out of…)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncf <- coef(lm_treelight_r_add)\ncf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   (Intercept)          depth speciesConifer \n     7962.0316      -262.1656     -3113.0265 \n```\n:::\n\n```{.r .cell-code}\ncf_Broadleaf <- c(cf[1], cf[2])\ncf_Conifer <- c(cf[1] + cf[3], cf[2])\n\ncf_Broadleaf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)       depth \n  7962.0316   -262.1656 \n```\n:::\n\n```{.r .cell-code}\ncf_Conifer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)       depth \n  4849.0051   -262.1656 \n```\n:::\n:::\n\n\n* The first line extracts the three (in this case) coefficients from the `lm` object as a vector called `cf`, and the second line prints this to the screen.\n* In the third line we take the 1st and 2nd components of `cf` and store them as the coefficients for the `Broadleaf` line in a vector called `cf_Broadleaf`\n* The fourth line is where we do some actual calculations. Here we realise that the intercept of the conifer line is actually the sum of the 1st and 3rd values of `cf`, whereas the slope is just the 2nd value, and so we create a vector for the conifer line that reflects this.\n* The 5th and 6th lines just print these two vectors to the screen.\n\nWe can now use these two vectors to add the appropriate regression lines to the plot with the original data. We've created _that_ particular plot before, so we can copy/paste the code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot original data\nplot(light ~ depth,\n     data = treelight_r,\n     col = factor(species))\n\n# add the Broadleaf regression line\nabline(cf_Broadleaf, col = \"black\")\n\n# add the Conifer regression line\nabline(cf_Conifer, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n\n## Python\nTo do this, we need to do the following:\n\n* create a linear model _without_ the interaction term (we did this previously)\n* extract the predicted values of the model\n* plot these against the original data\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# get predicted values\nlm_treelight_add_py.predict()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([4586.83958789, 4390.21542165, 4193.59125541, 3996.96708918,\n       3800.34292294, 3603.7187567 , 3407.09459046, 3210.47042422,\n       3013.84625799, 2817.22209175, 2620.59792551, 2423.97375927,\n       2227.34959303, 7322.87199901, 7047.59816627, 5781.86286681,\n       7543.35323075, 6319.56442008, 7267.03073579, 7773.27242247,\n       5822.76069339, 7766.98044915, 6532.70501628])\n```\n:::\n:::\n\n\nWe can't easily use the predicted values in this kind of format, so we're adding them to the existing data, in a column called `.fitted`:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# add predicted values to data set\ntreelight_py['.fitted'] = lm_treelight_add_py.predict()\n\n# have a peek at the data\ntreelight_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   id        light  depth  species      .fitted\n0   1  4105.646110   1.00  Conifer  4586.839588\n1   2  4933.925144   1.75  Conifer  4390.215422\n2   3  4416.527443   2.50  Conifer  4193.591255\n3   4  4528.618186   3.25  Conifer  3996.967089\n4   5  3442.610306   4.00  Conifer  3800.342923\n```\n:::\n:::\n\n\nNow we can simply plot the data:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# plot the data\n(\n  ggplot(treelight_py,\n    aes(x = \"depth\", y = \"light\", colour = \"species\")) +\n    geom_point() +\n    # add regression lines\n    geom_line(aes(x = \"depth\", y = \".fitted\", colour = \"species\")) +\n    scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n    labs(x = \"Depth (m)\",\n         y = \"Light intensity (lux)\")\n)\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-38-1.png){width=614}\n:::\n:::\n\n:::\n\n## Assumptions\nIn this case we first wanted to check if the interaction was significant, prior to checking the assumptions. If we would have checked the assumptions first, then we would have done that one the full model (with the interaction), then done the ANOVA if everything was OK. We would have then found out that the interaction was not significant, meaning we'd have to re-check the assumptions with the new model. In what order you do it is a bit less important here. The main thing is that you check the assumptions and report on it!\n\nAnyway, hopefully you’ve got the gist of checking assumptions for linear models by now: diagnostic plots!\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_additive %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-39-3.png){width=672}\n:::\n:::\n\n\n* The **Residuals plot** looks OK, no systematic pattern.\n* The **Q-Q plot** isn’t perfect, but I’m happy with the normality assumption.\n* The **Location-Scale plot** is OK, some very slight suggestion of heterogeneity of variance, but nothing to be too worried about.\n* The **Cook's D plot** shows that all of the points are OK\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(lm(light ~ depth + species,\n        data = treelight_r))\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\n* The top left graph looks OK, no systematic pattern.\n* The top right graph isn’t perfect, but I’m happy with the normality assumption.\n* The bottom left graph is OK, some very slight suggestion of heterogeneity of variance, but nothing to be too worried about.\n* The bottom right graph shows that all of the points are OK\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndgplots(lm_treelight_add_py)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/dgplots/2022_10_26-02:15:30_pm_dgplots.png){width=818}\n:::\n:::\n\n\n* The **Residuals plot** looks OK, no systematic pattern.\n* The **Q-Q plot** isn’t perfect, but I’m happy with the normality assumption.\n* The **Location-Scale plot** is OK, some very slight suggestion of heterogeneity of variance, but nothing to be too worried about.\n* The **Influential points plot** shows that all of the points are OK\n\n:::\n\nWoohoo!\n\n## Dealing with interaction\nIf there _had been_ a significant interaction between the two predictor variables (for example, if light intensity had dropped off significantly faster in conifer woods than in broad leaf woods, in addition to being lower overall, then we would again be looking for two equations for the linear regression, but this time the gradients vary as well.\nIn this case interaction is important and so we need the output from a linear regression that explicitly includes the interaction term:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(light ~ depth + species + depth:species,\n   data = treelight)\n```\n:::\n\n\nor written using the short-hand:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(light ~ depth * species,\n   data = treelight)\n```\n:::\n\n\nThere really is absolutely no difference in the end result.\nEither way this gives us the following output:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = light ~ depth * species, data = treelight)\n\nCoefficients:\n         (Intercept)                 depth        speciesConifer  \n             7798.57               -221.13              -2784.58  \ndepth:speciesConifer  \n              -71.04  \n```\n:::\n:::\n\n\nAs before the Broadleaf line is used as the baseline regression and we can read off the values for its intercept and slope directly:\n\nBroadleaf:\n\\begin{equation}\nLight = 7798.57 + -221.13 \\times Depth\n\\end{equation}\n\nNote that this is different from the previous section, by allowing for an interaction all fitted values will change.\n\nFor the conifer line we will have a different intercept value and a different gradient value. As before the value underneath `speciesConifer` gives us the difference between the intercept of the conifer line and the broad leaf line. The new, additional term `depth:speciesConifer` tells us how the coefficient of `depth` varies for the conifer line i.e. how the gradient is different. Putting these two together gives us the following equation for the regression line conifer woodland:\n\nConifer:\n\\begin{equation}\nLight = (7798.57 + -2784.58) + (-221.13 + -71.04) \\times Depth\n\\end{equation}\n\n\\begin{equation}\nLight = 5014 + -292.2 \\times Depth\n\\end{equation}\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(light ~ depth + species + depth:species,\n   data = treelight_r)\n```\n:::\n\n\nor written using the short-hand:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(light ~ depth * species,\n   data = treelight_r)\n```\n:::\n\n\nThere really is absolutely no difference in the end result.\nEither way this gives us the following output:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = light ~ depth * species, data = treelight_r)\n\nCoefficients:\n         (Intercept)                 depth        speciesConifer  \n             7798.57               -221.13              -2784.58  \ndepth:speciesConifer  \n              -71.04  \n```\n:::\n:::\n\n\nAs before the Broadleaf line is used as the baseline regression and we can read off the values for its intercept and slope directly:\n\nBroadleaf:\n\\begin{equation}\nLight = 7798.57 + -221.13 \\times Depth\n\\end{equation}\n\nNote that this is different from the previous section, by allowing for an interaction all fitted values will change.\n\nFor the conifer line we will have a different intercept value and a different gradient value. As before the value underneath `speciesConifer` gives us the difference between the intercept of the conifer line and the broad leaf line. The new, additional term `depth:speciesConifer` tells us how the coefficient of `depth` varies for the conifer line i.e. how the gradient is different. Putting these two together gives us the following equation for the regression line conifer woodland:\n\nConifer:\n\\begin{equation}\nLight = (7798.57 + -2784.58) + (-221.13 + -71.04) \\times Depth\n\\end{equation}\n\n\\begin{equation}\nLight = 5014 + -292.2 \\times Depth\n\\end{equation}\n\n## Python\n\nWe've actually created this model before, but for clarity we'll define it here again.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula = \"light ~ depth * C(species)\",\n                data = treelight_py)\n\n# and get the fitted parameters of the model\nlm_treelight_py = model.fit()\n```\n:::\n\n\nWe get the model parameters as follows:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(lm_treelight_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  light   R-squared:                       0.938\nModel:                            OLS   Adj. R-squared:                  0.928\nMethod:                 Least Squares   F-statistic:                     95.71\nDate:                Wed, 26 Oct 2022   Prob (F-statistic):           1.19e-11\nTime:                        14:15:32   Log-Likelihood:                -174.91\nNo. Observations:                  23   AIC:                             357.8\nDf Residuals:                      19   BIC:                             362.4\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n===============================================================================================\n                                  coef    std err          t      P>|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------\nIntercept                    7798.5655    298.623     26.115      0.000    7173.541    8423.590\nC(species)[T.Conifer]       -2784.5833    442.274     -6.296      0.000   -3710.274   -1858.893\ndepth                        -221.1256     61.802     -3.578      0.002    -350.478     -91.773\ndepth:C(species)[T.Conifer]   -71.0357     81.309     -0.874      0.393    -241.217      99.145\n==============================================================================\nOmnibus:                        1.435   Durbin-Watson:                   2.176\nProb(Omnibus):                  0.488   Jarque-Bera (JB):                1.269\nSkew:                           0.444   Prob(JB):                        0.530\nKurtosis:                       2.267   Cond. No.                         31.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nAs before the Broadleaf line is used as the baseline regression and we can read off the values for its intercept and slope directly:\n\nBroadleaf:\n\\begin{equation}\nLight = 7798.57 + -221.13 \\times Depth\n\\end{equation}\n\nNote that this is different from the previous section, by allowing for an interaction all fitted values will change.\n\nFor the conifer line we will have a different intercept value and a different gradient value. As before the value next to `C(species)[T.Conifer]` gives us the difference between the intercept of the conifer line and the broad leaf line. The interaction term `depth:C(species)[T.Conifer]` tells us how the coefficient of `depth` varies for the conifer line i.e. how the gradient is different. Putting these two together gives us the following equation for the regression line conifer woodland:\n\nConifer:\n\\begin{equation}\nLight = (7798.57 + -2784.58) + (-221.13 + -71.04) \\times Depth\n\\end{equation}\n\n\\begin{equation}\nLight = 5014 + -292.2 \\times Depth\n\\end{equation}\n:::\n\nThese also happen to be exactly the regression lines that you would get by calculating a linear regression on each group’s data separately.\n\n## Exercise: Clover and yarrow\nClover and yarrow field trials\n\nThe `data/CS4-clover.csv` data set contains information on field trials at three different farms (`A`, `B` and `C`). Each farm recorded the yield of clover in each of ten fields along with the density of yarrow stalks in each field.\n\n* Investigate how clover yield is affected by yarrow stalk density. Is there evidence of competition between the two species?\n* Is there a difference between farms?\n\n::: {.callout-tip collapse=\"true\"}\n## Answer\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n::: {.cell}\n\n```{.r .cell-code}\nclover <- read_csv(\"data/CS4-clover.csv\")\n```\n:::\n\n\nThis data set has three variables; `yield` (which is the response variable), `yarrow` (which is a continuous predictor variable) and `farm` (which is the categorical predictor variables). As always we'll visualise the data first:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the data\nclover %>% \n  ggplot(aes(x = yarrow, y = yield,\n             colour = farm)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Dark2\")\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-53-1.png){width=672}\n:::\n:::\n\n\nLooking at this plot as it stands, it's pretty clear that yarrow density has a significant effect on yield, but it's pretty hard to see from the plot whether there is any effect of `farm`, or whether there is any interaction. In order to work that out we'll want to add the regression lines for each `farm` separately.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the data\nclover %>% \n  ggplot(aes(x = yarrow, y = yield,\n             colour = farm, group = farm)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_brewer(palette = \"Dark2\")\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n:::\n\n\nThe regression lines are very close together, and it looks very much as if there isn't any interaction, but also that there isn't any effect of `farm`. Let's carry out the analysis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_clover <- lm(yield ~ yarrow * farm,\n                data = clover)\n\nanova(lm_clover)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: yield\n            Df Sum Sq Mean Sq F value    Pr(>F)    \nyarrow       1 8538.3  8538.3 28.3143 1.847e-05 ***\nfarm         2    3.8     1.9  0.0063    0.9937    \nyarrow:farm  2  374.7   187.4  0.6213    0.5457    \nResiduals   24 7237.3   301.6                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThis confirms our suspicions from looking at the plot. There isn't any interaction between `yarrow` and `farm.` `yarrow` density has a statistically significant effect on `yield` but there isn't any difference between the different farms on the yields of clover.\n\nLet's check the assumptions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_clover %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-56-1.png){width=672}\n:::\n:::\n\n\nThis is a borderline case.\n\n1. Normality is fine (Q-Q plot)\n2. There aren't any highly influential points (Cook's D plot)\n3. There is a strong suggestion of heterogeneity of variance (Location-Scale plot). Most of the points are relatively close to the regression lines, but the there is a much great spread of points low yarrow density (which corresponds to high yield values, which is what the predicted values correspond to).\n4. Finally, there is a slight suggestion that the data might not be linear, that it might curve slightly (Residual plot).\n\n## R\nFirst, we read in the data. I'm using the `stringsAsFactors = TRUE` argument to ensure that the farms are viewed as categorical variables. This is important for later. We could have also converted them to factors when we needed to.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclover_r <- read.csv(\"data/CS4-clover.csv\",\n                   stringsAsFactors = TRUE)\n```\n:::\n\n\nThis dataset has three variables; `yield` (which is the response variable), `yarrow` (which is a continuous predictor variable) and `farm` (which is the categorical predictor variables). As always we'll visualise the data first:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(yield ~ yarrow,\n     data = clover_r, col = farm)\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-58-1.png){width=672}\n:::\n:::\n\n\nIn our plot this means that farm `A` is black, farm `B` is red and farm `C` is green.\n\nAs it stands, it's pretty clear that yarrow density has a significant effect on yield, but it's pretty hard to see from the plot whether there is any effect of `farm`, or whether there is any interaction. In order to work that out we'll want to add the regression lines (AKA lines of best fit) for each `farm` separately.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(yield ~ yarrow,\n     data = clover_r, col = farm)\n\nfarmA <- subset(clover_r, subset = (farm == \"A\"))\nfarmB <- subset(clover_r, subset = (farm == \"B\"))\nfarmC <- subset(clover_r, subset = (farm == \"C\"))\n\nlmA <- lm(yield ~ yarrow, data = farmA)\nlmB <- lm(yield ~ yarrow, data = farmB)\nlmC <- lm(yield ~ yarrow, data = farmC)\n\nabline(lmA, col = \"black\")\nabline(lmB, col = \"red\")\nabline(lmC, col = \"green\")\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-59-1.png){width=672}\n:::\n:::\n\n\nThe lines of best fit are very close together, and it looks very much as if there isn't any interaction, but also that there isn't any effect of `Farm`. Let's carry out the analysis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_clover_r <- lm(yield ~ yarrow * farm,\n                data = clover_r)\n\nanova(lm_clover_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: yield\n            Df Sum Sq Mean Sq F value    Pr(>F)    \nyarrow       1 8538.3  8538.3 28.3143 1.847e-05 ***\nfarm         2    3.8     1.9  0.0063    0.9937    \nyarrow:farm  2  374.7   187.4  0.6213    0.5457    \nResiduals   24 7237.3   301.6                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThis confirms our suspicions from looking at the plot. There isn't any interaction between `yarrow` and `farm.` `yarrow` density has a statistically significant effect on `yield` but there isn't any difference between the different farms on the yields of clover.\n\nLet's check the assumptions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(lm_clover_r)\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-61-1.png){width=672}\n:::\n:::\n\n\nThis is a borderline case.\n\n1. Normality is fine (Normal Q-Q)\n2. There aren't any highly influential points (Residuals vs Leverage)\n3. There is a strong suggestion of heterogeneity of variance. Most of the points are relatively close to the lines of best fit, but the there is a much great spread of points low Yarrow density (which corresponds to high Yield values, which is what the fitted values correspond to).\n4. Finally, there is a slight suggestion that the data might not be linear, that it might curve slightly.\n\n## Python\n\n::: {.cell}\n\n```{.python .cell-code}\nclover_py = pd.read_csv(\"data/CS4-clover.csv\")\n```\n:::\n\n\nThis data set has three variables; `yield` (which is the response variable), `yarrow` (which is a continuous predictor variable) and `farm` (which is the categorical predictor variables). As always we'll visualise the data first:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# plot the data\n(\n  ggplot(clover_py,\n       aes(x = \"yarrow\", y = \"yield\", colour = \"farm\")) +\n       geom_point() +\n       scale_color_brewer(type = \"qual\", palette = \"Dark2\")\n )\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-63-1.png){width=614}\n:::\n:::\n\n\nLooking at this plot as it stands, it's pretty clear that yarrow density has a significant effect on yield, but it's pretty hard to see from the plot whether there is any effect of `farm`, or whether there is any interaction. In order to work that out we'll want to add the regression lines for each `farm` separately.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# plot the data\n(\n  ggplot(clover_py,\n       aes(x = \"yarrow\", y = \"yield\", colour = \"farm\")) +\n       geom_point() +\n       geom_smooth(method = \"lm\", se = False) +\n       scale_color_brewer(type = \"qual\", palette = \"Dark2\")\n )\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-64-3.png){width=614}\n:::\n:::\n\n\nThe regression lines are very close together, and it looks very much as if there isn't any interaction, but also that there isn't any effect of `farm`. \n\nLet's carry out the analysis. First, we need to change the name of the `yield` column, because `yield` is a keyword in Python, and we can't use it inside the model formula. How annoying.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# rename yield column\nclover_py = clover_py.rename(columns = {\"yield\": \"clover_yield\"})\n\n# create a linear model\nmodel = smf.ols(formula = \"clover_yield ~ yarrow * C(farm)\",\n                data = clover_py)\n\n# and get the fitted parameters of the model\nlm_clover_py = model.fit()\n```\n:::\n\n\nPerform an ANOVA on the model:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsm.stats.anova_lm(lm_clover_py, typ = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  df       sum_sq      mean_sq          F    PR(>F)\nC(farm)          2.0     1.290667     0.645333   0.002140  0.997862\nyarrow           1.0  8540.833898  8540.833898  28.322674  0.000018\nyarrow:C(farm)   2.0   374.718749   187.359374   0.621312  0.545659\nResidual        24.0  7237.311354   301.554640        NaN       NaN\n```\n:::\n:::\n\n\nThis confirms our suspicions from looking at the plot. There isn't any interaction between `yarrow` and `farm.` `yarrow` density has a statistically significant effect on `yield` but there isn't any difference between the different farms on the yields of clover.\n\nLet's check the assumptions:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndgplots(lm_clover_py)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/dgplots/2022_10_26-02:15:39_pm_dgplots.png){width=786}\n:::\n:::\n\n\nThis is a borderline case.\n\n1. Normality is fine (Q-Q plot)\n2. There aren't any highly influential points (Influential points plot)\n3. There is a strong suggestion of heterogeneity of variance (Location-Scale plot). Most of the points are relatively close to the regression lines, but the there is a much great spread of points low yarrow density (which corresponds to high yield values, which is what the predicted values correspond to).\n4. Finally, there is a slight suggestion that the data might not be linear, that it might curve slightly (Residuals plot).\n:::\n\nWe have two options; both of which are arguably OK to do in real life.\n\n1. We can claim that these assumptions are well enough met and just report the analysis that we've just done.\n2. We can decide that the analysis is not appropriate and look for other options.\n    a. We can try to transform the data by taking logs of `yield.` This might fix both of our problems: taking logs of the response variable has the effect of improving heterogeneity of variance when the Residuals plot is more spread out on the right vs. the left (like ours). It also is appropriate if we think the true relationship between the response and predictor variables is exponential rather than linear (which we might have). We do have the capabilities to try this option.\n    b. We could try a permutation based approach (beyond the remit of this course, and actually a bit tricky in this situation). This wouldn't address the non-linearity but it would deal with the variance assumption.\n    c. We could come up with a specific functional, mechanistic relationship between yarrow density and clover yield based upon other aspects of their biology. For example there might be a threshold effect such that for yarrow densities below a particular value, clover yields are unaffected, but as soon as yarrow values get above that threshold the clover yield decreases (maybe even linearly). This would require a much better understanding of clover-yarrow dynamics (of which I personally know very little).\n\nLet's do a quick little transformation of the data, and repeat our analysis see if our assumptions are better met this time (just for the hell of it):\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot log-transformed data\nclover %>% \n  ggplot(aes(x = yarrow, y = log(yield), colour = farm)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_brewer(palette = \"Dark2\")\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-70-1.png){width=672}\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(log(yield) ~ yarrow,\n     data = clover_r, col = farm)\n\nlmlogA <- lm(log(yield) ~ yarrow, data = farmA)\nlmlogB <- lm(log(yield) ~ yarrow, data = farmB)\nlmlogC <- lm(log(yield) ~ yarrow, data = farmC)\n\nabline(lmlogA, col = \"black\")\nabline(lmlogB, col = \"red\")\nabline(lmlogC, col = \"green\")\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-71-1.png){width=672}\n:::\n:::\n\n\n## Python\n\nTo log-transform our data, we require `numpy`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# import numpy\nimport numpy as np\n```\n:::\n\n\nFirst, we create a new column containing the log-transformed data:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclover_py[\"log_yield\"] = np.log(clover_py[\"clover_yield\"])\n```\n:::\n\n\nThen we can plot them:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# plot log-transformed data\n(\n  ggplot(clover_py,\n         aes(x = \"yarrow\", y = \"log_yield\", colour = \"farm\")) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = False) +\n  scale_color_brewer(type = \"qual\", palette = \"Dark2\")\n)\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-74-1.png){width=614}\n:::\n:::\n\n\n:::\n\nAgain, this looks plausible. There's a noticeable outlier from Farm B (data point at the bottom of the plot) but otherwise we see that: there probably isn't an interaction; there is likely to be an effect of `yarrow` on `log_yield`; and there probably isn't any difference between the farms.\n\nLet's do the analysis:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define linear model\nlm_log_clover <- lm(log(yield) ~ yarrow * farm,\n                    data = clover)\n\nanova(lm_log_clover)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: log(yield)\n            Df  Sum Sq Mean Sq F value   Pr(>F)    \nyarrow       1 10.6815 10.6815 27.3233 2.34e-05 ***\nfarm         2  0.0862  0.0431  0.1103   0.8960    \nyarrow:farm  2  0.8397  0.4199  1.0740   0.3575    \nResiduals   24  9.3823  0.3909                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nWoop. All good so far. We have the same conclusions as before in terms of what is significant and what isn't. Now we just need to check the assumptions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_log_clover %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-76-1.png){width=672}\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_log_clover_r <- lm(log(yield) ~ yarrow * farm,\n             data = clover_r)\nanova(lm_log_clover_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: log(yield)\n            Df  Sum Sq Mean Sq F value   Pr(>F)    \nyarrow       1 10.6815 10.6815 27.3233 2.34e-05 ***\nfarm         2  0.0862  0.0431  0.1103   0.8960    \nyarrow:farm  2  0.8397  0.4199  1.0740   0.3575    \nResiduals   24  9.3823  0.3909                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nWoop. All good so far. We have the same conclusions as before in terms of what is significant and what isn't. Now we just need to check the assumptions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(lm_log_clover_r)\n```\n\n::: {.cell-output-display}\n![](cs4_practical_linear-regression-grouped-data_files/figure-html/unnamed-chunk-78-1.png){width=672}\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula = \"log_yield ~ yarrow * C(farm)\",\n                data = clover_py)\n\n# and get the fitted parameters of the model\nlm_log_clover_py = model.fit()\n```\n:::\n\n\n\nPerform an ANOVA on the model:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsm.stats.anova_lm(lm_clover_py, typ = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  df       sum_sq      mean_sq          F    PR(>F)\nC(farm)          2.0     1.290667     0.645333   0.002140  0.997862\nyarrow           1.0  8540.833898  8540.833898  28.322674  0.000018\nyarrow:C(farm)   2.0   374.718749   187.359374   0.621312  0.545659\nResidual        24.0  7237.311354   301.554640        NaN       NaN\n```\n:::\n:::\n\n\nWoop. All good so far. We have the same conclusions as before in terms of what is significant and what isn't. Now we just need to check the assumptions:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndgplots(lm_clover_py)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/dgplots/2022_10_26-02:15:44_pm_dgplots.png){width=772}\n:::\n:::\n\n:::\n\nWell, this is actually a better set of diagnostic plots. Whilst one data point (for example in the Q-Q plot) is a clear outlier, if we ignore that point then all of the other plots do look better.\n\nSo now we know that `yarrow` is a significant predictor of `yield` and we're happy that the assumptions have been met.\n:::\n\n## Key points\n\n::: callout-note\n- A linear regression analysis with grouped data is used when we have one categorical and one continuous predictor variable, together with one continuous response variable\n- We can visualise the data by plotting a regression line together with the original data\n- When performing an ANOVA, we need to check for interaction terms\n- We check the underlying assumptions using diagnostic plots\n- We can create an equation for the regression line for each group in the data using the parameter from the linear model output\n:::\n",
    "supporting": [
      "cs4_practical_linear-regression-grouped-data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}