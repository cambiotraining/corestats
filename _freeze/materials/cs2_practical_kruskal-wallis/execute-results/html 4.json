{
  "hash": "719fc5b6b6d5a58ff0ee27315544310f",
  "result": {
    "markdown": "---\ntitle: \"Kruskal-Wallis\"\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n::: callout-tip\n## Learning outcomes\n\n**Questions**\n\n-   How do I analyse multiple samples of continuous data if the data are not normally distributed?\n-   What is a Kruskal-Wallis test?\n-   How do I check for differences between groups?\n\n**Objectives**\n\n-   Be able to perform an Kruskal-Wallis test in R\n-   Understand the output of the test and evaluate the assumptions\n-   Be able to perform post-hoc testing after a Kruskal-Wallis test\n:::\n\n## Purpose and aim\n\nThe Kruskal-Wallis one-way analysis of variance test is an analogue of ANOVA that can be used when the assumption of normality cannot be met. In this way it is an extension of the Mann-Whitney test for two groups.\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n### Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n```\n:::\n\n\n### Functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Performs a Kruskal-Wallis test\nrstatix::kruskal_test()\n\n# Performs Dunn's test for pairwise multiple comparisons of the ranked data\nrstatix::dunn_test()\n```\n:::\n\n\n## R\n\n### Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Library for performing Dunn's test\nlibrary(dunn.test)\n```\n:::\n\n\n### Functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Performs a Kruskal-Wallis test\nstats::kruskal.test()\n\n# Performs Dunn's test for pairwise multiple comparisons of the ranked data\ndunn.test::dunn.test()\n```\n:::\n\n\n## Python\n:::\n:::\n\n## Data and hypotheses\n\nFor example, suppose a behavioural ecologist records the rate at which [spider monkeys](https://en.wikipedia.org/wiki/Spider_monkey) behaved aggressively towards one another as a function of closely related the two monkeys are. The familiarity of the two monkeys involved in each interaction is classified as `high`, `low` or `none.` We want to test if the data support the hypothesis that aggression rates differ according to strength of relatedness. We form the following null and alternative hypotheses:\n\n-   $H_0$: The median aggression rates for all types of familiarity are the same\n-   $H_1$: The median aggression rates are not all equal\n\nWe will use a Kruskal-Wallis test to check this.\n\nThe data are stored in the file `data/CS2-spidermonkey.csv`.\n\n## Summarise and visualise\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\nFirst we read the data in:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspidermonkey <- read_csv(\"data/CS2-spidermonkey.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# look at the data\nspidermonkey\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 21 × 3\n      id aggression familiarity\n   <dbl>      <dbl> <chr>      \n 1     1        0.2 high       \n 2     2        0.1 high       \n 3     3        0.4 high       \n 4     4        0.8 high       \n 5     5        0.3 high       \n 6     6        0.5 high       \n 7     7        0.2 high       \n 8     8        0.5 low        \n 9     9        0.4 low        \n10    10        0.3 low        \n# … with 11 more rows\n```\n:::\n\n```{.r .cell-code}\n# summarise the data\nspidermonkey %>% \n  select(-id) %>% \n  group_by(familiarity) %>% \n  get_summary_stats(type = \"common\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 11\n  familiarity variable       n   min   max median   iqr  mean    sd    se    ci\n  <chr>       <chr>      <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 high        aggression     7   0.1   0.8    0.3  0.25 0.357 0.237 0.09  0.219\n2 low         aggression     7   0.3   1.2    0.5  0.3  0.629 0.315 0.119 0.291\n3 none        aggression     7   0.9   1.6    1.2  0.25 1.26  0.23  0.087 0.213\n```\n:::\n\n```{.r .cell-code}\n# create boxplot\nspidermonkey %>% \n  ggplot(aes(x = familiarity, y = aggression)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](cs2_practical_kruskal-wallis_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## R\n\nFirst we read the data in:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspidermonkey <- read.csv(\"data/CS2-spidermonkey.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# look at the data format\nhead(spidermonkey)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  id aggression familiarity\n1  1        0.2        high\n2  2        0.1        high\n3  3        0.4        high\n4  4        0.8        high\n5  5        0.3        high\n6  6        0.5        high\n```\n:::\n\n```{.r .cell-code}\n# summarise the data\naggregate(aggression ~ familiarity, data = spidermonkey, summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  familiarity aggression.Min. aggression.1st Qu. aggression.Median\n1        high       0.1000000          0.2000000         0.3000000\n2         low       0.3000000          0.4500000         0.5000000\n3        none       0.9000000          1.1500000         1.2000000\n  aggression.Mean aggression.3rd Qu. aggression.Max.\n1       0.3571429          0.4500000       0.8000000\n2       0.6285714          0.7500000       1.2000000\n3       1.2571429          1.4000000       1.6000000\n```\n:::\n\n```{.r .cell-code}\n# create boxplot\nboxplot(aggression ~ familiarity, data = spidermonkey)\n```\n\n::: {.cell-output-display}\n![](cs2_practical_kruskal-wallis_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Python\n:::\n\nThe data appear to show a very significant difference in aggression rates between the three types of familiarity. We would probably expect a reasonably significant result here.\n\n## Assumptions\n\nTo use the Kruskal-Wallis test we have to make three assumptions:\n\n1.  The parent distributions from which the samples are drawn have the same shape (if they're normal then we should use a one-way ANOVA)\n2.  Each data point in the samples is independent of the others\n3.  The parent distributions should have the same variance\n\nIndependence we'll ignore as usual. Similar shape is best assessed from the earlier visualisation of the data. That means that we only need to check equality of variance.\n\n### Equality of variance\n\nWe test for equality of variance using Levene's test (since we can't assume normal parent distributions which rules out Bartlett's test).\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# perform Levene's test\nspidermonkey %>% \n  levene_test(aggression ~ familiarity)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  <int> <int>     <dbl> <dbl>\n1     2    18     0.114 0.893\n```\n:::\n:::\n\n\nThe relevant p-value is given in the `p` column (0.893). As it is quite large we see that each group do appear to have the same variance.\n\nThere is also a warning about `group coerced to factor`. There is no need to worry about this - Levene's test needs to compare different groups and because `aggression` is encoded as a numeric value, it converts it to a categorical one before running the test.\n\n## R\n\nLevene's test is not included in the default R packages and may require the installation of an additional package called `car` (Companion to Applied Regression).\n\nTo install the `car` package, run the following command in your console:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"car\")\n```\n:::\n\n\nAlternatively, go to <kbd>Tools</kbd> \\> <kbd>Install packages...</kbd> \\> <kbd>Packages</kbd>, type in `car` and press <kbd>Install</kbd>\n\nRemember to load the library with `library(car)`.\n\nPerform Levene's test on the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleveneTest(aggression ~ familiarity, data = spidermonkey)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  2  0.1139  0.893\n      18               \n```\n:::\n:::\n\n\nThe relevant p-value is given on the 3rd line (`Pr(>F) = 0.893`). As it is quite large we see that each group do appear to have the same variance.\n\nThere is also a warning about `group coerced to factor`. There is no need to worry about this - Levene's test needs to compare different groups and because `aggression` is encoded as a numeric value, it converts it to a categorical one before running the test.\n\n## Python\n:::\n\n## Implement and interpret the test\n\nPerform a Kruskal-Wallis test on the data:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# implement Kruskal-Wallis test\nspidermonkey %>% \n  kruskal_test(aggression ~ familiarity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 6\n  .y.            n statistic    df       p method        \n* <chr>      <int>     <dbl> <int>   <dbl> <chr>         \n1 aggression    21      13.6     2 0.00112 Kruskal-Wallis\n```\n:::\n:::\n\n\n-   The `kruskal_test()` takes the formula in the following format: `variable ~ category`\n\nThe p-value is given in the `p` column. This shows us the probability of getting samples such as ours if the null hypothesis were actually true.\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkruskal.test(aggression ~ familiarity, data = spidermonkey)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tKruskal-Wallis rank sum test\n\ndata:  aggression by familiarity\nKruskal-Wallis chi-squared = 13.597, df = 2, p-value = 0.001115\n```\n:::\n:::\n\n\n-   The first argument must be in the formula format: `variable ~ category`\n-   If the data are stored in stacked format, then the second argument must be the name of the data frame\n\nThe p-value is given in the 3rd line. This shows us the probability of getting samples such as ours if the null hypothesis were actually true.\n\n## Python\n:::\n\nSince the p-value is very small (much smaller than the standard significance level of 0.05) we can say \"that it is very unlikely that these three samples came from the same parent distribution and as such we can reject our null hypothesis\" and state that:\n\n> A one-way Kruskal-Wallis rank sum test showed that aggression rates between spidermonkeys depends upon the degree of familiarity between them (KW = 13.597, df = 2, p = 0.0011).\n\n## Post-hoc testing (Dunn's test)\n\nThe equivalent of Tukey's range test for non-normal data is **Dunn's test**.\n\nDunn's test is used to check for significant differences in group medians:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# perform Dunn's test\nspidermonkey %>% \n  dunn_test(aggression ~ familiarity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 9\n  .y.        group1 group2    n1    n2 statistic        p    p.adj p.adj.signif\n* <chr>      <chr>  <chr>  <int> <int>     <dbl>    <dbl>    <dbl> <chr>       \n1 aggression high   low        7     7      1.41 0.160    0.160    ns          \n2 aggression high   none       7     7      3.66 0.000257 0.000771 ***         \n3 aggression low    none       7     7      2.25 0.0245   0.0490   *           \n```\n:::\n:::\n\n\nThe `dunn_test()` function performs a Kruskal-Wallis test on the data, followed by a post-hoc pairwise multiple comparison.\n\nThe comparison between the pairs of groups is reported in the table at the bottom. Each row contains a single comparison. We are interested in the `p` and `p.adj` columns, which contain the the p-values that we want. This table shows that there isn't a significant difference between the high and low groups, as the p-value (0.1598) is too high. The other two comparisons between the high familiarity and no familiarity groups and between the low and no groups are significant though.\n\nThe `dunn_test()` function has several arguments, of which the `p.adjust.method` is likely to be of interest. Here you can define which method needs to be used to account for multiple comparisons. The default is `\"holm\"`. We'll cover more about this in the chapter on [Power analysis](#cs6-intro).\n\n## R\n\nDunn's test is also not included in the default R packages and may require the installation of an additional package called `dunn.test`.\n\nTo install the `dunn.test` package, run the following command in your console:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"dunn.test\")\n```\n:::\n\n\nAlternatively, go to <kbd>Tools</kbd> \\> <kbd>Install packages...</kbd> \\> <kbd>Packages</kbd>, type in `dunn.test` and press <kbd>Install</kbd>\n\nRemember to load the library with `library(dunn.test)`.\n\nTest for a significant difference in group medians:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndunn.test(spidermonkey$aggression, spidermonkey$familiarity,\n          altp = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 13.5972, df = 2, p-value = 0\n\n                           Comparison of x by group                            \n                                (No adjustment)                                \nCol Mean-|\nRow Mean |       high        low\n---------+----------------------\n     low |  -1.405820\n         |     0.1598\n         |\n    none |  -3.655132  -2.249312\n         |    0.0003*    0.0245*\n\nalpha = 0.05\nReject Ho if p <= alpha\n```\n:::\n:::\n\n\nNote that Dunn's test requires us to enter two arguments, the first is the vector of values and the second is the vector containing the category labels (i.e. the factor).\n\nYou can see that the `dunn.test()` function also performs a Kruskal-Wallis test on the data, and these results are reported initially.\n\nThe comparison between the pairs of groups is reported in the table at the bottom. Each cell in the table has two rows. The bottom row contains the p-values that we want. This table shows that there isn't a significant difference between the high and low groups, as the p-value (0.1598) is too high. The other two comparisons between the high familiarity and no familiarity groups and between the low and no groups are significant though.\n\n## Python\n:::\n\n## Exercise: Lobster weight (revisited)\n\nPerform a Kruskal-Wallis test and do a post-hoc test on the `lobster` data set.\n\n::: {.callout-tip collapse=\"true\"}\n## Answer\n\n### Hypothesis\n\n-   $H_0$ : all medians are equal\n-   $H_1$ : not all medians are equal\n\n### Import data, summarise and visualise\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n:::\n\n\nAll done previously.\n\n## R\n\n\n::: {.cell}\n\n:::\n\n\nAll done previously.\n\n## Python\n:::\n\n### Assumptions\n\nFrom before, since the data are normal enough they are definitely similar enough for a Kruskal-Wallis test and they do all have equality of variance from out assessment of the diagnostic plots. For completeness though we will look at Levene's test.\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlobsters %>% \n  levene_test(weight ~ diet)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  <int> <int>     <dbl> <dbl>\n1     2    15   0.00280 0.997\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleveneTest(weight ~ diet, data = lobsters)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  2  0.0028 0.9972\n      15               \n```\n:::\n:::\n\n\n## Python\n:::\n\nGiven that the p-value is so high, this again agrees with our previous assessment that the equality of variance assumption is well met. Rock on.\n\n### Kruskal-Wallis test\n\nSo, we perform the Kruskall-Wallis test.\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# implement Kruskal-Wallis test\nlobsters %>% \n  kruskal_test(weight ~ diet)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 6\n  .y.        n statistic    df     p method        \n* <chr>  <int>     <dbl> <int> <dbl> <chr>         \n1 weight    18      3.26     2 0.196 Kruskal-Wallis\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkruskal.test(weight ~ diet, data = lobsters)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tKruskal-Wallis rank sum test\n\ndata:  weight by diet\nKruskal-Wallis chi-squared = 3.2565, df = 2, p-value = 0.1963\n```\n:::\n:::\n\n\n## Python\n:::\n\n> A Kruskal-Wallis test indicated that the median weight of juvenile lobsters did not differ significantly between diets (KW = 3.26, df = 2, p = 0.20).\n\n### Post-hoc testing\n\nIn this case we should not be doing any post-hoc testing, because we did not detect any statistically significant differences. Doing so anyway and then reporting any incidental groups that *would* differ, would be p-hacking.\n:::\n\n## Key points\n\n::: callout-note\n-   We use a Kruskal-Wallis test to see if there is a difference in medians between multiple continuous response variables\n-   We assume parent distributions have the same shape; each data point is independent and the parent distributions have the same variance\n-   We test for equality of variance using Levene's test\n-   Post-hoc testing to check for significant differences in the group medians is done with Dunn's test\n:::\n",
    "supporting": [
      "cs2_practical_kruskal-wallis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}