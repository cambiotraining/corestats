{
  "hash": "757e96e18fa8d4faeebf16308b9edad7",
  "result": {
    "markdown": "---\ntitle: \"ANOVA\"\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n::: callout-tip\n## Learning outcomes\n\n**Questions**\n\n-   How do I analyse multiple samples of continuous data?\n-   What is an ANOVA?\n-   How do I check for differences between groups?\n\n**Objectives**\n\n-   Be able to perform an ANOVA in R\n-   Understand the ANOVA output and evaluate the assumptions\n-   Understand what post-hoc testing is and how to do this in R\n:::\n\n## Purpose and aim\n\nAnalysis of variance or ANOVA is a test than can be used when we have multiple samples of continuous response data. Whilst it is possible to use ANOVA with only two samples, it is generally used when we have three or more groups. It is used to find out if the samples came from parent distributions with the same mean. It can be thought of as a generalisation of the two-sample Student's t-test.\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n### Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n```\n:::\n\n\n### Functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Computes summary statistics\nrstatix::get_summary_stats()\n\n# Perform Tukey's range test\nrstatix::tukey_hsd()\n\n# Creates diagnostic plots\nggResidpanel::resid_panel()\n```\n:::\n\n\n## R\n\n### Functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fits a linear model  \nstats::lm()\n\n# Carries out an ANOVA on a linear model \nstats::anova()\n\n# Plots a Q-Q plot for comparison with a normal distribution\nstats::qqnorm()\n\n# Adds a comparison line to the Q-Q plot\nstats::qqline()\n\n# Performs a Shapiro-Wilk test for normality\nstats::shapiro.test()\n\n# Perform Tukey's range test\nstats::TukeyHSD()\n```\n:::\n\n\n## Python\n\n### Libraries\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n```\n:::\n\n\n### Functions\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Summary statistics\npandas.DataFrame.describe()\n\n# Plots the first few rows of a DataFrame\npandas.DataFrame.head()\n\n# Query the columns of a DataFrame with a boolean expression\npandas.DataFrame.query()\n\n# Reads in a .csv file\npandas.read_csv\n\n# Performs an analysis of variance\npingouin.anova()\n\n# Tests for equality of variance\npingouin.homoscedasticity()\n\n# Performs the Shapiro-Wilk test for normality\npingouin.normality()\n\n# Creates a model from a formula and data frame\nstatsmodels.formula.api.ols\n\n# Creates an ANOVA table for one or more fitted linear models\nstatsmodels.stats.anova.anova_lm\n```\n:::\n\n\n:::\n:::\n\n## Data and hypotheses\n\nFor example, suppose we measure the feeding rate of oyster catchers (shellfish per hour) at three sites characterised by their degree of shelter from the wind, imaginatively called `exposed` (E), `partially sheltered` (P) and `sheltered` (S). We want to test whether the data support the hypothesis that feeding rates don't differ between locations. We form the following null and alternative hypotheses:\n\n-   $H_0$: The mean feeding rates at all three sites is the same $\\mu E = \\mu P = \\mu S$\n-   $H_1$: The mean feeding rates are not all equal.\n\nWe will use a one-way ANOVA test to check this.\n\n-   We use a **one-way** ANOVA test because we only have one predictor variable (the categorical variable location).\n-   We're using **ANOVA** because we have more than two groups and we don't know any better yet with respect to the exact assumptions.\n\nThe data are stored in the file `data/CS2-oystercatcher.csv`.\n\n## Summarise and visualise\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\nFirst we read in the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\noystercatcher <- read_csv(\"data/CS2-oystercatcher-feeding.csv\")\n\n# and have a look\noystercatcher\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 120 × 2\n   site    feeding\n   <chr>     <dbl>\n 1 exposed    12.2\n 2 exposed    13.1\n 3 exposed    17.9\n 4 exposed    13.9\n 5 exposed    14.1\n 6 exposed    18.4\n 7 exposed    15.0\n 8 exposed    10.3\n 9 exposed    11.8\n10 exposed    12.5\n# … with 110 more rows\n```\n:::\n:::\n\n\nThe `oystercatcher` data set contains two columns:\n\n1.  a `site` column with information on the amount of shelter of the feeding location\n2.  a `feeding` column containing feeding rates\n\nNext, we get some basic descriptive statistics:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get some basic descriptive statistics\noystercatcher %>% \n  group_by(site) %>% \n  get_summary_stats(type = \"common\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 11\n  site      variable     n   min   max median   iqr  mean    sd    se    ci\n  <chr>     <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 exposed   feeding     40  8.35  18.6   13.9  3.40  13.8  2.44 0.386 0.781\n2 partial   feeding     40 10.8   23.0   16.9  2.82  17.1  2.62 0.414 0.838\n3 sheltered feeding     40 18.9   28.5   23.2  3.79  23.4  2.42 0.383 0.774\n```\n:::\n:::\n\n\nFinally, we plot the data by `site`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the data\noystercatcher %>% \n  ggplot(aes(x = site, y = feeding)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](cs2_practical_anova_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## R\n\nFirst we read in the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\noystercatcher_r <- read.csv(\"data/CS2-oystercatcher-feeding.csv\")\n\n# have a look\nhead(oystercatcher_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     site  feeding\n1 exposed 12.17551\n2 exposed 13.07392\n3 exposed 17.93969\n4 exposed 13.89178\n5 exposed 14.05166\n6 exposed 18.36498\n```\n:::\n:::\n\n\nThe `oystercatcher_r` data set contains two columns:\n\n1.  a `site` column with information on the amount of shelter of the feeding location\n2.  a `feeding` column containing feeding rates\n\nNext, we get some basic descriptive statistics. We have three groups, so to get the summary statistics by group we do the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\naggregate(feeding ~ site,\n          FUN = summary,\n          data = oystercatcher_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       site feeding.Min. feeding.1st Qu. feeding.Median feeding.Mean\n1   exposed     8.350801       12.184961      13.946420    13.822899\n2   partial    10.795969       15.601927      16.927683    17.081666\n3 sheltered    18.856999       21.403028      23.166246    23.355503\n  feeding.3rd Qu. feeding.Max.\n1       15.581748    18.560404\n2       18.416708    23.021250\n3       25.197096    28.451252\n```\n:::\n:::\n\n\nFinally, we plot the data by `site`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the data by site\nboxplot(feeding ~ site,\n        data = oystercatcher_r)\n```\n\n::: {.cell-output-display}\n![](cs2_practical_anova_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n## Python\n\nFirst, we read in the data.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# load the data\noystercatcher_py = pd.read_csv(\"data/CS2-oystercatcher-feeding.csv\")\n\n# and have a look\noystercatcher_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      site    feeding\n0  exposed  12.175506\n1  exposed  13.073917\n2  exposed  17.939687\n3  exposed  13.891783\n4  exposed  14.051663\n```\n:::\n:::\n\n\nThe `oystercatcher_py` data set contains two columns:\n\n1.  a `site` column with information on the amount of shelter of the feeding location\n2.  a `feeding` column containing feeding rates\n\nNext, we get some basic descriptive statistics *per group*. Here we use the `pd.groupby()` function to group by `site`. We only want to have summary statistics for the `feeding` variable, so we specify that as well:\n\n\n::: {.cell}\n\n```{.python .cell-code}\noystercatcher_py.groupby(\"site\")[\"feeding\"].describe()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           count       mean       std  ...        50%        75%        max\nsite                                   ...                                 \nexposed     40.0  13.822899  2.441974  ...  13.946420  15.581748  18.560404\npartial     40.0  17.081666  2.619906  ...  16.927683  18.416708  23.021250\nsheltered   40.0  23.355503  2.419825  ...  23.166246  25.197096  28.451252\n\n[3 rows x 8 columns]\n```\n:::\n:::\n\n\nFinally, we plot the data:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# plot the data\n(ggplot(oystercatcher_py,\n        aes(x = \"site\",\n            y = \"feeding\")) +\n     geom_boxplot())\n```\n\n::: {.cell-output-display}\n![](cs2_practical_anova_files/figure-html/unnamed-chunk-16-1.png){width=614}\n:::\n:::\n\n:::\n\nLooking at the data, there appears to be a noticeable difference in feeding rates between the three sites. We would probably expect a reasonably significant statistical result here.\n\n## Assumptions\n\nTo use an ANOVA test, we have to make three assumptions:\n\n1.  The parent distributions from which the samples are taken are normally distributed\n2.  Each data point in the samples is independent of the others\n3.  The parent distributions should have the same variance\n\nIn a similar way to the two-sample tests we will consider the normality and equality of variance assumptions both using tests and by graphical inspection (and ignore the independence assumption).\n\n### Normality\n\nFirst we perform a Shapiro-Wilk test on each site separately.\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\nWe take the data, and `group_by()` site:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Shapiro-Wilk test on each site\noystercatcher %>% \n    filter(site == \"exposed\") %>% \n    pull(feeding) %>% \n    shapiro.test()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  .\nW = 0.98859, p-value = 0.953\n```\n:::\n\n```{.r .cell-code}\noystercatcher %>% \n    filter(site == \"partial\") %>% \n    pull(feeding) %>% \n    shapiro.test()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  .\nW = 0.98791, p-value = 0.9398\n```\n:::\n\n```{.r .cell-code}\noystercatcher %>% \n    filter(site == \"sheltered\") %>% \n    pull(feeding) %>% \n    shapiro.test()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  .\nW = 0.97511, p-value = 0.5136\n```\n:::\n:::\n\n\n## R\n\nUnstack the data and perform a Shapiro-Wilk test on each group separately.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a new object (a list) that contains the unstacked data\nuns_oystercatcher <- unstack(oystercatcher_r,\n                             form = feeding ~ site)\n# have a look at the data\nhead(uns_oystercatcher)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   exposed  partial sheltered\n1 12.17551 15.20345  23.87176\n2 13.07392 16.53239  21.41133\n3 17.93969 13.64547  22.46681\n4 13.89178 23.02125  23.00843\n5 14.05166 20.39774  27.85932\n6 18.36498 14.03391  22.09400\n```\n:::\n:::\n\n\nNext, we perform the Shapiro-Wilk test on each group:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(uns_oystercatcher$exposed)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  uns_oystercatcher$exposed\nW = 0.98859, p-value = 0.953\n```\n:::\n\n```{.r .cell-code}\nshapiro.test(uns_oystercatcher$partial)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  uns_oystercatcher$partial\nW = 0.98791, p-value = 0.9398\n```\n:::\n\n```{.r .cell-code}\nshapiro.test(uns_oystercatcher$sheltered)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  uns_oystercatcher$sheltered\nW = 0.97511, p-value = 0.5136\n```\n:::\n:::\n\n\n## Python\n\nWe use the `pg.normality()` function to calculate the statistic. This requires:\n\n-   the `dv` dependent variable (`feeding` in our case)\n-   the `group` variable (`site`)\n-   and some data\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.normality(dv = \"feeding\",\n             group = \"site\",\n             data = oystercatcher_py)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  W      pval  normal\nexposed    0.988586  0.953029    True\npartial    0.987907  0.939833    True\nsheltered  0.975106  0.513547    True\n```\n:::\n:::\n\n:::\n\nWe can see that all three groups appear to be normally distributed which is good.\n\nFor ANOVA however, considering each group in turn is often considered quite excessive and, in most cases, it is sufficient to consider the normality of the combined set of *residuals* from the data. We'll explain residuals properly in the [next session](#cs3-intro) but effectively they are the difference between each data point and its group mean. The residuals can be obtained directly from a linear model fitted to the data.\n\nSo, we create a linear model, extract the residuals and check their normality:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define the model\nlm_oystercatcher <- lm(feeding ~ site,\n                       data = oystercatcher)\n\n# extract the residuals\nresid_oyster <- residuals(lm_oystercatcher)\n\n# perform Shapiro-Wilk test on residuals\nresid_oyster %>% \n  shapiro.test()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  .\nW = 0.99355, p-value = 0.8571\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define the model\nlm_oystercatcher_r <- lm(feeding ~ site,\n                         data = oystercatcher_r)\n\n# extract the residuals\nresid_oyster_r <- residuals(lm_oystercatcher_r)\n\n# perform Shapiro-Wilk test on residuals\nshapiro.test(resid_oyster_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  resid_oyster_r\nW = 0.99355, p-value = 0.8571\n```\n:::\n:::\n\n\n## Python\n\nUnfortunately `pingouin` does not have a straightforward way of extracting residuals (if you know more, please raise an issue!).\n\nTo get our residuals we use `statsmodels`, a module that provides functions for statistical models. We'll be using this in upcoming sessions, so you'll have a head start!\n\nAt this point you shouldn't concern yourself too much with the exact syntax, just run it an have a look.\n\n::: {.callout-note collapse=\"true\"}\n## Technical details (optional)\n\nWe need to import a few extra modules. First, we load the `statsmodels.api` module, which contains an `OLS()` function (Ordinary Least Squares - the equivalent of the `lm()` function in R).\n\nWe also import `stats.models.formula.api` so we can use the formula notation in our linear model. We define the formula as `formula= \"feeding ~ C(site)\"` with `C` conveying that the `site` variable is a category. Lastly we can `.fit()` the model.\n\nIf you're familiar with this stuff then you can look at the model itself by running `summary(lm_oystercatcher_py)`. But we'll cover all of this in later sessions.\n:::\n\nWe load the modules, define a linear model, create a `fit()` and we get the residuals from the linear model fit with `.resid`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula= \"feeding ~ C(site)\", data = oystercatcher_py)\n# and get the fitted parameters of the model\nlm_oystercatcher_py = model.fit()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# get the residuals from the model fit\n# and perform Shapiro-Wilk test\npg.normality(lm_oystercatcher_py.resid)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          W      pval  normal\n0  0.993545  0.857013    True\n```\n:::\n:::\n\n:::\n\nAgain, we can see that the combined residuals from all three groups appear to be normally distributed (which is as we would have expected given that they were all normally distributed individually!)\n\n### Equality of Variance\n\nWe now test for equality of variance using Bartlett's test (since we've just found that all of the individual groups are normally distributed).\n\nPerform Bartlett's test on the data:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check equality of variance\nbartlett.test(feeding ~ site,\n              data = oystercatcher)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBartlett test of homogeneity of variances\n\ndata:  feeding by site\nBartlett's K-squared = 0.29598, df = 2, p-value = 0.8624\n```\n:::\n:::\n\n\nWhere the relevant p-value is given on the 3rd line. Here we see that each group appears to have the same variance.\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check equality of variance\nbartlett.test(feeding ~ site,\n              data = oystercatcher)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBartlett test of homogeneity of variances\n\ndata:  feeding by site\nBartlett's K-squared = 0.29598, df = 2, p-value = 0.8624\n```\n:::\n:::\n\n\nWhere the relevant p-value is given on the 3rd line. Here we see that each group appears to have the same variance.\n\n## Python\n\nWe use the `homoscedasticity()` function from `pingouin` (homoscedasticity is another way of describing equality of variance). The default `method` is `levene`, so we need to specify that we want to use `bartlett`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.homoscedasticity(dv = \"feeding\",\n                    group = \"site\",\n                    method = \"bartlett\",\n                    data = oystercatcher_py)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 T      pval  equal_var\nbartlett  0.295983  0.862439       True\n```\n:::\n:::\n\n\nWhere the relevant p-value is given in the `pval` column. Here we see that each group appears to have the same variance.\n:::\n\n### Graphical interpretation and diagnostic plots\n\nAssessing assumptions via these tests can be cumbersome, but also a bit misleading at times. It reduces the answer to the question \"is the assumption met?\" to a yes/no, based on some statistic and associated p-value.\n\nThis does not convey that things aren't always so clear-cut and that there is a lot of grey area that we need to navigate. As such, assessing assumptions through graphical means - using diagnostic plots - is often preferred.\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\nIn the first session we already created diagnostic Q-Q plots directly from our data, using `stat_qq()` and `stat_qq_line()`. For more specific plots this becomes a bit cumbersome. There is an option to create ggplot-friendly diagnostic plots, using the `ggResidPanel` package.\n\nIf you haven't got `ggResidpanel` installed, please run the following code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install package\ninstall.packages(\"ggResidpanel\")\n\n# load library\nlibrary(ggResidpanel)\n```\n:::\n\n\nLet's create the diagnostic plots we're interested in using `ggResidPanel`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_oystercatcher %>% \n    resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n                smoother = TRUE)\n```\n\n::: {.cell-output-display}\n![](cs2_practical_anova_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\n-   The top left graph plots the **Residuals plot**. If the data are best explained by a linear line then there should be a uniform distribution of points above and below the horizontal blue line (and if there are sufficient points then the red line, which is a smoother line, should be on top of the blue line). This plot looks pretty good.\n-   The top right graph shows the **Q-Q plot** which allows a visual inspection of normality. If the residuals are normally distributed, then the points should lie on the diagonal blue line. This plot looks good.\n-   The bottom left **Location-Scale** graph allows us to investigate whether there is any correlation between the residuals and the predicted values and whether the variance of the residuals changes significantly. If not, then the red line should be horizontal. If there is any correlation or change in variance then the red line will not be horizontal. This plot is fine.\n-   The last graph shows the **Cook's distance** and tests if any one point has an unnecessarily large effect on the fit. A rule of thumb is that if any value is larger than 1.0, then it might have a large effect on the model. If not, then no point has undue influence. This plot is good. There are different ways to determine the threshold (apart from simply setting it to 1) and in this plot the blue dashed line is at `4/n`, with `n` being the number of samples. At this threshold there are some data points that may be influential, but I personally find this threshold rather strict.\n\n## R\n\nWe can create the default diagnostic plots in base R by simply plotting the `lm` object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a neat 2x2 window\npar(mfrow = c(2,2))\n# create the diagnostic plots\nplot(lm_oystercatcher)\n```\n\n::: {.cell-output-display}\n![](cs2_practical_anova_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# and return the window back to normal\npar(mfrow = c(1,1))\n```\n:::\n\n\nThe second line creates four diagnostic plots.\n\n-   The top left graph plots the **Residuals vs Fitted**. If the data are best explained by a linear line then there should be a uniform distribution of points above and below the horizontal red line. This plot looks pretty good.\n-   The top right graph shows the **Normal Q-Q plot** which allows a visual inspection of normality. If the residuals are normally distributed, then the points should lie on the diagonal dotted line. This plot looks good.\n-   The bottom left **Scale-Location** graph allows us to investigate whether there is any correlation between the residuals and the predicted values and whether the variance of the residuals changes significantly. If not, then the red line should be horizontal. If there is any correlation or change in variance then the red line will not be horizontal. This plot is fine.\n-   The bottom right **Residuals vs Factor Levels** plot shows the residuals for each group (= site). This plot is only displayed if there are equal number of observations in each group and we'll explain more about this in the next session.\n\n## Python\n\nUnfortunately Python doesn't provide a convenient way of displaying the same diagnostic plots as R does.\n\nI created a function `dgplots()` (which stands for Diagnostic Plots, very original I know...) that does this for you. All you need to do is create a linear model, get the fit and feed that to the `dgplots()` function.\n\nYou can of course plot the model values yourself by extracting them from the linear model fit, but this should provide a convenient way to avoid that kind of stuff.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndgplots(lm_oystercatcher_py)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/dgplots/2022_11_07-01:03:50_pm_dgplots.png){width=780}\n:::\n:::\n\n\n-   The top left graph plots the **Residuals plot**. If the data are best explained by a linear line then there should be a uniform distribution of points above and below the horizontal blue line (and if there are sufficient points then the red line, which is a smoother line, should be on top of the blue line). This plot looks pretty good.\n-   The top right graph shows the **Q-Q plot** which allows a visual inspection of normality. If the residuals are normally distributed, then the points should lie on the diagonal blue line. This plot looks good.\n-   The bottom left **Location-Scale** graph allows us to investigate whether there is any correlation between the residuals and the predicted values and whether the variance of the residuals changes significantly. If not, then the red line should be horizontal. If there is any correlation or change in variance then the red line will not be horizontal. This plot is fine.\n-   The last graph shows the **Influential points** and tests if any one point has an unnecessarily large effect on the fit. Here we're using the Cook's distance as a measure. A rule of thumb is that if any value is larger than 1.0, then it might have a large effect on the model. If not, then no point has undue influence. This plot is good. There are different ways to determine the threshold (apart from simply setting it to 1) and in this plot the blue dashed line is at `4/n`, with `n` being the number of samples. At this threshold there are some data points that may be influential, but I personally find this threshold rather strict.\n:::\n\nWe can see that these graphs are very much in line with what we've just looked at using the test, which is reassuring. The groups all appear to have the same spread of data, and the Q-Q plot shows that the assumption of normality is alright.\n\n::: callout-important\n## Assessing assumptions\n\nAt this stage, I should point out that I nearly always stick with the graphical method for assessing the assumptions of a test. Assumptions are rarely either completely met or not met and there is always some degree of personal assessment.\n\nWhilst the formal statistical tests (like Shapiro-Wilk) are technically fine, they can often create a false sense of things being absolutely right or wrong in spite of the fact that they themselves are still probabilistic statistical tests. In these exercises we are using both approaches whilst you gain confidence and experience in interpreting the graphical output and whilst it is absolutely fine to use both in the future I would strongly recommend that you don't rely solely on the statistical tests in isolation.\n:::\n\n## Implement and interpret the test\n\nPerform an ANOVA test on the data:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lm_oystercatcher)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: feeding\n           Df  Sum Sq Mean Sq F value    Pr(>F)    \nsite        2 1878.02  939.01  150.78 < 2.2e-16 ***\nResiduals 117  728.63    6.23                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThis takes the linear model (i.e. finds the means of the three groups and calculates a load of intermediary data that we need for the statistical analysis) and stores this information in an R object (which we've called `lm_oystercatcher`, but which you can call what you like).\n\nIn the output:\n\n-   The 1st line just tells you the that this is an ANOVA test\n-   The 2nd line tells you what the response variable is (in this case feeding)\n-   The 3rd, 4th and 5th lines are an ANOVA table which contain some useful values:\n    -   The `Df` column contains the degrees of freedom values on each row, 2 and 117 (which we can use for the reporting)\n    -   The `F` value column contains the F statistic, 150.78 (which again we'll need for reporting).\n    -   The p-value is 2.2e-16 and is the number directly under the `Pr(>F)` on the 4th line (to be precise, it is 4.13e-33 but anything smaller than 2.2e-16 gets reported as `< 2.2e-16`).\n    -   The other values in the table (in the `Sum Sq` and `Mean Sq`) columns are used to calculate the F statistic itself and we don't need to know these.\n-   The 6th line has some symbolic codes to represent how big (small) the p-value is; so, a p-value smaller than 0.001 would have a \\*\\*\\* symbol next to it (which ours does). Whereas if the p-value was between 0.01 and 0.05 then there would simply be a \\* character next to it, etc. Thankfully we can all cope with actual numbers and don't need a short-hand code to determine the reporting of our experiments (please tell me that's true...!)\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lm_oystercatcher_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: feeding\n           Df  Sum Sq Mean Sq F value    Pr(>F)    \nsite        2 1878.02  939.01  150.78 < 2.2e-16 ***\nResiduals 117  728.63    6.23                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThis takes the linear model (i.e. finds the means of the three groups and calculates a load of intermediary data that we need for the statistical analysis) and stores this information in an R object (which we've called `lm_oystercatcher_r`, but which you can call what you like).\n\nIn the output:\n\n-   The 1st line just tells you the that this is an ANOVA test\n-   The 2nd line tells you what the response variable is (in this case feeding)\n-   The 3rd, 4th and 5th lines are an ANOVA table which contain some useful values:\n-   The `Df` column contains the degrees of freedom values on each row, 2 and 117 (which we can use for the reporting)\n-   The `F` value column contains the F statistic, 150.78 (which again we'll need for reporting).\n-   The p-value is 2.2e-16 and is the number directly under the `Pr(>F)` on the 4th line (to be precise, it is 4.13e-33 but anything smaller than 2.2e-16 gets reported as `< 2.2e-16`).\n-   The other values in the table (in the `Sum Sq` and `Mean Sq`) columns are used to calculate the F statistic itself and we don't need to know these.\n-   The 6th line has some symbolic codes to represent how big (small) the p-value is; so, a p-value smaller than 0.001 would have a \\*\\*\\* symbol next to it (which ours does). Whereas if the p-value was between 0.01 and 0.05 then there would simply be a \\* character next to it, etc. Thankfully we can all cope with actual numbers and don't need a short-hand code to determine the reporting of our experiments (please tell me that's true...!)\n\n## Python\n\nThere are different ways of conducting an ANOVA in Python, with `scipy.stats` [proving an option](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f_oneway.html). However, I find that the [anova()](https://pingouin-stats.org/generated/pingouin.anova.html#pingouin.anova) function in `pingouin` provides the easiest and most-detailed option to do this, if you're using the data directly.\n\nIt takes the following arguments:\n\n-   `dv`: dependent variable (response variable; in our case `feeding`)\n-   `between`: between-subject factor (predictor variable; in our case `site`)\n-   `data`: which function doesn't!?\n-   `detailed`: optional `True` or `False`, we're setting it to `True` because we like to know what we're doing!\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.anova(dv = \"feeding\",\n         between = \"site\",\n         data = oystercatcher_py,\n         detailed = True)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Source           SS   DF          MS           F         p-unc       np2\n0    site  1878.015371    2  939.007685  150.782449  4.128088e-33  0.720473\n1  Within   728.625249  117    6.227566         NaN           NaN       NaN\n```\n:::\n:::\n\n\nThis creates a linear model based on the data, *i.e*. finds the means of the three groups and calculates a load of intermediary data that we need for the statistical analysis.\n\nIn the output:\n\n-   `Source`: Factor names - in our case these are the different sites (`site`)\n-   `SS`: Sums of squares (we'll get to that in a bit)\n-   `DF`: Degrees of freedom (at the moment only used for reporting)\n-   `MS`: Mean squares\n-   `F`: Our F-statistic\n-   `p-unc`: p-value (`unc` stands for \"uncorrected\" - more on multiple testing correction later)\n-   `np2`: Partial eta-square effect sizes (more on this later)\n\nAlternatively, and we'll be using this method later on in the course, you can perform an ANOVA on the `lm_oystercatcher_py` object we created earlier.\n\nThis uses the `sm.stats.anova_lm()` function from `statsmodels`. As you'll see, the output is very similar:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsm.stats.anova_lm(lm_oystercatcher_py)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             df       sum_sq     mean_sq           F        PR(>F)\nC(site)     2.0  1878.015371  939.007685  150.782449  4.128088e-33\nResidual  117.0   728.625249    6.227566         NaN           NaN\n```\n:::\n:::\n\n:::\n\nAgain, the p-value is what we're most interested in here and shows us the probability of getting samples such as ours if the null hypothesis were actually true.\n\nSince the p-value is very small (much smaller than the standard significance level of 0.05) we can say \"that it is very unlikely that these three samples came from the same parent distribution\" and as such we can reject our null hypothesis and state that:\n\n> A one-way ANOVA showed that the mean feeding rate of oystercatchers differed significantly between locations (p = 4.13e-33).\n\n## Post-hoc testing (Tukey's rank test)\n\nOne drawback with using an ANOVA test is that it only tests to see if all of the means are the same, and if we get a significant result using ANOVA then all we can say is that not all of the means are the same, rather than anything about how the pairs of groups differ. For example, consider the following box plot for three samples.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](cs2_practical_anova_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\nEach group is a random sample of 20 points from a normal distribution with variance 1. Groups 1 and 2 come from a parent population with mean 0 whereas group 3 come from a parent population with mean 2. The data clearly satisfy the assumptions of an ANOVA test.\n\n### Read in data and plot\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the data\ntukey <- read_csv(\"data/CS2-tukey.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 60 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): group\ndbl (1): response\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\n# have a look at the data\ntukey\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 × 2\n   response group  \n      <dbl> <chr>  \n 1    1.58  sample1\n 2    0.380 sample1\n 3   -0.997 sample1\n 4   -0.771 sample1\n 5    0.169 sample1\n 6   -0.698 sample1\n 7   -0.167 sample1\n 8    1.38  sample1\n 9   -0.839 sample1\n10   -1.05  sample1\n# … with 50 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the data\ntukey %>%\n    ggplot(aes(x = group, y = response)) +\n    geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](cs2_practical_anova_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntukey <- read.csv(\"data/CS2-tukey.csv\")\n\nboxplot(response ~ group, data = tukey)\n```\n\n::: {.cell-output-display}\n![](cs2_practical_anova_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n:::\n\n\n## Python\n\n::: {.cell}\n\n```{.python .cell-code}\n# load the data\ntukey_py = pd.read_csv(\"data/CS2-tukey.csv\")\n\n# have a look at the data\ntukey_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   response    group\n0  1.580048  sample1\n1  0.379544  sample1\n2 -0.996505  sample1\n3 -0.770799  sample1\n4  0.169046  sample1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# plot the data\n(ggplot(tukey_py,\n        aes(x = \"group\",\n            y = \"response\")) +\n     geom_boxplot())\n```\n\n::: {.cell-output-display}\n![](cs2_practical_anova_files/figure-html/unnamed-chunk-45-1.png){width=614}\n:::\n:::\n\n\n:::\n\n### Test for a significant difference in group means\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a linear model\nlm_tukey <- lm(response ~ group,\n               data = tukey)\n\n# perform an ANOVA\nanova(lm_tukey)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: response\n          Df Sum Sq Mean Sq F value    Pr(>F)    \ngroup      2 33.850 16.9250   20.16 2.392e-07 ***\nResiduals 57 47.854  0.8395                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_tukey <- lm(response ~ group, data = tukey)\n\nanova(lm_tukey)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: response\n          Df Sum Sq Mean Sq F value    Pr(>F)    \ngroup      2 33.850 16.9250   20.16 2.392e-07 ***\nResiduals 57 47.854  0.8395                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.anova(dv = \"response\",\n         between = \"group\",\n         data = tukey_py,\n         detailed = True)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Source         SS  DF         MS          F         p-unc       np2\n0   group  33.850044   2  16.925022  20.159922  2.391626e-07  0.414302\n1  Within  47.853670  57   0.839538        NaN           NaN       NaN\n```\n:::\n:::\n\n:::\n\nHere we have a p-value of 2.39 $\\times$ 10<sup>-7</sup> and so the test has very conclusively rejected the hypothesis that all means are equal.\n\nHowever, this was not due to all of the sample means being different, but rather just because one of the groups is very different from the others. In order to drill down and investigate this further we use a new test called **Tukey's range test** (or **Tukey's honest significant difference test** -- this always makes me think of some terrible cowboy/western dialogue).\n\nThis will compare all of the groups in a pairwise fashion and reports on whether a significant difference exists.\n\n### Performing Tukey's test\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\nTo perform Tukey's range test we can use the `tukey_hsd()` function from the `rstatix` package. Note, there is a `TukeyHSD()` function in base R as well, but the `tukey_hsd()` function can take a linear model object as input, whereas the `TukeyHSD()` function cannot. So the former is a bit easier to work with.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# perform Tukey's range test on linear model\nlm_tukey %>%\n    tukey_hsd()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 9\n  term  group1  group2  null.value estimate conf.low conf.high       p.adj\n* <chr> <chr>   <chr>        <dbl>    <dbl>    <dbl>     <dbl>       <dbl>\n1 group sample1 sample2          0    0.304   -0.393      1.00 0.55       \n2 group sample1 sample3          0    1.72     1.03       2.42 0.000000522\n3 group sample2 sample3          0    1.42     0.722      2.12 0.0000246  \n# … with 1 more variable: p.adj.signif <chr>\n```\n:::\n:::\n\n\nThe `tukey_hsd()` function takes our linear model (`lm_tukey`) as its input. The output is a pair-by-pair comparison between the different groups (samples 1 to 3). We are interested in the `p.adj` column, which gives us the adjusted p-value. The null hypothesis in each case is that there is no difference in the mean between the two groups.\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov_tukey <- aov(response ~ group, data = tukey)\n\nTukeyHSD(aov_tukey)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = response ~ group, data = tukey)\n\n$group\n                     diff        lwr      upr     p adj\nsample2-sample1 0.3037563 -0.3934982 1.001011 0.5498005\nsample3-sample1 1.7233591  1.0261047 2.420614 0.0000005\nsample3-sample2 1.4196028  0.7223484 2.116857 0.0000246\n```\n:::\n:::\n\n\nThe first argument repeats our ANOVA using a different function `aov()`. We store the output of this function in an R object called `aov_tukey`. Note that the `TukeyHSD()` function takes the output of the `aov()` function as its argument and not the raw data.\n\nThe bottom three lines contain the information that we want. The final column of each (entitled `p adj`) is the p-value that we're looking for. The null hypothesis in each case is that there is no difference in the mean between the two groups.\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.pairwise_tukey(dv = \"response\",\n                  between = \"group\",\n                  data = tukey_py).transpose()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                0         1         2\nA         sample1   sample1   sample2\nB         sample2   sample3   sample3\nmean(A) -0.049878 -0.049878  0.253878\nmean(B)  0.253878  1.673481  1.673481\ndiff    -0.303756 -1.723359 -1.419603\nse       0.289748  0.289748  0.289748\nT       -1.048347 -5.947789 -4.899442\np-tukey  0.549801  0.000001  0.000025\nhedges   -0.32493 -1.843488 -1.518558\n```\n:::\n:::\n\n\nThe `dv` argument is the response variable, whereas the `between` argument defines the explanatory variable.\n\nWe `.transpose()` the data, so we can look at the output a bit easier. Doing so, we focus on the `p-tukey` values.\n:::\n\nAs we can see that there isn't a significant difference between `sample1` and `sample2` but there is a significant difference between `sample1` and `sample3`, as well as `sample2` and `sample3`. This matches with what we expected based on the box plot.\n\n### Assumptions\n\nWhen to use Tukey's range test is a matter of debate (strangely enough a lot of statistical analysis techniques are currently matters of opinion rather than mathematical fact -- it does explain a little why this whole field appears so bloody confusing!)\n\n-   Some people claim that we should only perform Tukey's range test (or any other post-hoc tests) if the preceding ANOVA test showed that there was a significant difference between the groups and that if the ANOVA test had not shown any significant differences between groups then we would have to stop there.\n-   Other people say that this is rubbish and we can do whatever we like as long as we tell people what we did.\n\nThe background to this is rather involved but one of the reasons for this debate is to prevent so-called **data-dredging** or **p-hacking**. This is where scientists/analysts are so fixated on getting a \"significant\" result that they perform a huge variety of statistical techniques until they find one that shows that their data is significant (this was a particular problem in psychological studies for while -- not to point fingers though, they are working hard to sort their stuff out. Kudos!).\n\nWhether you should use post-hoc testing or not will depend on your experimental design and the questions that you're attempting to answer.\n\nTukey's range test, when we decide to use it, requires the same three assumptions as an ANOVA test:\n\n1.  Normality of distributions\n2.  Equality of variance between groups\n3.  Independence of observations\n\n## Exercise: Lobster weight\n\nJuvenile lobsters in aquaculture were grown on three different diets (fresh mussels, semi-dry pellets and dry flakes). After nine weeks, their wet weight was:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 × 3\n  Mussels Pellets Flakes\n    <dbl>   <dbl>  <dbl>\n1    152.    118.  102. \n2    132.    111.  103. \n3    104.    129.   90.4\n4    154.    110.  133. \n5    132     175.  129. \n6    119      NA   129. \n7    162.     NA    NA  \n```\n:::\n:::\n\n\nIs there any evidence that diet affects the growth rate of lobsters?\n\n1.  Write down the null and alternative hypotheses\n2.  Import the data from `data/CS2-lobsters.csv`\n3.  Summarise and visualise the data\n4.  Check the assumptions using appropriate tests and graphical analyses\n5.  Perform an ANOVA\n6.  Write down a sentence that summarises the results that you have found\n7.  Perform a post-hoc test and report the findings\n\n::: {.callout-tip collapse=\"true\"}\n## Answer\n\n### Hypotheses\n\n$H_0$ : all means are equal\n\n$H_1$ : not all means are equal\n\n### Import Data, summarise and visualise\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the data\nlobsters <- read_csv(\"data/CS2-lobsters.csv\")\n\n# look at the data\nlobsters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 18 × 3\n      id weight diet   \n   <dbl>  <dbl> <chr>  \n 1     1  152.  Mussels\n 2     2  132.  Mussels\n 3     3  104.  Mussels\n 4     4  154.  Mussels\n 5     5  132   Mussels\n 6     6  119   Mussels\n 7     7  162.  Mussels\n 8     8  118.  Pellets\n 9     9  111.  Pellets\n10    10  129.  Pellets\n11    11  110.  Pellets\n12    12  175.  Pellets\n13    13  102.  Flakes \n14    14  103.  Flakes \n15    15   90.4 Flakes \n16    16  133.  Flakes \n17    17  129.  Flakes \n18    18  129.  Flakes \n```\n:::\n:::\n\n\nThe data have a unique `id` column, which we don't need any summary statistics for, so we deselect it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create some summary statistics\nlobsters %>% \n  select(-id) %>% \n  group_by(diet) %>% \n  get_summary_stats(type = \"common\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 11\n  diet    variable     n   min   max median   iqr  mean    sd    se    ci\n  <chr>   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Flakes  weight       6  90.4  133.   116.  27.3  114.  18.2  7.42  19.1\n2 Mussels weight       7 104.   162.   132.  27.0  136.  20.6  7.79  19.1\n3 Pellets weight       5 110.   175.   118.  17.8  128.  27.2 12.1   33.7\n```\n:::\n:::\n\n\nNext, we visualise the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlobsters %>% \n  ggplot(aes(x = diet, y = weight)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](cs2_practical_anova_files/figure-html/unnamed-chunk-55-1.png){width=672}\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlobsters <- read.csv(\"data/CS2-lobsters.csv\")\n```\n:::\n\n\nLet's look at the data and see what we can see.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n   id weight    diet\n1   1  151.6 Mussels\n2   2  132.1 Mussels\n3   3  104.2 Mussels\n4   4  153.5 Mussels\n5   5  132.0 Mussels\n6   6  119.0 Mussels\n7   7  161.9 Mussels\n8   8  117.7 Pellets\n9   9  110.8 Pellets\n10 10  128.6 Pellets\n11 11  110.1 Pellets\n12 12  175.2 Pellets\n13 13  101.8  Flakes\n14 14  102.9  Flakes\n15 15   90.4  Flakes\n16 16  132.8  Flakes\n17 17  129.3  Flakes\n18 18  129.4  Flakes\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naggregate(weight ~ diet,\n          FUN = summary,\n          data = lobsters)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     diet weight.Min. weight.1st Qu. weight.Median weight.Mean weight.3rd Qu.\n1  Flakes     90.4000       102.0750      116.1000    114.4333       129.3750\n2 Mussels    104.2000       125.5000      132.1000    136.3286       152.5500\n3 Pellets    110.1000       110.8000      117.7000    128.4800       128.6000\n  weight.Max.\n1    132.8000\n2    161.9000\n3    175.2000\n```\n:::\n\n```{.r .cell-code}\nboxplot(weight ~ diet, data = lobsters)\n```\n\n::: {.cell-output-display}\n![](cs2_practical_anova_files/figure-html/unnamed-chunk-58-1.png){width=672}\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlobsters_py = pd.read_csv(\"data/CS2-lobsters.csv\")\n```\n:::\n\n\nNext, we visualise the data:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(lobsters_py,\n        aes(x = \"diet\",\n            y = \"weight\")) +\n     geom_boxplot())\n```\n\n::: {.cell-output-display}\n![](cs2_practical_anova_files/figure-html/unnamed-chunk-60-1.png){width=614}\n:::\n:::\n\n\n:::\n\nAs always we use the plot and summary to assess three things:\n\n1.  Did we load the data in properly?\n\n-   We see three groups with reasonable values. There aren't any data points that are obviously wrong (negative, zero or massively big) and we have the right number of groups. So it looks as if we didn't do anything obviously wrong.\n\n2.  What do we expect as a result of a statistical test?\n\n-   Whilst the `Mussels` group does look higher than the other two groups, `Pellets` and `Flakes` appear almost identical in terms of average values, and there's quite a bit of overlap with the `Mussels` group. A non-significant result is the most likely answer, and I would be surprised to see a significant p-value - especially given the small sample size that we have here.\n\n3.  What do we think about assumptions?\n\n-   The groups appear mainly symmetric (although `Pellets` is a bit weird) and so we're not immediately massively worried about lack of normality. Again, `Flakes` and `Mussels` appear to have very similar variances but it's a bit hard to decide what's going on with `Pellets.` It's hard to say what's going on with the assumptions and so I'll wait and see what the other tests say.\n\n### Explore Assumptions\n\nWe'll explore the assumption of normality and equality of variance, assuming that the data are independent.\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n**Normality**\n\nWe'll be really thorough here and consider the normality of each group separately and jointly using the Shapiro-Wilk test, as well as looking at the Q-Q plot. In reality, and after these examples , we'll only use the Q-Q plot.\n\nFirst, we perform the Shapiro-Wilk test on the individual groups:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Shapiro-Wilk on lobster groups\nlobsters %>% \n    filter(diet == \"Flakes\") %>% \n    pull(weight) %>% \n    shapiro.test()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  .\nW = 0.84368, p-value = 0.1398\n```\n:::\n\n```{.r .cell-code}\nlobsters %>% \n    filter(diet == \"Mussels\") %>% \n    pull(weight) %>% \n    shapiro.test()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  .\nW = 0.94784, p-value = 0.71\n```\n:::\n\n```{.r .cell-code}\nlobsters %>% \n    filter(diet == \"Pellets\") %>% \n    pull(weight) %>% \n    shapiro.test()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  .\nW = 0.76706, p-value = 0.0425\n```\n:::\n:::\n\n\n`Flakes` and `Mussels` are fine, but, as we suspected from earlier, `Pellets` appears to have a marginally significant Normality test result.\n\nLet's look at the Shapiro-Wilk test for all of the data together:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a linear model\nlm_lobsters <- lm(weight ~ diet,\n                  data = lobsters)\n\n# extract the residuals\nresid_lobsters <- residuals(lm_lobsters)\n\n# and perform the Shapiro-Wilk test on the residuals\nresid_lobsters %>% \n  shapiro.test()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  .\nW = 0.94779, p-value = 0.3914\n```\n:::\n:::\n\n\nThis on the other hand says that everything is fine. Let's look at Q-Q plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Q-Q plots\nlm_lobsters %>% \n  resid_panel(plots = \"qq\")\n```\n\n::: {.cell-output-display}\n![](cs2_practical_anova_files/figure-html/unnamed-chunk-63-1.png){width=672}\n:::\n:::\n\n\nHere, I've used an extra argument to the normal diagnostic plots call. The default option is to plot 4 diagnostic plots. You can tell `resid_panel()` to only plot a specific one, using the `plots =` arguments. If you want to know more about this have a look at the [help documentation](https://goodekat.github.io/ggResidpanel-tutorial/tutorial.html#overview) or by using `?resid_panel`.\n\nThe Q-Q plot looks OK, not perfect, but more than good enough for us to have confidence in the normality of the data.\n\nOverall, I'd be happy that the assumption of normality has been adequately well met here. The suggested lack of normality in the `Pellets` was only just significant and we have to take into account that there are only 5 data points in that group. If there had been a lot more points in that group, or if the Q-Q plot was considerably worse then I wouldn't be confident.\n\n**Equality of Variance**\n\nWe'll consider the Bartlett test and we'll look at some diagnostic plots too.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# perform Bartlett's test\nbartlett.test(weight ~ diet,\n              data = lobsters)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBartlett test of homogeneity of variances\n\ndata:  weight by diet\nBartlett's K-squared = 0.71273, df = 2, p-value = 0.7002\n```\n:::\n\n```{.r .cell-code}\n# plot the residuals and scale-location plots\nlm_lobsters %>% \n  resid_panel(plots = c(\"resid\", \"ls\"),\n              smoother = TRUE)\n```\n\n::: {.cell-output-display}\n![](cs2_practical_anova_files/figure-html/unnamed-chunk-64-1.png){width=672}\n:::\n:::\n\n\nIn the above code I've specified which diagnostic plots I wanted. I have also added a loess smoother line (`smoother = TRUE`) to the plots\n\n1.  The Residuals Plot. What we're looking for there is that the points are evenly spread on either side of the line. Looks good.\n2.  The Location-Scale Plot (this is displayed by default in base R's diagnostic plots). Here we're looking at the red line. If that line is more or less horizontal, then the equality of variance assumption is met.\n\nHere all three methods agree that there aren't any issues with equality of variance:\n\n-   the Bartlett test p-value is large and non-significant\n-   the spread of points in all three groups in the residuals vs fitted graph are roughly the same\n-   the red line in the scale-location graph is pretty horizontal\n\nOverall, this assumption is pretty well met.\n\n## R\n\n**Normality**\n\nWe'll be really thorough here and consider the normality of each group separately and jointly using the Shapiro-Wilk test, as well as looking at the Q-Q plot. In reality, and after these examples , we'll only use the Q-Q plot.\n\nWe'll need to unstack the data to use the Shapiro-Wilk test on the individual groups:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlobst_uns <- unstack(lobsters, weight ~ diet)\nshapiro.test(lobst_uns$Flakes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  lobst_uns$Flakes\nW = 0.84368, p-value = 0.1398\n```\n:::\n\n```{.r .cell-code}\nshapiro.test(lobst_uns$Mussels)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  lobst_uns$Mussels\nW = 0.94784, p-value = 0.71\n```\n:::\n\n```{.r .cell-code}\nshapiro.test(lobst_uns$Pellets)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  lobst_uns$Pellets\nW = 0.76706, p-value = 0.0425\n```\n:::\n:::\n\n\n`Flakes` and `Mussels` are fine, but, as we suspected from earlier, `Pellets` appears to have a marginally significant Normality test result.\n\nLet's look at the Shapiro-Wilk test for all of the data together:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_lobsters_r <- lm(weight ~ diet,\n                    data = lobsters)\n\nresid_lobst <- residuals(lm_lobsters_r)\nshapiro.test(resid_lobst)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  resid_lobst\nW = 0.94779, p-value = 0.3914\n```\n:::\n:::\n\n\nThis on the other hand says that everything is fine. Let's look at the Q-Q-plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(lm_lobsters_r,\n     which = 2)\n```\n\n::: {.cell-output-display}\n![](cs2_practical_anova_files/figure-html/unnamed-chunk-67-1.png){width=672}\n:::\n:::\n\n\nHere, I've used an extra argument to the normal diagnostic plots call. The default option is to plot 4 diagnostic plots, but you can tell R to only plot a specific one. (If you want to know more about this have a look at the `plot.lm` help documentation using `?plot.lm`). I've asked R to only plot the Q-Q plot with the `which = 2` argument.\n\nThe Q-Q plot looks OK, not perfect, but more than good enough for us to have confidence in the normality of the data.\n\nOverall, I'd be happy that the assumption of normality has been adequately well met here. The suggested lack of normality in the `Pellets` was only just significant and we have to take into account that there are only 5 data points in that group. If there had been a lot more points in that group, or if the Q-Q plot was considerably worse then I wouldn't be confident.\n\n**Equality of Variance**\n\nWe'll consider the Bartlett test and we'll look at some diagnostic plots too.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbartlett.test(weight ~ diet,\n              data = lobsters)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBartlett test of homogeneity of variances\n\ndata:  weight by diet\nBartlett's K-squared = 0.71273, df = 2, p-value = 0.7002\n```\n:::\n\n```{.r .cell-code}\nplot(lm_lobsters_r,\n     which = c(1,3))\n```\n\n::: {.cell-output-display}\n![](cs2_practical_anova_files/figure-html/unnamed-chunk-68-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](cs2_practical_anova_files/figure-html/unnamed-chunk-68-2.png){width=672}\n:::\n:::\n\n\nIn the above code, I've used the same trick as before with the `which` argument to only plot the two diagnostic plots that relate to equality of variance (residuals vs fitted and scale-location).\n\nHere all three methods agree that there aren't any issues with equality of variance:\n\n-   the Bartlett test p-value is large and non-significant\n-   the spread of points in all three groups in the residuals vs fitted graph are roughly the same\n-   the red line in the scale-location graph is pretty horizontal\n\nOverall, this assumption is pretty well met.\n\n## Python\n\n**Normality**\n\nWe'll be really thorough here and consider the normality of each group separately and jointly using the Shapiro-Wilk test, as well as looking at the Q-Q plot. In reality, and after these examples , we'll only use the Q-Q plot.\n\nFirst, we perform the Shapiro-Wilk test on the individual groups:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Shapiro-Wilk on lobster groups\npg.normality(dv = \"weight\",\n             group = \"diet\",\n             data = lobsters_py)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                W      pval  normal\nMussels  0.947836  0.709968    True\nPellets  0.767059  0.042495   False\nFlakes   0.843678  0.139796    True\n```\n:::\n:::\n\n\n`Flakes` and `Mussels` are fine, but, as we suspected from earlier, `Pellets` appears to have a marginally significant Normality test result.\n\nLet's look at the Shapiro-Wilk test for all of the data together:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula= \"weight ~ C(diet)\", data = lobsters_py)\n# and get the fitted parameters of the model\nlm_lobsters_py = model.fit()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# get the residuals from the model fit\n# and perform Shapiro-Wilk test\npg.normality(lm_lobsters_py.resid)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          W      pval  normal\n0  0.947787  0.391367    True\n```\n:::\n:::\n\n\nThis on the other hand says that everything is fine. Let's look at Q-Q plot.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Q-Q plots\n(ggplot(lobsters_py,\n        aes(sample = \"weight\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"red\"))\n```\n\n::: {.cell-output-display}\n![](cs2_practical_anova_files/figure-html/unnamed-chunk-72-1.png){width=614}\n:::\n:::\n\n\nThe Q-Q plot looks OK, not perfect, but more than good enough for us to have confidence in the normality of the data.\n\nOverall, I'd be happy that the assumption of normality has been adequately well met here. The suggested lack of normality in the `Pellets` was only just significant and we have to take into account that there are only 5 data points in that group. If there had been a lot more points in that group, or if the Q-Q plot was considerably worse then I wouldn't be confident.\n\n**Equality of Variance**\n\nWe'll consider the Bartlett test and we'll look at the diagnostic plots too.\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.homoscedasticity(dv = \"weight\",\n                    group = \"diet\",\n                    method = \"bartlett\",\n                    data = lobsters_py)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 T      pval  equal_var\nbartlett  0.712733  0.700216       True\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndgplots(lm_lobsters_py)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/dgplots/2022_11_07-01:04:00_pm_dgplots.png){width=786}\n:::\n:::\n\n\nWe'll just focus on the following:\n\n1.  The Residuals Plot. What we're looking for there is that the points are evenly spread on either side of the line. Looks good.\n2.  The Location-Scale Plot (this is displayed by default in base R's diagnostic plots). Here we're looking at the red line. If that line is more or less horizontal, then the equality of variance assumption is met.\n\nHere all three methods agree that there aren't any issues with equality of variance:\n\n-   the Bartlett test p-value is large and non-significant\n-   the spread of points in all three groups in the residuals vs fitted graph are roughly the same\n-   the red line in the scale-location graph is pretty horizontal\n\nOverall, this assumption is pretty well met.\n:::\n\n### Carry out one-way ANOVA\n\nWith our assumptions of normality and equality of variance met we can be confident that a one-way ANOVA is an appropriate test.\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lm_lobsters)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: weight\n          Df Sum Sq Mean Sq F value Pr(>F)\ndiet       2 1567.2  783.61  1.6432 0.2263\nResiduals 15 7153.1  476.87               \n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lm_lobsters_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: weight\n          Df Sum Sq Mean Sq F value Pr(>F)\ndiet       2 1567.2  783.61  1.6432 0.2263\nResiduals 15 7153.1  476.87               \n```\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\npg.anova(dv = \"weight\",\n         between = \"diet\",\n         data = lobsters_py,\n         detailed = True)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Source           SS  DF          MS        F     p-unc       np2\n0    diet  1567.229381   2  783.614690  1.64324  0.226313  0.179722\n1  Within  7153.075619  15  476.871708      NaN       NaN       NaN\n```\n:::\n:::\n\n:::\n\n> A one-way ANOVA test indicated that the mean weight of juvenile lobsters did not differ significantly between diets (p = 0.23).\n\n### Post-hoc testing with Tukey\n\nIn this case we did not find any significant differences between the different diets. So that is a good time for me to reiterate that carrying out the post-hoc test after getting a non-significant result with ANOVA is something that you have to think very carefully about and it all depends on what your research question it.\n\nIf your research question was:\n\n> Does diet affect lobster weight?\n\nor\n\n> Is there any effect of diet on lobster weight?\\_\n\nThen when we got the non-significant result from the ANOVA test we should have just stopped there as we have our answer. Going digging for \"significant\" results by running more tests is a main factor that contributes towards lack of reproducibility in research.\n\nIf on the other hand your research question was:\n\n> Are any specific diets better or worse for lobster weight than others?\n\nThen we should probably have just skipped the one-way ANOVA test entirely and just jumped straight in with the Tukey's range test. The important point here is that the result of the one-way ANOVA test doesn't stop you from carrying out the Tukey test - but it's up to you to decide whether it is sensible.\n:::\n\n## Key points\n\n::: callout-note\n-   We use an ANOVA to test if there is a difference in means between multiple continuous response variables\n-   We check assumptions with diagnostic plots and check if the residuals are normally distributed\n-   We use post-hoc testing to check for significant differences between the group means, for example using Tukey's range test\n:::\n",
    "supporting": [
      "cs2_practical_anova_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}