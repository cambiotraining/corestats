{
  "hash": "d9285abac44b166f1b2fd840e6deb988",
  "result": {
    "markdown": "---\ntitle: \"Linear regression\"\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n::: callout-tip\n## Learning outcomes\n\n**Questions**\n\n-   When should I use a linear regression?\n-   How do I interpret the results?\n\n**Objectives**\n\n-   Be able to perform a linear regression in R or Python\n-   Use ANOVA to check if the slope of the regression differs from zero\n-   Understand the underlying assumptions for linear regression analysis\n-   Use diagnostic plots to check these assumptions\n:::\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n### Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n```\n:::\n\n\n### Functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creates diagnostic plots\nggResidpanel::resid_panel()\n```\n:::\n\n\n## R\n\n## Python\n:::\n:::\n\n## Purpose and aim\n\nRegression analysis not only tests for an association between two or more variables, but also allows one to investigate quantitatively the nature of any relationship which is present, and thus determine whether one variable may be used to predict values of another. Simple linear regression essentially models the dependence of a scalar dependent variable (y) on an independent (or explanatory) variable (x) according to the relationship:\n\n\n```{=tex}\n\\begin{equation*} \ny = \\beta_0 + \\beta_1 x\n\\end{equation*}\n```\n\nwhere $\\beta_0$ is the value of the intercept and $\\beta_1$ is the slope of the fitted line. The aim of simple linear regression analysis to assess whether the coefficient of the slope, $\\beta_1$, is actually different from zero. If it is different from zero then we can say that $x$ has a significant effect on $y$ (since changing $x$ leads to a predicted change in $y$), whereas if it isn't significantly different from zero, then we say that there isn't sufficient evidence of a relationship. Of course, in order to assess whether the slope is significantly different from zero we first need to calculate the values of $\\beta_0$ and $\\beta_1$.\n\n## Data and hypotheses\n\nWe will perform a simple linear regression analysis on the two variables `murder` and `assault` from the `USArrests` data set. We wish to determine whether the `assault` variable is a significant predictor of the `murder` variable. This means that we will need to find the coefficients $\\beta_0$ and $\\beta_1$ that best fit the following macabre equation:\n\n\n```{=tex}\n\\begin{equation*}\nMurder  = \\beta_0 + \\beta_1 \\times Assault\n\\end{equation*}\n```\n\nAnd then will be testing the following null and alternative hypotheses:\n\n-   $H_0$: `assault` is not a significant predictor of `murder`, $\\beta_1 = 0$\n-   $H_1$: `assault` is a significant predictor of `murder`, $\\beta_1 \\neq 0$\n\n## Summarise and visualise\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\nFirst, we read in the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSArrests <- read_csv(\"data/CS3-usarrests.csv\")\n```\n:::\n\n\nYou can visualise the data with:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create scatterplot of the data\nUSArrests %>% \n  ggplot(aes(x = assault, y = murder)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](cs3_practical_linear-regression_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## R\n\nFirst, we read in the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSArrests_r <- read.csv(\"data/CS3-usarrests.csv\")\n```\n:::\n\n\nYou can visualise the data with:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(murder ~ assault, data = USArrests_r)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_linear-regression_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## Python\n:::\n\nPerhaps unsurprisingly, there appears to be a relatively strong positive relationship between these two variables. Whilst there is a reasonable scatter of the points around any trend line, we would probably expect a significant result in this case.\n\n## Assumptions\n\nIn order for a linear regression analysis to be valid 4 key assumptions need to be met:\n\n::: callout-important\n1.  The data must be linear (it is entirely possible to calculate a straight line through data that is not straight - it doesn't mean that you should!)\n2.  The residuals must be normally distributed\n3.  The residuals must not be correlated with their fitted values\n4.  The fit should not depend overly much on a single point (no point should have high leverage).\n:::\n\nWhether these assumptions are met can easily be checked visually by producing four key diagnostic plots.\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\nFirst we need to define the linear model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_1 <- lm(murder ~ assault,\n           data = USArrests)\n```\n:::\n\n\n-   The first argument to `lm` is a formula saying that `murder` depends on `assault`. As we have seen before, the syntax is generally `dependent variable` \\~ `independent variable`.\n-   The second argument specifies which dataset to use\n\nNext, we can create diagnostic plots for the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_1 %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_linear-regression_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n-   The top left graph plots the **Residuals plot**. If the data are best explained by a straight line then there should be a uniform distribution of points above and below the horizontal blue line (and if there are sufficient points then the red line, which is a smoother line, should be on top of the blue line). This plot is pretty good.\n-   The top right graph shows the **Q-Q plot** which allows a visual inspection of normality. If the residuals are normally distributed, then the points should lie on the diagonal dotted line. This isn't too bad but there is some slight snaking towards the upper end and there appears to be an outlier.\n-   The bottom left **Location-scale** graph allows us to investigate whether there is any correlation between the residuals and the predicted values and whether the variance of the residuals changes significantly. If not, then the red line should be horizontal. If there is any correlation or change in variance then the red line will not be horizontal. This plot is fine.\n-   The last graph shows the **Cook's distance** and tests if any one point has an unnecessarily large effect on the fit. The important aspect here is to see if any points are larger than 0.5 (meaning you'd have to be careful) or 1.0 (meaning you'd definitely have to check if that point has an large effect on the model). If not, then no point has undue influence. This plot is good.\n\n## R\n\nFirst we need to define the linear model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_1 <- lm(murder ~ assault, data = USArrests_r)\n```\n:::\n\n\n-   The first argument to `lm` is a formula saying that `Murder` depends on `Assaults`. As we have seen before, the syntax is generally `dependent variable` \\~ `independent variable`.\n-   The second argument specifies which dataset to use\n\nNext, we can create the diagnostic plots for the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a 2 x 2 output window\npar(mfrow = c(2,2))\n\n# and create the diagnostic plots for our model\nplot(lm_1)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_linear-regression_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n-   The top left graph plots the **residuals against the fitted values**. If the data are best explained by a straight line then there should be a uniform distribution of points above and below the horizontal grey dotted line (and if there are sufficient points then the red line, which is a moving average, should be on top of the grey dotted line). This plot is pretty good.\n-   The top right graph shows the **Q-Q plot** which allows a visual inspection of normality. If the residuals are normally distributed, then the points should lie on the diagonal dotted line. This isn't too bad but there is some slight snaking towards the upper end and Georgia appears to be an outlier here.\n-   The bottom left **scale-location** graph allows us to investigate whether there is any correlation between the residuals and the fitted values and whether the variance of the residuals changes significantly. If not, then the red line should be horizontal. If there is any correlation or change in variance then the red line will not be horizontal. This plot is fine.\n-   The last graph shows the **Cook's distance** and tests if any one point has an unnecessarily large effect on the fit. The important aspect here is to see if any points lie beyond the red dashed contour line in the top right corner of this plot. If not, then no point has undue influence. This plot is good.\n\n## Python\n:::\n\n::: callout-note\nFormally, if there is any concern after looking at the diagnostic plots, then a linear regression is not valid. However, disappointingly, very few people ever check whether the linear regression assumptions have been met before quoting the results.\n\nLet's change this through leading by example!\n:::\n\n## Implement and interpret test\n\nWe have already defined the linear model, so we can have a closer look at it:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# show the linear model\nlm_1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = murder ~ assault, data = USArrests_r)\n\nCoefficients:\n(Intercept)      assault  \n    0.63168      0.04191  \n```\n:::\n:::\n\n\nThe `lm()` function returns a linear model object which is essentially a list containing everything necessary to understand and analyse a linear model. However, if we just type the model name (as we have above) then it just prints to the screen the actual coefficients of the model i.e. the intercept and the slope of the line.\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# show the linear model\nlm_1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = murder ~ assault, data = USArrests_r)\n\nCoefficients:\n(Intercept)      assault  \n    0.63168      0.04191  \n```\n:::\n:::\n\n\nThe `lm()` function returns a linear model object which is essentially a list containing everything necessary to understand and analyse a linear model. However, if we just type the model name (as we have above) then it just prints to the screen the actual coefficients of the model i.e. the intercept and the slope of the line.\n\n## Python\n:::\n\nSo here we have found that the line of best fit is given by:\n\n\n```{=tex}\n\\begin{equation*}\nMurder = 0.63 + 0.042 Assault\n\\end{equation*}\n```\n\nNext we can assess whether the slope is significantly different from zero:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lm_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: murder\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nassault    1 597.70  597.70  86.454 2.596e-12 ***\nResiduals 48 331.85    6.91                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nHere, we again use the `anova()` command to assess significance. This shouldn't be too surprising at this stage if the introductory lectures made any sense. From a mathematical perspective, one-way ANOVA and simple linear regression are exactly the same as each other and it makes sense that we should use the same command to analyse them in R.\n\nThis is exactly the same format as the table we saw for one-way ANOVA:\n\n-   The 1st line just tells you the that this is an ANOVA test\n-   The 2nd line tells you what the response variable is (in this case `Murder`)\n-   The 3rd, 4th and 5th lines are an ANOVA table which contain some useful values:\n    -   The `Df` column contains the degrees of freedom values on each row, 1 and 48 (which we'll need for the reporting)\n    -   The `F` value column contains the F statistic, 86.454 (which again we'll need for reporting).\n    -   The p-value is 2.596e-12 and is the number directly under the `Pr(>F)` on the 4th line.\n    -   The other values in the table (in the `Sum Sq` and `Mean Sq`) column are used to calculate the F statistic itself and we don't need to know these.\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lm_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: murder\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nassault    1 597.70  597.70  86.454 2.596e-12 ***\nResiduals 48 331.85    6.91                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nHere, we again use the `anova()` command to assess significance. This shouldn't be too surprising at this stage if the introductory lectures made any sense. From a mathematical perspective, one-way ANOVA and simple linear regression are exactly the same as each other and it makes sense that we should use the same command to analyse them in R.\n\nThis is exactly the same format as the table we saw for one-way ANOVA:\n\n-   The 1st line just tells you the that this is an ANOVA test\n-   The 2nd line tells you what the response variable is (in this case `Murder`)\n-   The 3rd, 4th and 5th lines are an ANOVA table which contain some useful values:\n    -   The `Df` column contains the degrees of freedom values on each row, 1 and 48 (which we'll need for the reporting)\n    -   The `F` value column contains the F statistic, 86.454 (which again we'll need for reporting).\n    -   The p-value is 2.596e-12 and is the number directly under the `Pr(>F)` on the 4th line.\n    -   The other values in the table (in the `Sum Sq` and `Mean Sq`) column are used to calculate the F statistic itself and we don't need to know these.\n\n## Python\n:::\n\nAgain, the p-value is what we're most interested in here and shows us the probability of getting data such as ours if the null hypothesis were actually true and the slope of the line were actually zero. Since the p-value is excruciatingly tiny we can reject our null hypothesis and state that:\n\n> A simple linear regression showed that the assault rate in US states was a significant predictor of the number of murders (F = 86.45, df = 1,48, p = 2.59x10<sup>-12</sup>).\n\n### Plotting the regression line\n\nIt can be very helpful to plot the regression line with the original data to see how far the data are from the predicted linear values. We can do this as follows:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the data\nUSArrests %>% \n  ggplot(aes(x = assault, y = murder)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_linear-regression_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n-   We plot all the data using `geom_point()`\n-   Next, we add the linear model using `geom_smooth(method = \"lm\")`, hiding the confidence intervals (`se = FALSE`)\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the data\nplot(murder ~ assault, data = USArrests_r)\n\n# add the regression line\nabline(lm_1, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](cs3_practical_linear-regression_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n-   The first command creates a scatter plot of the data\n-   The second command uses the results of the linear model fitting (the object `lm_1`) to add the line of best fit to the plot (and colour it blue).\n\n## Python\n:::\n\n## Exercise: State data\n\nCalculate two simple linear regressions using the `data/CS3-statedata.csv` data set, first for the variable `life_exp` on the variable `murder` and then for the variable `hs_grad` on `frost`.\n\nDo the following in both cases:\n\n1.  Find the value of the slope and intercept coefficients for both regressions\n2.  Determine if the slope is significantly different from zero (i.e. is there a relationship between the two variables)\n3.  Produce a scatter plot of the data with the line of best fit superimposed on top.\n4.  Produce diagnostic plots and discuss with your (virtual) neighbour if you should have carried out a simple linear regression in each case\n\n::: {.callout-caution collapse=\"true\"}\n## Answer\n\n### Load and visualise the data\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\nFirst, we read in the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSAstate <- read_csv(\"data/CS3-statedata.csv\")\n```\n:::\n\n\nNext, we visualise the `murder` variable against the `life_exp` variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the data and add the regression line\nUSAstate %>% \n  ggplot(aes(x = murder, y = life_exp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_linear-regression_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n## R\n\nFirst, we read in the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSAstate_r <- read.csv(\"data/CS3-statedata.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the data\nplot(life_exp ~ murder, data = USAstate_r)\n\n# create a linear model\nlm1 <- lm(life_exp ~ murder, data = USAstate_r)\n\n# and add a regression line\nabline(lm1, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](cs3_practical_linear-regression_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n## Python\n:::\n\nWe visualise for the same reasons as before:\n\n1.  We check that the data aren't obviously wrong. Here we have sensible values for life expectancy (nothing massively large or small), and plausible values for murder rates (not that I'm that *au fait* with US murder rates in 1973 but small positive numbers seem plausible).\n2.  We check to see what we would expect from the statistical analysis. Here there does appear to be a reasonable downward trend to the data. I would be surprised if we didn't get a significant result given the amount of data and the spread of the data about the line\n3.  We check the assumptions (only roughly though as we'll be doing this properly in a minute). Nothing immediately gives me cause for concern; the data appear linear, the spread of the data around the line appears homogeneous and symmetrical. No outliers either.\n\n### Check assumptions\n\nNow, let's check the assumptions with the diagnostic plots.\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a linear model\nlm_murder <- lm(life_exp ~ murder,\n           data = USAstate)\n\n# create the diagnostic plots\nlm_murder %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_linear-regression_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nThe **Residuals** plot appears symmetric enough (similar distribution of points above and below the horizontal blue line) for me to happy with linearity. Similarly the red line in the **Location-Scale** plot looks horizontal enough for me to be happy with homogeneity of variance. There aren't any influential points in the **Cook's distance** plot. The only plot that does give me a bit of concern is the **Q-Q** plot. Here we see clear evidence of snaking, although the degree of snaking isn't actually that bad. This just means that we can be pretty certain that the distribution of residuals isn't normal, but also that it isn't *very* non-normal.\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2,2))\nplot(lm1)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_linear-regression_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\nThe **Residuals vs Fitted** plot appears symmetric enough (similar distribution of points above and below the horizontal grey dotted line) for me to happy with linearity. Similarly the red line in the **Scale-Location** plot looks horizontal enough for me to be happy with homogeneity of variance. There aren't any influential points in the residuals vs leverage. The only plot that does give me a bit of concern is the **Normal Q-Q** graph. Here we see clear evidence of snaking, although the degree of snaking isn't actually that bad. This just means that we can be pretty certain that the distribution of residuals isn't normal, but also that it isn't *very* non-normal.\n\n## Python\n:::\n\nWhat do we do in this situation? Well, there are three possible options:\n\n1.  Appeal to the **Central Limit Theorem**. This states that if we have a large enough sample size we don't have to worry about whether the distribution of the residuals are normally distributed. **Large enough** is a bit of a moving target here and to be honest it depends on how non-normal the underlying data are. If the data are only a little bit non-normal then we can get away with using a smaller sample than if the data are massively skewed (for example). This is not an exact science, but anything over 30 data points is considered a lot for mild to moderate non-normality (as we have in this case). If the data were very skewed then we would be looking for more data points (50-100). So, for this example we can legitimately just carry on with our analysis without worrying.\n2.  Try transforming the data. Here we would try applying some mathematical functions to the response variable (`life_exp`) in the hope that repeating the analysis with this transformed variable would make things better. To be honest with you it might not work and we won't know until we try. Dealing with transformed variables is legitimate as an approach but it can make interpreting the model a bit more challenging. In this particular example none of the traditional transformations (log, square-root, reciprocal) do anything to fix the slight lack of normality (you can take my word for it or you could try it using; `lm(log(life_exp ~ murder, data = USAstate))` for example.\n3.  Go with permutation methods / bootstrapping. This approach would definitely work. I don't have time to explain it here (it's the subject of an entire other practical). This approach also requires us to have a reasonably large sample size to work well as we have to assume that the distribution of the sample is a good approximation for the distribution of the entire data set.\n\nSo in this case, because we have a large enough sample size and our deviation from normality isn't too bad, we can just crack on with the standard analysis.\n\n### Implement and interpret test\n\nSo, let's actually do the analysis:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lm_murder)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: life_exp\n          Df Sum Sq Mean Sq F value   Pr(>F)    \nmurder     1 53.838  53.838  74.989 2.26e-11 ***\nResiduals 48 34.461   0.718                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lm_murder)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: life_exp\n          Df Sum Sq Mean Sq F value   Pr(>F)    \nmurder     1 53.838  53.838  74.989 2.26e-11 ***\nResiduals 48 34.461   0.718                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n## Python\n:::\n\nAnd after all of that we find that the murder rate is a statistically significant predictor of life expectancy in US states. Woohoo!\n\n## High School Graduation and Frosty Days\n\nNow let's investigate the relationship between the proportion of High School Graduates a state has (`hs_grad`) and the mean number of days below freezing (`frost`) within each state.\n\nWe'll run through this a bit quicker:\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the data\nUSAstate %>% \n  ggplot(aes(x = frost, y = hs_grad)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_linear-regression_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the data\nplot(hs_grad ~ frost, data = USAstate_r)\n\n# create a linear model\nlm2<-lm(hs_grad ~ frost, data = USAstate_r)\n\n# and add a regression line\nabline(lm2, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](cs3_practical_linear-regression_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n## Python\n:::\n\nOnce again, we look at the data.\n\n1.  There doesn't appear to be any ridiculous errors with the data; High School graduation proportions are in the 0-100% range and the mean number of sub-zero days for each state are between 0 and 365, so these numbers are plausible.\n2.  Whilst there is a trend upwards, which wouldn't surprise me if it came back as being significant, I'm a bit concerned about...\n3.  The assumptions. I'm mainly concerned that the data aren't very linear. There appears to be a noticeable pattern to the data with some sort of minimum around 50-60 Frost days. This means that it's hard to assess the other assumptions.\n\nLet's check these out properly\n\nNow, let's check the assumptions with the diagnostic plots.\n\n::: {.panel-tabset group=\"language\"}\n## tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a linear model\nlm_frost <- lm(hs_grad ~ frost,\n               data = USAstate)\n\n# create the diagnostic plots\nlm_frost %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_linear-regression_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2,2))\nplot(lm2)\n```\n\n::: {.cell-output-display}\n![](cs3_practical_linear-regression_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\n## Python\n:::\n\nWe can see that what we suspected from before is backed up by the residuals plot. The data aren't linear and there appears to be some sort of odd down-up pattern here. Given the lack of linearity it just isn't worth worrying about the other plots because our model is **misspecified**: a straight line just doesn't represent our data at all.\n\nJust for reference, and as practice for looking at diagnostic plots, if we ignore the lack of linearity then we can say that\n\n-   Normality is pretty good from the Q-Q plot\n-   Homogeneity of variance isn't very good and there appears to be a noticeable drop in variance as we go from left to right (from consideration of the Location-Scale plot)\n-   There don't appear to be any influential points (by looking at the Cook's distance plot)\n\nHowever, none of that is relevant in this particular case since the data aren't linear and a straight line would be the wrong model to fit.\n\nSo what do we do in this situation?\n\nWell actually, this is a bit tricky as there aren't any easy fixes here. There are two broad solutions for dealing with a misspecified model.\n\n1.  The most common solution is that we need more predictor variables in the model. Here we're trying to explain/predict high school graduation only using the number of frost days. Obviously there are many more things that would affect the proportion of high school graduates than just how cold it is in a State (which is a weird potential predictor when you think about it) and so what we would need is a statistical approach that allows us to look at multiple predictor variables. We'll cover that approach in the next two sessions.\n2.  The other potential solution is to say that high school graduation can in fact be predicted only by the number of frost days but that the relationship between them isn't linear. We would then need to specify a relationship (a curve basically) and then try to fit the data to the new, non-linear, curve. This process is called, unsurprisingly, non-linear regression and we don't cover that in this course. This process is best used when there is already a strong theoretical reason for a non-linear relationship between two variables (such as sigmoidal dose-response curves in pharmacology or exponential relationships in cell growth). In this case we don't have any such preconceived notions and so it wouldn't really be appropriate in this case.\n\nNeither of these solutions can be tackled with the knowledge that we have so far in the course but we can definitely say that based upon this data set, there isn't a linear relationship (significant or otherwise) between frosty days and high school graduation rates.\n:::\n\n## Key points\n\n::: callout-note\n-   Linear regression tests if a linear relationship exists between two or more variables\n-   If so, we can use one variable to predict another\n-   A linear model has an intercept and slope and we test if the slope differs from zero\n-   We create linear models and perform an ANOVA to assess the slope coefficient\n-   We can only use a linear regression if these four assumptions are met:\n    1.  The data are linear\n    2.  Residuals are normally distributed\n    3.  Residuals are not correlated with their fitted values\n    4.  No single point should have a large influence on the linear model\n-   We can use diagnostic plots to evaluate these assumptions\n:::\n",
    "supporting": [
      "cs3_practical_linear-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}