---
title: "Model comparisons"
---

```{r}
#| echo: false
#| message: false
#| results: hide
source(file = "setup_files/setup.R")
```

```{python}
#| echo: false
#| message: false
import shutup;shutup.please()
exec(open('setup_files/setup.py').read())
```

::: {.callout-tip}
#### Learning outcomes

**Questions**

- How do I compare linear models?
- How do decide which one is the "best" model?

**Objectives**

- Be able to compare models using the Akaike Information Criterion (AIC)
- Use AIC in the context of Backwards Stepwise Elimination

:::

## Libraries and functions

::: {.callout-note collapse="true"}
## Click to expand

::: {.panel-tabset group="language"}
## R

### Libraries
### Functions

## Python

### Libraries
### Functions
:::
:::

## Purpose and aim

In the previous example we used a single data set and fitted five linear models to it depending on which predictor variables we used. Whilst this was fun (seriously, what else would you be doing right now?) it seems that there should be a "better way". Well, thankfully there is! In fact there a several methods that can be used to compare different models in order to help identify "the best" model. More specifically, we can determine if a full model (which uses all available predictor variables and interactions) is necessary to appropriately describe the dependent variable, or whether we can throw away some of the terms (e.g. an interaction term) because they don’t offer any useful predictive power.

Here we will use the **Akaike Information Criterion** in order to compare different models.

## Data and hypotheses

This section uses the `data/CS5-ladybird.csv` data set. This data set comprises of 20 observations of three variables (one dependent and two predictor). This records the clutch size (`eggs`) in a species of ladybird, alongside two potential predictor variables; the mass of the female (`weight`), and the colour of the male (`male`) which is a categorical variable.

## Backwards Stepwise Elimination

First, we load the data and visualise it:

::: {.panel-tabset group="language"}
## R

```{r}
#| message: false
ladybird <- read_csv("data/CS5-ladybird.csv")

head(ladybird)
```

We can visualise the data by `male`, so we can see if the eggs clutch size differs a lot between the two groups:

```{r}
ggplot(ladybird,
       aes(x = male, y = eggs)) +
    geom_boxplot() +
    geom_jitter(width = 0.05)
```

We can also plot the egg clutch size against the weight, again for each male colour group:

```{r}
# visualise the data
ggplot(ladybird,
       aes(x = weight, y = eggs,
           colour = male)) +
    geom_point() +
    scale_color_brewer(palette = "Dark2")
```

## Python

```{python}
ladybird_py = pd.read_csv("data/CS5-ladybird.csv")
```

We can visualise the data by `male`, so we can see if the eggs clutch size differs a lot between the two groups:

```{python}
#| results: hide
(ggplot(ladybird_py,
        aes(x = "male", y = "eggs")) +
        geom_boxplot() +
        geom_jitter(width = 0.05))
```

We can also plot the egg clutch size against the weight, again for each male colour group:

```{python}
#| results: hide
(ggplot(ladybird_py,
        aes(x = "weight", y = "eggs",
            colour = "male")) +
        geom_point() +
        scale_color_brewer(type = "qual",
                           palette = "Dark2"))
```

:::

We can see a few things already:

1. There aren't a huge number of data points in each group, so we need to be a bit cautious with drawing any firm conclusions.
2. There is quite some spread in the egg clutch sizes, with two observations in the `Melanic` group being rather low.
3. From the box plot, there does not seem to be much difference in the egg clutch size between the two male colour groups.
4. The scatter plot suggests that egg clutch size seems to increase somewhat linearly as the weight of the female goes up. There does not seem to be much difference between the two male colour groups in this respect.

### Comparing models with AIC (step 1)

We start with the complete or _full_ model, that takes into account any possible interaction between `weight` and `male`.

Next, we define the reduced model. This is the next, most simple model. In this case we're removing the interaction and constructing an additive model.

::: {.panel-tabset group="language"}
## R

```{r}
# define the full model
lm_full <- lm(eggs ~ weight * male,
              data = ladybird)
```

```{r}
# define the additive model
lm_add <- lm(eggs ~ weight + male,
             data = ladybird)
```

We then extract the AIC values for each of the models:

```{r}
AIC(lm_full)
AIC(lm_add)
```

## Python

```{python}
# create the model
model = smf.ols(formula= "eggs ~ weight * C(male)", data = ladybird_py)
# and get the fitted parameters of the model
lm_full_py = model.fit()
```

```{python}
# create the additive linear model
model = smf.ols(formula= "eggs ~ weight + C(male)", data = ladybird_py)
# and get the fitted parameters of the model
lm_add_py = model.fit()
```

We then extract the AIC values for each of the models:

```{python}
lm_full_py.aic
lm_add_py.aic
```
:::

Each line tells you the AIC score for that model. The full model has 4 parameters (the intercept, the coefficient for the continuous variable `weight`, the coefficient for the categorical variable `male` and a coefficient for the interaction term `weight:male`). The additive model has a lower AIC score with only 3 parameters (since we’ve dropped the interaction term). There are different ways of interpreting AIC scores but the most widely used interpretation says that:

* if the difference between two AIC scores is **greater than 2**, then the model with the **smallest AIC score is more supported** than the model with the higher AIC score
* if the difference between the two models’ AIC scores is **less than 2** then both models are **equally well supported**

This choice of language (supported vs significant) is deliberate and there are areas of statistics where AIC scores are used differently from the way we are going to use them here (ask if you want a bit of philosophical ramble from me). However, in this situation we will use the AIC scores to decide whether our reduced model is at least as good as the full model. Here since the difference in AIC scores is less than 2, we can say that dropping the interaction term has left us with a model that is both simpler (fewer terms) and as least as good (AIC score) as the full model. As such our additive model `eggs ~ weight + male` is designated our current _working minimal model_.

### Comparing models with AIC (step 2)

Next, we see which of the remaining terms can be dropped. We will look at the models where we have dropped both `male` and `weight` (i.e. `eggs ~ weight` and `eggs ~ male`) and compare their AIC values with the AIC of our current minimal model (`eggs ~ weight + male`). If the AIC values of at least one of our new reduced models is lower (or at least no more than 2 greater) than the AIC of our current minimal model, then we can drop the relevant term and get ourselves a new minimal model. If we find ourselves in a situation where we can drop more than one term we will drop the term that gives us the model with the lowest AIC.

::: {.panel-tabset group="language"}
## R

Drop the variable `weight` and examine the AIC:

```{r}
# define the model
lm_male <- lm(eggs ~ male,
              data = ladybird)

# extract the AIC
AIC(lm_male)
```

Drop the variable `male` and examine the AIC:

```{r}
# define the model
lm_weight <- lm(eggs ~ weight,
                data = ladybird)

# extract the AIC
AIC(lm_weight)
```

## Python

Drop the variable `weight` and examine the AIC:

```{python}
# create the model
model = smf.ols(formula= "eggs ~ C(male)", data = ladybird_py)
# and get the fitted parameters of the model
lm_male_py = model.fit()

# extract the AIC
lm_male_py.aic
```

Drop the variable `male` and examine the AIC:

```{python}
# create the model
model = smf.ols(formula= "eggs ~ weight", data = ladybird_py)
# and get the fitted parameters of the model
lm_weight_py = model.fit()

# extract the AIC
lm_weight_py.aic
```
:::

Considering both outputs together and comparing with the AIC of our current minimal model, we can see that dropping `male` has decreased the AIC further, whereas dropping `weight` has actually increased the AIC and thus worsened the model quality.

Hence we can drop `male` and our new minimal model is `eggs ~ weight`.

### Comparing models with AIC (step 3)

Our final comparison is to drop the variable `weight` and compare this simple model with a null model (`eggs ~ 1`), which assumes that the clutch size is constant across all parameters.

Drop the variable `weight` and see if that has an effect:

::: {.panel-tabset group="language"}
## R

```{r}
# define the model
lm_null <- lm(eggs ~ 1,
              data = ladybird)

# extract the AIC
AIC(lm_null)
```

## Python

```{python}
# create the model
model = smf.ols(formula= "eggs ~ 1", data = ladybird_py)
# and get the fitted parameters of the model
lm_null_py = model.fit()

# extract the AIC
lm_null_py.aic
```

:::

The AIC of our null model is quite a bit larger than that of our current minimal model `eggs ~ weight` and so we conclude that `weight` is important. As such our minimal model is `eggs ~ weight`.

So, in summary, we could conclude that:

> Female size is a useful predictor of clutch size, but male type is not so important.

At this point we can then continue analysing this minimal model, by checking the diagnostic plots and checking the assumptions. If they all pan out, then we can continue with an ANOVA.

## Notes on Backwards Stepwise Elimination

This method of finding a minimal model by starting with a full model and removing variables is called backward stepwise elimination. Although regularly practised in data analysis, there is increasing criticism of this approach, with calls for it to be avoided entirely.

Why have we made you work through this procedure then? Given their prevalence in academic papers, it is very useful to be aware of these procedures and to know that there are issues with them. In other situations, using AIC for model comparisons are justified and you will come across them regularly. Additionally, there may be situations where you feel there are good reasons to drop a parameter from your model – using this technique you can justify that this doesn’t affect the model fit. Taken together, using backwards stepwise elimination for model comparison is still a useful technique.

::: {.callout-note}
## Automatic BSE in R (but not Python)

Performing backwards stepwise elimination manually can be quite tedious. Thankfully R acknowledges this and there is a single inbuilt function called `step()` that can perform all of the necessary steps for you using AIC.

```{r}
#| eval: false
# define the full model
lm_full <- lm(eggs ~ weight * male,
              data = ladybird)

# perform backwards stepwise elimination
step(lm_full)
```

This will perform a full backwards stepwise elimination process and will find the minimal model for you.

Yes, I could have told you this earlier, but where’s the fun in that? (it is also useful for you to understand the steps behind the technique I suppose...)

When doing this in Python, you are a bit stuck. There does not seem to be an equivalent function. If you want to cobble something together yourself, then use [this link](https://stackoverflow.com/questions/22428625/does-statsmodels-or-another-python-package-offer-an-equivalent-to-rs-step-f) as a starting point.

:::

## Exercises

We are going to practice the backwards stepwise elimination technique on some of the data sets we analysed previously.

For each of the following data sets I would like you to:

1. Define the response variable
2. Define the relevant predictor variables
3. Define relevant interactions
4. Perform a backwards stepwise elimination and discover the minimal model using AIC

NB: if an interaction term is significant then any main factor that is part of the interaction term cannot be dropped from the model.

Perform a BSE on the following data sets:

* `data/CS5-trees.csv`
* `data/CS5-pm2_5.csv`

### BSE: Trees {#sec-exr_bsetrees}

:::{.callout-exercise}

{{< level 2 >}}

BSE on `trees`:

::: {.callout-answer collapse="true"}
## Answer

Let's start by reading in the data and checking which variables are in the data set.

::: {.panel-tabset group="language"}
## R

```{r}
#| message: false
trees <- read_csv("data/CS5-trees.csv")

head(trees)
```

## Python

First, we read in the data (if needed) and have a look at the variables:

```{python}
#| message: false
trees_py = pd.read_csv("data/CS5-trees.csv")

trees_py.head()
```

:::

1. The response variable is `volume`
2. The predictor variables are `girth` and `height`
3. The only possible interaction term is `girth:height`
4. We perform a BSE on the model using the `step()` function

The full model is `volume ~ girth * height`.

We perform the BSE as follows:

::: {.panel-tabset group="language"}
## R

Define the model and use the `step()` function:

```{r}
# define the full model
lm_trees <- lm(volume ~ girth * height,
               data = trees)

# perform BSE
step(lm_trees)
```

## Python

We first define the full model, then the model without the interaction (`model_1`).

We extract the AICs for both models. Because we do not have an automated way of performing the BSE, we're stringing together a few operations to make the code a bit more concise (getting the `.fit()` of the model and immediately extracting the `aic` value):

```{python}
# define the models
model_full = smf.ols(formula= "volume ~ girth * height",
                     data = trees_py)

model_1 = smf.ols(formula= "volume ~ girth + height",
                  data = trees_py)

# get the AIC of the model
model_full.fit().aic
model_1.fit().aic
```

:::

This BSE approach only gets as far as the first step (trying to drop the interaction term). We see immediately that dropping the interaction term makes the model worse. This means that the best model is still the full model.
:::
:::

### BSE: Air pollution {#sec-exr_bsepollution}

:::{.callout-exercise}

{{< level 2 >}}

Perform a BSE on `pm2_5`. Let's start by reading in the data and checking which variables are in the data set.

::: {.callout-answer collapse="true"}
## Answer
::: {.panel-tabset group="language"}
## R

```{r}
#| message: false
pm2_5 <- read_csv("data/CS5-pm2_5.csv")

head(pm2_5)
```

## Python

```{python}
pm2_5_py = pd.read_csv("data/CS5-pm2_5.csv")

pm2_5_py.head()
```
:::

1. The response variable is `pm2_5`
2. The predictor variables are all the variables, apart from `date` and `pm2_5`. It would be strange to try and create a model that relies on each individual measurement!
3. We can add the `wind_m_s:location` interaction, since it appeared that there is a difference between `inner` and `outer` London pollution levels, in relation to wind speed
4. We can start the backwards stepwise elimination with the following model:

`pm2_5 ~ avg_temp + rain_mm + wind_m_s * location`

::: {.panel-tabset group="language"}
## R

```{r}
# define the model
lm_pm2_5 <- lm(pm2_5 ~ avg_temp + rain_mm + wind_m_s * location,
               data = pm2_5)

# perform BSE
step(lm_pm2_5)
```

## Python

We first define the full model, again stringing together a few operations to be more concise.

```{python}
# define the model
model_full = smf.ols(formula= "pm2_5 ~ avg_temp + rain_mm + wind_m_s * C(location)", data = pm2_5_py)

# get the AIC of the model
model_full.fit().aic
```

Can we drop the interaction term or any of the remaining main effects?

```{python}
# define the model
model_1 = smf.ols(
    formula= "pm2_5 ~ avg_temp + rain_mm + wind_m_s + C(location)",
    data = pm2_5_py)
    
model_2 = smf.ols(
    formula= "pm2_5 ~ avg_temp + wind_m_s * C(location)",
    data = pm2_5_py)
    
model_3 = smf.ols(
    formula= "pm2_5 ~ rain_mm + wind_m_s * C(location)",
    data = pm2_5_py)

# get the AIC of the models
model_1.fit().aic
model_2.fit().aic
model_3.fit().aic
# compare to the full model
model_full.fit().aic
```

The AIC goes up quite a bit if we drop the interaction term. This means that we cannot drop the interaction term, _nor any of the main effects that are included in the interaction_. These are `wind_m_s` and `location`.

The model with the lowest AIC is the one without the `rain_mm` term, so our working model is:

```{python}
working_model = smf.ols(
    formula= "pm2_5 ~ avg_temp + wind_m_s * C(location)",
    data = pm2_5_py)
```

Now we can again check if dropping the `avg_temp` term or the `wind_m_s:location` interaction has any effect on the model performance:

```{python}
model_1 = smf.ols(
    formula= "pm2_5 ~ wind_m_s * C(location)",
    data = pm2_5_py)
    
model_2 = smf.ols(
    formula= "pm2_5 ~ avg_temp + wind_m_s + C(location)",
    data = pm2_5_py)

# get the AIC of the models
model_1.fit().aic
model_2.fit().aic
working_model.fit().aic
```

This shows that dropping the `avg_temp` term lowers the AIC, whereas dropping the interaction term makes the model markedly worse.

So, our new working model is:

```{python}
working_model = smf.ols(
    formula= "pm2_5 ~ wind_m_s * C(location)",
    data = pm2_5_py)
```

Lastly, now we've dropped the `avg_temp` term we can do one final check on the interaction term:

```{python}
model_1 = smf.ols(
    formula= "pm2_5 ~ wind_m_s + C(location) + wind_m_s + C(location)",
    data = pm2_5_py)

model_1.fit().aic
working_model.fit().aic
```

Dropping the interaction _still_ makes the model worse.
:::

Our minimal model is thus:

`pm2_5 ~ wind_m_s + location + wind_m_s:location`

:::
:::

## Summary

::: {.callout-tip}
#### Key points

- We can use Backwards Stepwise Elimination (BSE) on a full model to see if certain terms add to the predictive power of the model or not
- The AIC allows us to compare different models - if there is a difference in AIC of more than 2 between two models, then the smallest AIC score is more supported
:::
