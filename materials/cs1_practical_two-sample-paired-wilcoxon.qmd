---
title: "Wilcoxon signed rank test"
---

```{r}
#| echo: false
#| message: false
#| results: hide
source(file = "setup_files/setup.R")
```

```{python}
#| echo: false
#| message: false
import shutup;shutup.please()
exec(open('setup_files/setup.py').read())
```

```{r}
#| echo: false
#| message: false
# R data required for this session
cortisol <- read_csv("data/CS1-twopaired.csv")
cortisol_r <- read.csv("data/CS1-twopaired.csv")
```

```{python}
#| echo: false
# Python data required for this session
cortisol_py = pd.read_csv('data/CS1-twopaired.csv')
cortisol_diff_py = pd.pivot(cortisol_py, index = "patient_id", columns = "time", values = "cortisol")

# add a new column with difference between
# evening and morning cortisol levels
cortisol_diff_py["cortisol_change"] = cortisol_diff_py["evening"].subtract(cortisol_diff_py["morning"])
```

::: callout-tip
## Learning outcomes

**Questions**

- When do I perform a Wilcoxon signed-rank test?
- What are the assumptions?
- How do I interpret and present the results of the test?

**Objectives**

- Set out your hypothesis for comparing two samples of paired, non-normal continuous data
- Be able to summarise and visualise the data
- Understand and assess the underlying assumptions of the test
- Perform a Wilcoxon signed-rank test
- Be able to interpret and report the results

:::

A Wilcoxon signed-rank test is an alternative to a paired t-test. It does not require that the data are drawn from normal distributions, but it does require that the distribution of the differences is similar in shape. We're effectively testing to see if the median of the differences between the two samples differs significantly from zero.

## Libraries and functions

::: {.callout-note collapse="true"}
## Click to expand

::: {.panel-tabset group="language"}
## tidyverse

### Libraries

```{r}
#| eval: false
# A collection of R packages designed for data science
library(tidyverse)

# Converts stats functions to a tidyverse-friendly format
library(rstatix)
```

### Functions

```{r}
#| eval: false
#| warning: false
# Computes summary statistics                         
rstatix::get_summary_stats() 

# Performs a one-sample t-test, Student's t-test
# and Welch's t-test in later sections
stats::t.test()

# Performs a Shapiro-Wilk test for normality
stats::shapiro.test()

# Plots a Q-Q plot for comparison with a normal distribution
ggplot2::stat_qq()

# Adds a comparison line to the Q-Q plot
ggplot2::stat_qq_line()

# Plots jittered points by adding a small amount of random variation
# to each point, to handle overplotting
ggplot2::geom_jitter()

# "Widens" the data, increasing the number of columns
tidyr::pivot_wider()
```

## R

### Functions

```{r}
#| eval: false
#| warning: false
# Reshapes a data frame from 'long' to 'wide' and vice versa
stats::reshape()

# Performs a one-sample t-test, Student's t-test
# and Welch's t-test in later sections
stats::t.test()

# Plots a Q-Q plot for comparison with a normal distribution
stats::qqnorm()

# Adds a comparison line to the Q-Q plot
stats::qqline()

# Performs a Shapiro-Wilk test for normality
stats::shapiro.test()
```

## Python

| Libraries                                                             | Description                                                              |
|:----------------------|:------------------------------------------------|
| [`pandas`](https://pandas.pydata.org/docs/getting_started/index.html) | A Python data analysis and manipulation tool.                            |
| [`pingouin`](https://pingouin-stats.org)                              | A Python module developed to have simple yet exhaustive stats functions. |
| [`plotnine`](https://plotnine.readthedocs.io/en/stable/)              | The Python equivalent of `ggplot2`.                                      |

| Functions                                                                                                               | Description                                                                                |
|:-----------------------------------|:-----------------------------------|
| [`pandas.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)                                  | Reads in a `.csv` file.                                                                    |
| [`pingouin.wilcoxon()`](https://pingouin-stats.org/generated/pingouin.wilcoxon.html#pingouin.wilcoxon)                  | Tests the null hypothesis that two related paired samples come from the same distribution. |
| [`pingouin.normality()`](https://pingouin-stats.org/generated/pingouin.normality.html)                                  | Performs the Shapiro-Wilk test for normality.                                              |
| [`plotnine.stats.stat_qq()`](https://plotnine.readthedocs.io/en/stable/generated/plotnine.stats.stat_qq.html)           | Plots a Q-Q plot for comparison with a normal distribution.                                |
| [`plotnine.stats.stat_qq_line()`](https://plotnine.readthedocs.io/en/stable/generated/plotnine.stats.stat_qq_line.html) | Adds a comparison line to the Q-Q plot.                                                    |
:::
:::

## Data and hypotheses

Using the `cortisol` dataset from before we form the following null and alternative hypotheses:

-   $H_0$: The median of the difference in cortisol levels between the two groups is 0 $(\mu M = \mu E)$
-   $H_1$: The median of the difference in cortisol levels between the two groups is not 0 $(\mu M \neq \mu E)$

We use a two-tailed Wilcoxon signed-rank test to see if we can reject the null hypothesis.

## Summarise and visualise

Already implemented previously.

## Assumptions

These have been checked previously.

## Implement and interpret the test

Perform a two-tailed, Wilcoxon signed-rank test:

::: {.panel-tabset group="language"}
## tidyverse

```{r}
# perform the test
cortisol %>% 
    wilcox.test(cortisol ~ time,
                alternative = "two.sided",
                paired = TRUE,
                data = .)
```

-   The first argument gives the formula
-   The second argument gives the type of alternative hypothesis and must be one of `two.sided`, `greater` or `less`
-   The third argument indicates that the test is paired
-   The last argument is the data set (coming from the pipe, so denoted as `.`)

## R

```{r}
wilcox.test(cortisol ~ time,
            alternative = "two.sided",
            paired = TRUE,
            data = cortisol_r)
```

-   The first argument gives the formula
-   The second argument gives the type of alternative hypothesis and must be one of `two.sided`, `greater` or `less`
-   The third argument indicates that the test is paired
-   The last argument is the data set

## Python

We'll use the wide format data set that we created previously:

```{python}
pg.wilcoxon(x = cortisol_diff_py["evening"],
            y = cortisol_diff_py["morning"],
            alternative = "two-sided",
            correction = True)
```
:::

The p-value is given in the `p` column (p-value = 0.000168). Given that this is less than 0.05 we can still reject the null hypothesis.

> A two-tailed, Wilcoxon signed-rank test indicated that the median cortisol level in adult females differed significantly between the morning (320.5 nmol/l) and the evening (188.9 nmol/l, p = 0.00017).


## Exercise: Deer legs

Using the following data on deer legs (yes, really!), test the null hypothesis that the fore and hind legs of the deer in this data set are the same length.

```{r}
#| echo: false
#| warning: false
#| message: false
# deer leg length (cm)
read_csv("data/CS1-deer.csv") %>% 
  pivot_wider(names_from = leg, values_from = length) %>% 
  select(-id)
```

Do these results provide any evidence to suggest that fore- and hind-leg length differ in deer?

1.  Write down the null and alternative hypotheses
2.  Import the data from `data/CS1-deer.csv`
3.  Summarise and visualise the data
4.  Check your assumptions (normality and variance) using appropriate tests
5.  Discuss with your (virtual) neighbour which test is most appropriate?
6.  Perform the test
7.  Write down a sentence that summarises the results that you have found

::: {.callout-tip collapse="true"}
## Answer

### Hypotheses

$H_0$ : foreleg average (mean or median) $=$ hindleg average (mean or median)

$H_1$ : foreleg average $\neq$ hindleg average

### Import data, summarise and visualise

First of all, we need to load in the data.

::: {.panel-tabset group="language"}
## tidyverse

```{r}
#| message: false
#| warning: false
# load the data
deer <- read_csv("data/CS1-deer.csv")

# have a look
deer
```

The ordering of the data is important here; the first hind leg row corresponds to the first fore leg row, the second to the second and so on. To indicate this we use an `id` column, where each observation has a unique ID.

Let's look at the data and see what it tells us.

```{r}
# summarise the data
summary(deer)
```

```{r}
# or even summarise by leg type
deer %>% 
  select(-id) %>% 
  group_by(leg) %>% 
  get_summary_stats(type = "common")
```

```{r}
# we can also visualise the data
deer %>% 
  ggplot(aes(x = leg, y = length)) +
  geom_boxplot()
```

All of this suggests that there might be a difference between the legs, with hind legs being longer than forelegs. However, this representation obscures the fact that we have *paired* data. What we really need to look at is the difference in leg length for each observation:

```{r}
# create a data set that contains the difference in leg length
leg_diff <- deer %>% 
  pivot_wider(id_cols = id,
              names_from = leg,
              values_from = length) %>% 
  mutate(leg_diff = hindleg - foreleg)
```

```{r}
# plot the difference in leg length
leg_diff %>% 
  ggplot(aes(y = leg_diff)) +
  geom_boxplot()
```

Additionally, we can also plot the data by observation:

```{r}
# plot the data by observation
deer %>% 
  ggplot(aes(x = leg, y = length, group = id)) +
  geom_point() +
  geom_line()
```

## R

```{r}
deer_r <- read.csv("data/CS1-deer.csv")

head(deer_r)
```

The ordering of the data is important here; the first hind leg row corresponds to the first fore leg row, the second to the second and so on. To indicate this we use an `id` column, where each observation has a unique ID.

Let's look at the data and see what we can see.

```{r}
# summarise the data
summary(deer_r)
```

```{r}
# or even summarise by leg type
aggregate(length ~ leg,
          FUN = summary,
          data = deer_r)
```

```{r}
# we can also visualise the data
boxplot(length ~ leg, data = deer_r)
```

All of this suggests that there might be a difference between the legs, with hind legs being longer than forelegs. However, this representation obscures the fact that we have *paired* data. What we really need to look at is the difference in leg length for each observation:

```{r}
leg_diff_r <- reshape(deer_r,
        idvar = "id",
        timevar = "leg",
        direction = "wide")

# calculate difference in leg length
leg_diff_r$leg_diff <- leg_diff_r$length.hindleg - leg_diff_r$length.foreleg

head(leg_diff_r)
```

```{r}
# plot the difference in leg length
boxplot(leg_diff_r$leg_diff)
```

Additionally, we can also plot the data by observation:

```{r}
# plot the data by observation
matplot(t(leg_diff_r[ , 2:3]),
        pch = 1,
        type = c("b"),
        col = 1:10)
```

Again, as far as I am aware of, there isn't a straightforward method of plotting paired data using the base R functionality. Hence the data gymnastics:

-   the default `plot()` function doesn't support this - the standard `matplot()` function does
-   the `t` function transposes the data, and I'm only selecting the second and third columns (`[ , 2:3]`) which contain the paired leg measurements.
-   to group (pair) the data, we're using colours, one for each of the 10 observations (`col = 1:10`)

## Python

```{python}
#| message: false
#| warning: false
# load the data
deer_py = pd.read_csv("data/CS1-deer.csv")

# have a look
deer_py.head()
```

The ordering of the data is important here; the first hind leg row corresponds to the first fore leg row, the second to the second and so on. To indicate this we use an `id` column, where each observation has a unique ID.

Let's look at the data and see what we can see.

```{python}
# summarise the data
deer_py.describe()
```

We can also summarise by leg type:

```{python}
deer_py.groupby("leg")["length"].describe()
```

It might be more helpful to look at the *difference* in leg length. In order to calculate that, we need to reformat our data into a 'wide' format first:

```{python}
# reformat the data into a 'wide' format
leg_diff_py = pd.pivot(deer_py,
                       index = "id",
                       columns = "leg",
                       values = "length")

# have a look at the format
leg_diff_py.head()
```

Next, we can add a new column `leg_diff` that contains the leg difference:

```{python}
# add a new column with difference between
# hind and fore leg length
leg_diff_py["leg_diff"] = leg_diff_py["hindleg"].subtract(leg_diff_py["foreleg"])
 
```

Finally, we can visualise this:

```{python}
#| results: hide
# we can also visualise the data
(
  ggplot(leg_diff_py,
    aes(x = "1",
        y = "leg_diff")) +
    geom_boxplot()
)
```

All of this suggests that there might be a difference between the legs, with hind legs being longer than forelegs. However, this representation obscures the fact that we have *paired* data. What we really need to look at is the difference in leg length for each observation:

```{python}
#| results: hide
# plot paired observations
(
  ggplot(deer_py,
    aes(x = "leg",
        y = "length",
        group = "id")) +
    geom_point() +
    geom_line()
)
```
:::

All of this gives us a much clearer picture. It looks as though the hindlegs are about 4 cm longer than the forelegs, on average. It also suggests that our leg differences might not be normally distributed (the data look a bit skewed in the boxplot).

### Assumptions

We need to consider the distribution of the *difference* in leg lengths rather than the individual distributions.

::: {.panel-tabset group="language"}
## tidyverse

Shapiro-Wilk test:

```{r}
# perform Shapiro-Wilk test on leg differences
leg_diff %>% 
    pull(leg_diff) %>% 
    shapiro.test()
```

Q-Q plot:

```{r}
# create a Q-Q plot
leg_diff %>% 
  ggplot(aes(sample = leg_diff)) +
  stat_qq() +
  stat_qq_line(colour = "red")
```

## R

Shapiro-Wilk test:

```{r}
# perform Shapiro-Wilk test on leg differences
shapiro.test(leg_diff_r$leg_diff)
```

Q-Q plot:

```{r}
# create a Q-Q plot
qqnorm(leg_diff_r$leg_diff)
qqline(leg_diff_r$leg_diff, col = "red")
```

## Python

Shapiro-Wilk test:

```{python}
# perform Shapiro-Wilk test on leg length differences
pg.normality(leg_diff_py["leg_diff"])
```

Create the Q-Q plot:

```{python}
#| results: hide
# create the Q-Q plot
(
  ggplot(leg_diff_py,
    aes(sample = "leg_diff")) +
    stat_qq() +
    stat_qq_line(colour = "red")
)
```
:::

Both our Shapiro-Wilk test and our Q-Q plot suggest that the difference data aren't normally distributed, which rules out a paired t-test. We should therefore consider a paired Wilcoxon test next. Remember that this test requires that the distribution of differences be symmetric, whereas our box plot from before suggested that the data were very much skewed.

This means that we're not able to perform a paired Wilcoxon test either!

### Conclusions

So, frustratingly, neither of the tests at our disposal are appropriate for this data set. The differences in fore leg and hind leg lengths are neither normal enough for a paired t-test nor are they symmetric enough for a Wilcoxon test. We also don't have enough data to just use the t-test (we'd need more than 30 points or so). So what do we do in this situation? Well, the answer is that there aren't actually any traditional statistical tests that are valid for this data set as it stands!

There are two options available to someone:

1.  try transforming the raw data (take logs, square root, reciprocals) and hope that one of them leads to a modified data set that satisfies the assumptions of one of the tests we've covered, or
2.  use a permutation test approach (which would work but is beyond the scope of this course).

The reason I included this example in the first practical is purely to illustrate how a very simple data set with an apparently clear message (leg lengths differ within deer) can be intractable. You don't need to have very complex data sets before you go beyond the capabilities of classical statistics.

As Jeremy Clarkson [would put it](https://www.quotes.net/mquote/941330):

> And on that bombshell, it's time to end. Goodnight!
:::

## Key points

::: callout-note
-   We use two-sample tests to see if two samples of continuous data come from the same parent distribution
-   This essentially boils down to testing if the mean or median differs between the two samples
-   There are 5 key two-sample tests: Student's t-test, Welch's t-test, Mann-Whitney U test, paired t-test and Wilcoxon signed-rank test
-   Which one you use depends on normality of the distribution, sample size, paired or unpaired data and variance of the samples
-   Parametric tests are used if the data are normally distributed or the sample size is large
-   Non-parametric tests are used if the data are not normally distributed *and* the sample size is small
-   Equality of variance then determines which test is appropriate
-   You three questions to determine the test:
    1.  is my data paired?
    2.  do I need a parametric or non-parametric test
    3.  can I assume equality of variance?
:::
