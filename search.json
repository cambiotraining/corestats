[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Core aims\nWelcome to Core statistics!\nThese sessions are intended to enable you to perform core data analysis techniques appropriately and confidently using R or Python.\nThey are not a “how to mindlessly use a stats program” course!\nThere are several things that we try to achieve during this course.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#core-aims",
    "href": "index.html#core-aims",
    "title": "Introduction",
    "section": "",
    "text": "NoteCourse aims\n\n\n\nTo know what to do when presented with an arbitrary data set e.g.\n\nKnow what data analysis techniques are available\nKnow which ones are allowable\nBe able to carry these out and understand the results",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#core-topics",
    "href": "index.html#core-topics",
    "title": "Introduction",
    "section": "Core topics",
    "text": "Core topics\n\nSimple hypothesis testing\nCategorical predictors\nContinuous predictors\nTwo predictors\nMultiple predictors\nPower analysis",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#practicals",
    "href": "index.html#practicals",
    "title": "Introduction",
    "section": "Practicals",
    "text": "Practicals\nEach practical document is divided up into various sections. In each section there will be some explanatory text which should help you to understand what is going on and what you’re trying to achieve.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "2  Setup",
    "section": "",
    "text": "2.1 Installation",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#installation",
    "href": "setup.html#installation",
    "title": "2  Setup",
    "section": "",
    "text": "RPython\n\n\n\nRequired software\n\nDownload R and install it using default options. (Note: choose the “base” version for Windows)\nDownload RStudio and install it using default options.\n\n\n\nSetting up RStudio\nAfter installing RStudio, change some of its default options (you only need to do this once):\n\nFrom the upper menu go to Tools &gt; Global Options…\nUntick the option “Restore .RData to workspace on startup.”\nChange “Save workspace to .RData on exit” option to “Never”\nPress OK\n\n\n\n\nFor this course we’ll be using Visual Studio Code. This provides support for various programming languages (including Python and R). It works on Windows, MacOS and Linux. It’s also open-source and free.\nPlease refer to the installation instructions and make sure that you verify that Python code will run.\nA brief sequence of events:\n\nInstall Visual Studio Code\nInstall the VS Code Python extension\nInstall a Python interpreter\n\nWindows: install from Python.org or use the Microsoft Store\nMacOS: install the Homebrew package manager, then use this to install Python\nLinux: comes with Python 3, but needs pip to install additional packages",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#packages",
    "href": "setup.html#packages",
    "title": "2  Setup",
    "section": "2.2 Packages",
    "text": "2.2 Packages\nWe will be using the following packages throughout this course:\n\nRPython\n\n\nInstall the required packages. Run the following code in the console:\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"rstatix\")\ninstall.packages(\"ggResidpanel\")\n\n\nTesting your installation\nOn the RStudio panel named “Console” type library(tidyverse) and press Enter\nA message similar to this should print:\n── Attaching packages ─────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──\n✔ ggplot2 3.2.1     ✔ purrr   0.3.2\n✔ tibble  2.1.3     ✔ dplyr   0.8.3\n✔ tidyr   1.0.0     ✔ stringr 1.4.0\n✔ readr   1.3.1     ✔ forcats 0.4.0\n── Conflicts ────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nIf instead you get the message:\nError in library(tidyverse) : there is no package called ‘tidyverse’\nthen your package installation did not work. Please ask the instructors for assistance before the course.\n\n\n\n\nNumPy\nThe numpy package provides fundamental data science functionality to Python. For more information see: https://numpy.org/doc/stable/#\nIt can be installed via pip\n\npip install numpy\n\nor conda\n\nconda install -c conda-forge numpy\n\n\n\npandas\nThe pandas package provides data structures to Python. For more information see: https://pandas.pydata.org/docs/getting_started/install.html.\nIt can be installed via pip\n\npip install pandas\n\nor conda\n\nconda install pandas\n\n\n\npingouin\nThe pingouin package provides statistical functionality to Python. For more information see: https://pingouin-stats.org.\nIt can be installed via pip\n\npip install pingouin\n\nor conda\n\nconda install -c conda-forge pingouin\n\n\n\npatchworklib\nThe patchworklib package provides an easy way for assembling figures. This package is required to run the course-specific dgplots() function. For more information see: https://pypi.org/project/patchworklib/.\nIt can be installed via pip\n\npip install patchworklib\n\n\n\nplotnine\nThe plotnine packages provides a grammar of graphics to Python - an equivalent to the ggplot2 package in R. For more information see: https://plotnine.readthedocs.io/en/stable/#.\nIt can be installed via pip\n\npip install plotnine\n\nor conda\n\nconda install -c conda-forge plotnine\n\n\n\n2.2.1 scikit-posthocs\nThe scikit-posthocs package provides post-hoc functionality. For more information see: https://scikit-posthocs.readthedocs.io/en/latest/\nIt can be installed via pip\n\npip install scikit-posthocs\n\n\n\n2.2.2 statsmodels\nThe statsmodels package provides statistical functionality. For more information see: https://www.statsmodels.org/stable/index.html.\nIt can be installed via pip\n\npip install statsmodels\n\nor conda\n\nconda install -c conda-forge statsmodels",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setup</span>"
    ]
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "3  Data",
    "section": "",
    "text": "3.1 Data\nThe data we will be using throughout all the sessions are contained in a single ZIP file. They are all small CSV files (comma separated values). You can download the data below:\nDownload ZIP file",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "materials.html#index-datasets",
    "href": "materials.html#index-datasets",
    "title": "3  Data",
    "section": "",
    "text": "Warning\n\n\n\nThe data we use throughout the course is varied, covering many different topics. In some cases the data on medical or socioeconomic topics may be uncomfortable to some, since they can touch on diseases or death.\nAll the data are chosen for their pedagogical effectiveness.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "materials.html#tidy-data",
    "href": "materials.html#tidy-data",
    "title": "3  Data",
    "section": "3.2 Tidy data",
    "text": "3.2 Tidy data\nFor two samples the data can be stored in one of three formats:\n\nas two separate vectors,\nin a stacked data frame,\nor in an unstacked data frame/list.\n\nTwo separate vectors case is (hopefully) obvious.\nWhen using a data frame we have different options to organise our data. The best way of formatting data is by using the tidy data format.\n\nTidy data has the following properties:\n\nEach variable has its own column\nEach observation has its own row\nEach value has its own cell\n\n\nStacked form (or long format data) is where the data is arranged in such a way that each variable (thing that we measured) has its own column. If we consider a dataset containing meerkat weights (in g) from two different countries then a stacked format of the data would look like:\n\n\n# A tibble: 6 × 2\n  country  weight\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Botswana    514\n2 Botswana    568\n3 Botswana    519\n4 Uganda      624\n5 Uganda      662\n6 Uganda      633\n\n\nIn the unstacked (or wide format) form a variable (measured thing) is present in more than one column. For example, let’s say we measured meerkat weight in two countries over a period of years. We could then organise our data in such a way that for each year the measured values are split by country:\n\n\n# A tibble: 3 × 3\n   year Botswana Uganda\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1  1990      514    624\n2  1992      568    662\n3  1995      519    633\n\n\nHaving tidy data is the easiest way of doing analyses in programming languages and I would strongly encourage you all to start adopting this format as standard for data collection and processing.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "materials.html#conditional-operators",
    "href": "materials.html#conditional-operators",
    "title": "3  Data",
    "section": "3.3 Conditional operators",
    "text": "3.3 Conditional operators\nTo set filtering conditions, use the following relational operators:\n\n&gt; is greater than\n&gt;= is greater than or equal to\n&lt; is less than\n&lt;= is less than or equal to\n== is equal to\n!= is different from\n%in% is contained in\n\nTo combine conditions, use the following logical operators:\n\n& AND\n| OR",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html",
    "href": "materials/cs1_practical_one-sample.html",
    "title": "4  One-sample data",
    "section": "",
    "text": "4.1 Libraries and functions",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#libraries-and-functions",
    "href": "materials/cs1_practical_one-sample.html#libraries-and-functions",
    "title": "4  One-sample data",
    "section": "",
    "text": "NoteClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n4.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n\n\n4.1.2 Functions\n\n# Performs a one-sample t-test, Student's t-test and Welch's t-test in later sections\nrstatix::t_test()\n\n# Performs a Shapiro-Wilk test for normality\nstats::shapiro.test()\n\n# Performs one and two sample Wilcoxon tests\nrstatix::wilcox_test()\n\n# Plots a Q-Q plot for comparison with a normal distribution\nggplot2::stat_qq()\n\n# Adds a comparison line to the Q-Q plot\nggplot2::stat_qq_line()\n\n\n\n\n\n4.1.3 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n\n\n4.1.4 Functions\n\n# Reads in a .csv file\npandas.DataFrame.read_csv()\n\n# Performs the Shapiro-Wilk test for normality\npingouin.normality()\n\n# Performs a t-test\npingouin.ttest()\n\n# Performs Wilcoxon signed rank test\npingouin.wilcoxon()\n\n# Plots a Q-Q plot for comparison with a normal distribution\nplotnine.stats.stat_qq()\n\n# Adds a comparison line to the Q-Q plot\nplotnine.stats.stat_qq_line()",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#purpose-and-aim",
    "href": "materials/cs1_practical_one-sample.html#purpose-and-aim",
    "title": "4  One-sample data",
    "section": "4.2 Purpose and aim",
    "text": "4.2 Purpose and aim\nOne sample tests are used when we have a single sample of continuous data. It is used to find out if the sample came from a parent distribution with a given mean (or median). This essentially boils down to finding out if the sample mean (or median) is “close enough” to our hypothesised parent population mean (or median). So, in the figure below, we could use these tests to see what the probability is that the sample of ten points comes from the distribution plotted above it i.e. a population with a mean of 20 mm.",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#choosing-a-test",
    "href": "materials/cs1_practical_one-sample.html#choosing-a-test",
    "title": "4  One-sample data",
    "section": "4.3 Choosing a test",
    "text": "4.3 Choosing a test\nThere are two tests that we are going to look at in this situation; the one-sample t-test, and the one-sample Wilcoxon signed rank-sum test. Both tests work on the sort of data that we’re considering here, but they both have different assumptions.\nIf your data is normally distributed, then a one-sample t-test is appropriate. If your data aren’t normally distributed, but their distribution is symmetric, and the sample size is small then a one-sample Wilcoxon signed rank-sum test is more appropriate.\nFor each statistical test we consider there will be five tasks. These will come back again and again, so pay extra close attention.\n\n\n\n\n\n\nImportant\n\n\n\n\nSetting out of the hypothesis\nSummarise and visualisation of the data\nAssessment of assumptions\nImplementation of the statistical test\nInterpreting the output and presentation of results\n\n\n\nWe won’t always carry these out in exactly the same order, but we will always consider each of the five tasks for every test.",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#data-and-hypotheses",
    "href": "materials/cs1_practical_one-sample.html#data-and-hypotheses",
    "title": "4  One-sample data",
    "section": "4.4 Data and hypotheses",
    "text": "4.4 Data and hypotheses\nFor example, suppose we measure the body lengths of male guppies (in mm) collected from the Guanapo River in Trinidad. We want to test whether the data support the hypothesis that the mean body is actually 20 mm. We form the following null and alternative hypotheses:\n\n\\(H_0\\): The mean body length is equal to 20mm (\\(\\mu =\\) 20).\n\\(H_1\\): The mean body length is not equal to 20mm (\\(\\mu \\neq\\) 20).\n\nWe will use a one-sample, two-tailed t-test to see if we should reject the null hypothesis or not.\n\nWe use a one-sample test because we only have one sample.\nWe use a two-tailed t-test because we want to know if our data suggest that the true (population) mean is different from 20 mm in either direction rather than just to see if it is greater than or less than 20 mm (in which case we would use a one-tailed test).\nWe’re using a t-test because we don’t know any better yet and because I’m telling you to. We’ll look at what the precise assumptions/requirements need to be in a moment.\n\nMake sure you have downloaded the data (see: Data section) and placed it within your working directory.\n\nRPython\n\n\nFirst we load the relevant libraries:\n\n# load tidyverse\nlibrary(tidyverse)\n\nWe then read in the data and create a table containing the data.\n\n# import the data\nfishlengthDF &lt;- read_csv(\"data/CS1-onesample.csv\")\n\nfishlengthDF\n\n# A tibble: 29 × 3\n      id river   length\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 Guanapo   19.1\n 2     2 Guanapo   23.3\n 3     3 Guanapo   18.2\n 4     4 Guanapo   16.4\n 5     5 Guanapo   19.7\n 6     6 Guanapo   16.6\n 7     7 Guanapo   17.5\n 8     8 Guanapo   19.9\n 9     9 Guanapo   19.1\n10    10 Guanapo   18.8\n# ℹ 19 more rows\n\n\nThe first line reads the data into R and creates an object called a tibble, which is a type of data frame. This data frame contains 3 columns: a unique id, river encoding the river and length with the measured guppy length.\n\n\nWe then read the data in:\n\n# load the data\nfishlength_py = pd.read_csv('data/CS1-onesample.csv')\n\n# inspect the data\nfishlength_py.head()\n\n   id    river  length\n0   1  Guanapo    19.1\n1   2  Guanapo    23.3\n2   3  Guanapo    18.2\n3   4  Guanapo    16.4\n4   5  Guanapo    19.7",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#summarise-and-visualise",
    "href": "materials/cs1_practical_one-sample.html#summarise-and-visualise",
    "title": "4  One-sample data",
    "section": "4.5 Summarise and visualise",
    "text": "4.5 Summarise and visualise\nSummarise the data and visualise it:\n\nRPython\n\n\n\nsummary(fishlengthDF)\n\n       id        river               length    \n Min.   : 1   Length:29          Min.   :11.2  \n 1st Qu.: 8   Class :character   1st Qu.:17.5  \n Median :15   Mode  :character   Median :18.8  \n Mean   :15                      Mean   :18.3  \n 3rd Qu.:22                      3rd Qu.:19.7  \n Max.   :29                      Max.   :23.3  \n\n\nFrom the summary() output we can see that the mean and median of the length variable are quite close together. The id column also has minimum, maximum, mean etc. values - these are not useful! The numbers in the id column have no numerical value, but are just to ensure each observation can be traced back, if needed.\n\nggplot(fishlengthDF,\n       aes(x = river,\n           y = length)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nFirst we have a look at a numerical summary of the data:\n\nfishlength_py.describe()\n\n              id     length\ncount  29.000000  29.000000\nmean   15.000000  18.296552\nstd     8.514693   2.584636\nmin     1.000000  11.200000\n25%     8.000000  17.500000\n50%    15.000000  18.800000\n75%    22.000000  19.700000\nmax    29.000000  23.300000\n\n\nFrom the describe() output we can see that the mean and median of the length variable are quite close together. The id column also has minimum, maximum, mean etc. values - these are not useful! The numbers in the id column have no numerical value, but are just to ensure each observation can be traced back, if needed.\n\np = (ggplot(fishlength_py,\n        aes(x = \"river\",\n            y = \"length\")) +\n     geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nThe data do not appear to contain any obvious errors, and whilst both the mean and median are less than 20 (18.3 and 18.8 respectively) it is not absolutely certain that the sample mean is sufficiently different from this value to be “statistically significant”, although we may anticipate such a result.",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#assumptions",
    "href": "materials/cs1_practical_one-sample.html#assumptions",
    "title": "4  One-sample data",
    "section": "4.6 Assumptions",
    "text": "4.6 Assumptions\nWhen it comes to one-sample tests, we have two options:\n\nt-test\nWilcoxon signed-rank test\n\nFor us to use a t-test for this analysis (and for the results to be valid) we have to make two assumptions:\n\nThe parent distribution from which the sample is taken is normally distributed (and as such the sample data are normally distributed themselves).\n\n\n\n\n\n\n\nNote\n\n\n\nIt is worth noting though that the t-test is actually pretty robust in situations where the sample data are not normal. For sufficiently large sample sizes (your guess is as good as mine, but conventionally this means about 30 data points), you can use a t-test without worrying about whether the underlying population is normally distributed or not.\n\n\n\nEach data point in the sample is independent of the others. This is in general not something that can be tested for and instead has to be considered from the sampling procedure. For example, taking repeated measurements from the same individual would generate data that are not independent.\n\nThe second point we know nothing about and so we ignore it here (this is an issue that needs to be considered from the experimental design), whereas the first assumption can be checked. There are three ways of checking for normality:\nIn increasing order of rigour, we have\n\nHistogram\nQuantile-quantile plot\nShapiro-Wilk test\n\n\n4.6.1 Histogram of the data\nPlot a histogram of the data, which gives:\n\nRPython\n\n\n\nggplot(fishlengthDF,\n       aes(x = length)) +\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n\n\n\n\np = (ggplot(fishlength_py,\n        aes(x = \"length\")) +\n     geom_histogram(bins = 15))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nThe distribution appears to be uni-modal and symmetric, and so it isn’t obviously non-normal. However, there are a lot of distributions that have these simple properties but which aren’t normal, so this isn’t exactly rigorous. Thankfully there are other, more rigorous tests.\nNB. By even looking at this distribution to assess the assumption of normality we are already going far beyond what anyone else ever does. Nevertheless, we will continue.\n\n\n4.6.2 Q-Q plot of the data\nQ-Q plot is the short for quantile-quantile plot. This diagnostic plot (as it is sometimes called) is a way of comparing two distributions. How Q-Q plots work won’t be explained here but will be addressed in the next session.\nConstruct a Q-Q Plot of the quantiles of the data against the quantiles of a normal distribution:\n\nRPython\n\n\n\nggplot(fishlengthDF,\n       aes(sample = length)) +\n  stat_qq() +\n  stat_qq_line(colour = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\np = (ggplot(fishlength_py,\n        aes(sample = \"length\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"blue\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nWhat is important to know is that if the data were normally distributed then all of the points should lie on (or close to) the diagonal line in this graph.\nIn this case, the points lie quite close to the line for the most part but the sample quantiles (points) from either end of the sample distribution are either smaller (below the line on the left) or larger (above the line on the right) than expected if they were supposed to be normally distributed. This suggests that the sample distribution is a bit more spread out than would be expected if it came from a normal distribution.\nIt is important to recognise that there isn’t a simple unambiguous answer when interpreting these types of graph, in terms of whether the assumption of normality has been well met or not and instead it often boils down to a matter of experience.\nIt is a very rare situation indeed where the assumptions necessary for a test will be met unequivocally and a certain degree of personal interpretation is always needed. Here you have to ask yourself whether the data are normal “enough” for you to be confident in the validity of the test.\nBelow are four examples of QQ plots for different types of distributions:\n\n\n\n\n\n\n\n\n\nThese two graphs relate to 200 data points that have been drawn from a normal distribution. Even here you can see that the points do not all lie perfectly on the diagonal line in the QQ plot, and a certain amount of deviation at the top and bottom of the graph can happen just by chance (if I were to draw a different set of point then the graph would look slightly different).\n\n\n\n\n\n\n\n\n\n\nThese two graphs relate to 200 data points that have been drawn from a uniform distribution. Uniform distributions are more condensed than normal distributions, and this is reflected in the QQ plot having a very pronounced S-shaped pattern to it (this is colloquially known as snaking).\n\n\n\n\n\n\n\n\n\n\nThese two graphs relate to 200 data points that have been drawn from a t distribution. t distributions are more spread out than normal distributions, and this is reflected in the QQ plot again having a very pronounced S-shaped pattern to it, but this time the snaking is a reflection of that observed for the uniform distribution.\n\n\n\n\n\n\n\n\n\n\nThese two graphs relate to 200 data points that have been drawn from an exponential distribution. Exponential distributions are not symmetric and are very skewed compared with normal distributions. The significant right-skew in this distribution is reflected in the QQ plot again having points that curve away above the diagonal line at both ends (a left-skew would have the points being below the line at both ends).\nIn all four cases it is worth noting that the deviations are only at the ends of the plot.\n\n\n4.6.3 Shapiro-Wilk test\nThis is one of a number of formal statistical test that assess whether a given sample of numbers come from a normal distribution. It calculates the probability of getting the sample data if the underlying distribution is in fact normal. It is very easy to carry out in R.\nPerform a Shapiro-Wilk test on the data:\n\nRPython\n\n\nThe shapiro.test() function needs a numerical vector as input. We get this by extracting the length column.\n\nshapiro.test(fishlengthDF$length)\n\n\n    Shapiro-Wilk normality test\n\ndata:  fishlengthDF$length\nW = 0.94938, p-value = 0.1764\n\n\n\nThe 1st line gives the name of the test and the data: tells you which data are used.\nThe 3rd line contains the two key outputs from the test:\nThe calculated W-statistic is 0.9494 (we don’t need to know this)\nThe p-value is 0.1764\n\n\n\nWe take the length values from the fishlength_py data frame and pass that to the normality() function in pingouin:\n\npg.normality(fishlength_py.length)\n\n               W      pval  normal\nlength  0.949384  0.176423    True\n\n\n\nthe W column gives us the W-statistic\nthe pval column gives us the p-value\nthe normal column gives us the outcome of the test in True/False\n\n\n\n\nAs the p-value is bigger than 0.05 (say) then we can say that there is insufficient evidence to reject the null hypothesis that the sample came from a normal distribution.\nIt is important to recognise that the Shapiro-Wilk test is not without limitations. It is rather sensitive to the sample size being considered. In general, for small sample sizes, the test is very relaxed about normality (and nearly all data sets are considered normal), whereas for large sample sizes the test can be overly strict, and it can fail to recognise data sets that are very nearly normal indeed.\n\n\n4.6.4 Assumptions overview\n\n\n\n\n\n\nImportant\n\n\n\nIn terms of assessing the assumptions of a test it is always worth considering several methods, both graphical and analytic, and not just relying on a single method.\n\n\nIn the fishlength example, the graphical Q-Q plot analysis was not especially conclusive as there was some suggestion of snaking in the plots, but the Shapiro-Wilk test gave a non-significant p-value (0.1764). Putting these two together, along with the original histogram and the recognition that there were only 30 data points in the data set I personally would be happy that the assumptions of the t-test were met well enough to trust the result of the t-test, but you may not be…\nIn which case we would consider an alternative test that has less stringent assumptions (but is less powerful): the one-sample Wilcoxon signed-rank test.",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#implement-and-interpret-the-test",
    "href": "materials/cs1_practical_one-sample.html#implement-and-interpret-the-test",
    "title": "4  One-sample data",
    "section": "4.7 Implement and interpret the test",
    "text": "4.7 Implement and interpret the test\nPerform a one-sample, two-tailed t-test:\n\nRPython\n\n\n\nt_test(length ~ 1,\n       mu = 20, alternative = \"two.sided\",\n       data = fishlengthDF)\n\n# A tibble: 1 × 7\n  .y.    group1 group2         n statistic    df       p\n* &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 length 1      null model    29     -3.55    28 0.00139\n\n\n\nThe first argument must be a numerical column of data values. In our case it’s the length values. We add ~ 1 to indicate that we’re going to compare these to a “null model”. More on this later, but for now it’s just a quirky syntax thing in R.\nThe mu argument must be a number and is the mean to be tested under the null hypothesis.\nThe alternative argument gives the type of alternative hypothesis and must be one of two.sided, greater or less. We have no prior assumptions on whether the alternative fish length would be greater or less than 20, so we choose two.sided.\n\nIn the output:\n\n.y. is the variable of interest\nn is the number of observations\nignore the group columns for now\nstatistic is the calculated t-value (-3.549184)\ndf gives us the number of degrees of freedom (28). Again, more on this later.\np gives us the p-value of 0.00139\n\n\n\n\npg.ttest(x = fishlength_py.length,\n         y = 20,\n         alternative = \"two-sided\").round(3)\n\n            T  dof alternative  p-val           CI95%  cohen-d    BF10  power\nT-test -3.549   28   two-sided  0.001  [17.31, 19.28]    0.659  25.071  0.929\n\n\n\nthe x argument must be a numerical series of data values\nthe y argument must be a number and is the mean to be tested under the null hypothesis\nthe alternative argument defines the alternative hypothesis (we have no expectation that the fish length is smaller or larger than 20, if the null hypothesis does not hold up)\nwith .round(3) we’re rounding the outcome to 3 digits\n\nIn the output:\nWe’re not focussing on all of the output just yet, but\n\nT gives us the value of the t-statistic\ndof gives us the degrees of freedom (we’ll need this for reporting)\npval gives us the p-value\n\n\n\n\nThe p-value is what we’re mostly interested in. It gives the probability of us getting a sample such as ours if the null hypothesis were actually true.\n\n\n\n\n\n\nImportantp-value interpretation\n\n\n\n\na high p-value means that there is a high probability of observing a sample such as ours and the null hypothesis is probably true whereas\na low p-value means that there is a low probability of observing a sample such as ours and the null hypothesis is probably not true.\n\nIt is important to realise that the p-value is just an indication and there is no absolute certainty here in this interpretation.\nPeople, however like more definite answers and so we pick an artificial probability threshold (called a significance level) in order to be able to say something more decisive. The standard significance level is 0.05 and since our p-value is smaller than this we choose to say that “it is very unlikely that we would have this particular sample if the null hypothesis were true”.\n\n\nIn this case the p-value is much smaller than 0.05, so we reject our null hypothesis and state that:\n\nA one-sample t-test indicated that the mean body length of male guppies (\\(\\bar{x}\\) = 18.29mm) differs significantly from 20 mm (p = 0.0014).\n\nThe above sentence is an adequate concluding statement for this test and is what we would write in any paper or report. Note that we have included (in brackets) information on the actual mean value of our group(\\(\\bar{x}\\) = 18.29mm) and the p-value (p = 0.0014). In some journals you are only required to report whether the p-value is less than the critical value (e.g. p &lt; 0.05) but I would always recommend reporting the actual p-value obtained.\n\n\n\n\n\n\nImportant\n\n\n\nAdditional information, such as the test statistic and degrees of freedom, are sometimes also reported. This is more of a legacy from the time where people did the calculations by hand and used tables. I personally find it much more useful to report as above and supply the data and analysis, so other people can see what I’ve done and why!",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#cs1-one-sample-non-normal",
    "href": "materials/cs1_practical_one-sample.html#cs1-one-sample-non-normal",
    "title": "4  One-sample data",
    "section": "4.8 Dealing with non-normal data",
    "text": "4.8 Dealing with non-normal data\nYour data might not always be normally distributed. That’s not a huge issue and there are statistical tests that can deal with this. For a one-sample data set there is the Wilcoxon signed rank test. This test, in contrast to the one-sample t-test does not assume that the parent distribution is normally distributed. We do still need the parent distribution (and consequently the sample) to be the same shape and scale.\nThe Wilcoxon signed rank test checks if the rank-transformed values are symmetric around the median. As such, using this test we look to see if the median of the parent distributions differs significantly from a given hypothesised value (in contrast with the t-test that looks at the mean).\n\n4.8.1 Data and hypotheses\nAgain, we use the fishlength data set. The one-sample Wilcoxon signed rank test allows to see if the median body length is different from a specified value. Here we want to test whether the data support the hypothesis that the median body is actually 20 mm. The following null and alternative hypotheses are very similar to those used for the one sample t-test:\n\n\\(H_0\\): The median body length is equal to 20 mm (\\(\\mu =\\) 20).\n\\(H_1\\): The median body length is not equal to 20 mm (\\(\\mu \\neq\\) 20).\n\nWe will use a one-sample, two-tailed Wilcoxon signed rank test to see if we should reject the null hypothesis or not.\n\n\n4.8.2 Summarise and visualise\nWe did this before in the previous section, nothing really should have changed between now and then (if it has then you’re not off to a good start on this practical!)\n\n\n4.8.3 Assumptions\nIn order to use a one-sample Wilcoxon signed rank test for this analysis (and for the results to be strictly valid) we have to make two assumptions:\n\nThe data are distributed symmetrically around the median\nEach data point in the sample is independent of the others. This is the same as for the t-test and is a common feature of nearly all statistical tests. Lack of independence in your data is really tough to deal with (if not impossible) and a large part of proper experimental design is ensuring this.\n\nWhilst there are formal statistical tests for symmetry we will opt for a simple visual inspection using both a box plot and a histogram.\nPlot a histogram and a box plot of the data:\n\nRPython\n\n\nLet’s first determine the median, so we can use that to compare our data to. We’ll also store the value in an object called median_fishlength.\n\n# determine the median\nmedian_fishlength &lt;- median(fishlengthDF$length)\n\n\n# create a histogram\nfishlengthDF %&gt;% \n  ggplot(aes(x = length)) +\n  geom_histogram(bins = 10) +\n  geom_vline(xintercept = median_fishlength,\n             colour = \"red\")\n\n\n\n\n\n\n\n# create box plot\nfishlengthDF %&gt;% \n  ggplot(aes(y = length)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nLet’s first determine the median, so we can use that to compare our data to.\n\nmedian_fishlength = fishlength_py.length.median()\n\n\n# create a histogram\np = (ggplot(fishlength_py,\n        aes(x = \"length\")) +\n     geom_histogram(bins = 10) +\n     geom_vline(xintercept = median_fishlength,\n                colour = \"red\"))\n\np.show()\n\n\n\n\n\n\n\n# create box plot\np = (ggplot(fishlength_py,\n        aes(x = 1,\n            y = \"length\")) +\n     geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nHere we can see that whilst the distribution isn’t perfectly symmetric, neither is it heavily skewed to the left or right and we can make the call that the distribution is symmetric enough for us to be happy with the results of the test.\n\n\n4.8.4 Implement and interpret the test\nPerform a one-sample, two-tailed Wilcoxon signed rank test:\n\nRPython\n\n\n\nwilcox_test(length ~ 1,\n            mu = 20, alternative = \"two.sided\",\n            data = fishlengthDF)\n\n# A tibble: 1 × 6\n  .y.    group1 group2         n statistic       p\n* &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 length 1      null model    29      67.5 0.00122\n\n\nThe syntax is identical to the one-sample t-test we carried out earlier.\n\nThe first argument contains the column with data values.\nThe mu argument must be a number and is the median to be tested under the null hypothesis.\nThe alternative argument gives the type of alternative hypothesis and must be one of two.sided, greater or less.\n\n\n\n\npg.wilcoxon(fishlength_py.length - 20,\n            alternative = \"two-sided\")\n\n          W-val alternative     p-val       RBC  CLES\nWilcoxon   67.5   two-sided  0.001222 -0.689655   NaN\n\n\nThe syntax is similar to what we did earlier:\n\nThe 1st argument we give to the wilcoxon() function is an array of the differences between our data points and the median to be tested under the null hypothesis, i.e. our data points (fishlength_py.length) minus the test median (20, in this case).\nThe 2nd argument gives us the type of alternative hypothesis and must be one of “two-sided”, “larger”, or “smaller”.\n\n\n\n\nAgain, the p-value is what we’re most interested in. It gives the probability of us getting a sample such as ours if the null hypothesis were actually true. So, in this case since our p-value is less than 0.05 we can reject our null hypothesis and state that:\n\nA one-sample Wilcoxon signed rank test indicated that the median body length of male guppies (\\(\\tilde{x}\\) = 18.8 mm) differs significantly from 20 mm (p = 0.0012).\n\nThe above sentence is an adequate concluding statement for this test and is what we would write in any paper or report. Note that we have included (in brackets) information on the median value of the group (\\(\\tilde{x}\\) = 18.8 mm) and the p-value (p = 0.0012). Keep in mind that, when publishing, you’d also submit your data and scripts, so people can follow your analysis.",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#exercises",
    "href": "materials/cs1_practical_one-sample.html#exercises",
    "title": "4  One-sample data",
    "section": "4.9 Exercises",
    "text": "4.9 Exercises\n\n4.9.1 Gastric juices\n\n\n\n\n\n\nExerciseExercise 1\n\n\n\n\n\n\nLevel: \nThe following data are the dissolving times (in seconds) of a drug in agitated gastric juice:\n42.7, 43.4, 44.6, 45.1, 45.6, 45.9, 46.8, 47.6\nThese data are stored in data/CS1-gastric_juices.csv.\nDo these results provide any evidence to suggest that dissolving time for this drug is different from 45 seconds?\n\nHere the data are already formatted for your convenience into a tidy format\nLoad the data from data/CS1-gastric_juices.csv\nWrite down the null and alternative hypotheses.\nSummarise and visualise the data and perform an appropriate one-sample t-test.\n\nWhat can you say about the dissolving time? (what sentence would you use to report this)\n\nCheck the assumptions for the test.\n\nWas the test valid?\n\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n4.10 Answer\n\nHypotheses\n\\(H_0\\) : mean \\(=\\) 45s\n\\(H_1\\) : mean \\(\\neq\\) 45s\n\n\nData, summarise & visualise\nWe read in the data from CS1-gastric_juices.csv. It contains two columns, an id column and a dissolving_time column with the measured values.\n\nRPython\n\n\n\n# load the data\ndissolving &lt;- read_csv(\"data/CS1-gastric_juices.csv\")\n\n# have a look at the data\ndissolving\n\n# A tibble: 8 × 2\n     id dissolving_time\n  &lt;dbl&gt;           &lt;dbl&gt;\n1     1            42.7\n2     2            43.4\n3     3            44.6\n4     4            45.1\n5     5            45.6\n6     6            45.9\n7     7            46.8\n8     8            47.6\n\n# summarise the data\nsummary(dissolving)\n\n       id       dissolving_time\n Min.   :1.00   Min.   :42.70  \n 1st Qu.:2.75   1st Qu.:44.30  \n Median :4.50   Median :45.35  \n Mean   :4.50   Mean   :45.21  \n 3rd Qu.:6.25   3rd Qu.:46.12  \n Max.   :8.00   Max.   :47.60  \n\n\nWe can look at the histogram and box plot of the data:\n\n# create a histogram\nggplot(dissolving,\n       aes(x = dissolving_time)) +\n  geom_histogram(bins = 4)\n\n\n\n\n\n\n\n# create a boxplot\nggplot(dissolving,\n       aes(y = dissolving_time)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n# load the data\ndissolving_py = pd.read_csv(\"data/CS1-gastric_juices.csv\")\n\n# have a look at the data\ndissolving_py.head()\n\n   id  dissolving_time\n0   1             42.7\n1   2             43.4\n2   3             44.6\n3   4             45.1\n4   5             45.6\n\n# summarise the data\ndissolving_py.describe()\n\n            id  dissolving_time\ncount  8.00000         8.000000\nmean   4.50000        45.212500\nstd    2.44949         1.640068\nmin    1.00000        42.700000\n25%    2.75000        44.300000\n50%    4.50000        45.350000\n75%    6.25000        46.125000\nmax    8.00000        47.600000\n\n\nWe can look at the histogram and box plot of the data:\n\n# create a histogram\np = (ggplot(dissolving_py,\n        aes(x = \"dissolving_time\")) +\n     geom_histogram(bins = 4))\n\np.show()\n\n\n\n\n\n\n\n\n\n# create a box plot\np = (ggplot(dissolving_py,\n        aes(x = 1,\n            y = \"dissolving_time\")) +\n     geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\nPython (or plotnine in particular) gets a bit cranky if you try to create a geom_boxplot but do not define the x aesthetic. Hence us putting it as 1. The value has no numerical meaning, however.\n\n\n\nThere are only 8 data points, so the histogram is rather uninformative. Thankfully the box plot is a bit more useful here. We can see:\n\nThere don’t appear to be any major errors in data entry and there aren’t any huge outliers\nThe median value in the box-plot (the thick black line) is pretty close to 45 and so I wouldn’t be surprised if the mean of the data isn’t significantly different from 45. We can confirm that by looking at the mean and median values that we calculated using the summary command from earlier.\nThe data appear to be symmetric, and so whilst we can’t tell if they’re normal they’re a least not massively skewed.\n\n\n\nAssumptions\nNormality:\n\nRPython\n\n\n\n# perform Shapiro-Wilk test\nshapiro.test(dissolving$dissolving_time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dissolving$dissolving_time\nW = 0.98023, p-value = 0.9641\n\n\n\n# create a Q-Q plot\nggplot(dissolving,\n       aes(sample = dissolving_time)) +\n  stat_qq() +\n  stat_qq_line(colour = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n# Perform Shapiro-Wilk test to check normality\npg.normality(dissolving_py.dissolving_time)\n\n                        W      pval  normal\ndissolving_time  0.980234  0.964054    True\n\n\n\n# Create a Q-Q plot\np = (ggplot(dissolving_py,\n        aes(sample = \"dissolving_time\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"blue\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe Shapiro test has a p-value of 0.964 which (given that it is bigger than 0.05) suggests that the data are normal enough.\nThe Q-Q plot isn’t perfect, with some deviation of the points away from the line but since the points aren’t accelerating away from the line and, since we only have 8 points, we can claim, with some slight reservations, that the assumption of normality appears to be adequately well met.\n\nOverall, we are somewhat confident that the assumption of normality is well-enough met for the t-test to be an appropriate method for analysing the data. Note the ridiculous number of caveats here and the slightly political/slippery language I’m using. This is intentional and reflects the ambiguous nature of assumption checking. This is an important approach to doing statistics that you need to embrace.\nIn reality, if I found myself in this situation I would also try doing a non-parametric test on the data (Wilcoxon signed-rank test) and see whether I get the same conclusion about whether the median dissolving time differs from 45s. Technically, you don’t know about the Wilcoxon test yet as you haven’t done that section of the materials. Anyway, if I get the same conclusion then my confidence in the result of the test goes up considerably; it doesn’t matter how well an assumption has been met, I get the same result. If on the other hand I get a completely different conclusion from carrying out the non-parametric test then all bets are off; I now have very little confidence in my test result as I don’t know which one to believe (in the case that the assumptions of the test are a bit unclear). In this example a Wilcoxon test also gives us a non-significant result and so all is good.\n\n\nImplement test\n\nRPython\n\n\n\n# perform one-sample t-test\ndissolving\n\n# A tibble: 8 × 2\n     id dissolving_time\n  &lt;dbl&gt;           &lt;dbl&gt;\n1     1            42.7\n2     2            43.4\n3     3            44.6\n4     4            45.1\n5     5            45.6\n6     6            45.9\n7     7            46.8\n8     8            47.6\n\nt_test(dissolving_time ~ 1,\n       mu = 45, alternative = \"two.sided\",\n       data = dissolving)\n\n# A tibble: 1 × 7\n  .y.             group1 group2         n statistic    df     p\n* &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 dissolving_time 1      null model     8     0.366     7 0.725\n\n\n\n\n\npg.ttest(x = dissolving_py.dissolving_time,\n         y = 45,\n         alternative = \"two-sided\").round(3)\n\n            T  dof alternative  p-val           CI95%  cohen-d   BF10  power\nT-test  0.366    7   two-sided  0.725  [43.84, 46.58]     0.13  0.356  0.062\n\n\n\n\n\n\nA one-sample t-test indicated that the mean dissolving time of the drug is not significantly different from 45s (\\(\\bar{x}\\) = 45.2, p = 0.725).\n\nAnd that, is that.\n\n\n\n\n\n\n\n\n\n\n\n4.10.1 Gastric juices (revisited)\n\n\n\n\n\n\nExerciseExercise 2\n\n\n\n\n\n\nLevel: \nWhat if we were unsure if we could assume normality here? In that case we’d have to perform a Wilcoxon signed rank test.\n\nAnalyse the drug data set from before using a one-sample Wilcoxon signed rank test\nDiscuss with a (virtual) neighbour which of the two tests you feel is best suited to the data.\nDoes it matter in this case?\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\n\nHypotheses\n\\(H_0\\) : median \\(=\\) 45s\n\\(H_1\\) : median \\(\\neq\\) 45s\n\n\nAssumptions\nFrom the box plot from the previous exercise we already know that the data are symmetric enough for the test to be valid.\n\n\nWilcoxon signed rank test\nCalculating the median and performing the test:\n\nRPython\n\n\n\nmedian(dissolving$dissolving_time)\n\n[1] 45.35\n\n\n\nwilcox_test(dissolving_time ~ 1,\n            mu = 45, alternative = \"two.sided\",\n            data = dissolving)\n\n# A tibble: 1 × 6\n  .y.             group1 group2         n statistic     p\n* &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 dissolving_time 1      null model     8        22 0.641\n\n\n\n\n\ndissolving_py.dissolving_time.median()\n\nnp.float64(45.35)\n\n\n\npg.wilcoxon(dissolving_py.dissolving_time - 45,\n            alternative = \"two-sided\")\n\n          W-val alternative     p-val       RBC  CLES\nWilcoxon   14.0   two-sided  0.640625  0.222222   NaN\n\n\n\n\n\n\nA one-sample Wilcoxon-signed rank test indicated that the median dissolving time of the drug is not significantly different from 45 s (\\(\\tilde{x}\\) = 45.35 , p = 0.64)\n\n\n\nDiscussion\nIn terms of choosing between the two test we can see that both meet their respective assumptions and so both tests are valid. In this case both tests also agree in terms of their conclusions i.e. that the average dissolving time (either mean or median) doesn’t differ significantly from the proposed value of 45 s.\n\nSo one answer would be that it doesn’t matter which test you use.\nAnother answer would be that you should pick the test that measures the quantity you’re interested in i.e. if you care about medians then use the Wilcoxon test, whereas if you care about means then use the t-test.\nA final answer would be that, since both test are valid we would prefer to use the test with greater power. t-tests always have more power than Wilcoxon tests (as long as they’re valid) and so we could report that one. (We’ll talk about this in the last session but power is effectively the capacity of a test to detect a significant difference - so more power is better).",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#answer",
    "href": "materials/cs1_practical_one-sample.html#answer",
    "title": "4  One-sample data",
    "section": "4.10 Answer",
    "text": "4.10 Answer\n\nHypotheses\n\\(H_0\\) : mean \\(=\\) 45s\n\\(H_1\\) : mean \\(\\neq\\) 45s\n\n\nData, summarise & visualise\nWe read in the data from CS1-gastric_juices.csv. It contains two columns, an id column and a dissolving_time column with the measured values.\n\nRPython\n\n\n\n# load the data\ndissolving &lt;- read_csv(\"data/CS1-gastric_juices.csv\")\n\n# have a look at the data\ndissolving\n\n# A tibble: 8 × 2\n     id dissolving_time\n  &lt;dbl&gt;           &lt;dbl&gt;\n1     1            42.7\n2     2            43.4\n3     3            44.6\n4     4            45.1\n5     5            45.6\n6     6            45.9\n7     7            46.8\n8     8            47.6\n\n# summarise the data\nsummary(dissolving)\n\n       id       dissolving_time\n Min.   :1.00   Min.   :42.70  \n 1st Qu.:2.75   1st Qu.:44.30  \n Median :4.50   Median :45.35  \n Mean   :4.50   Mean   :45.21  \n 3rd Qu.:6.25   3rd Qu.:46.12  \n Max.   :8.00   Max.   :47.60  \n\n\nWe can look at the histogram and box plot of the data:\n\n# create a histogram\nggplot(dissolving,\n       aes(x = dissolving_time)) +\n  geom_histogram(bins = 4)\n\n\n\n\n\n\n\n# create a boxplot\nggplot(dissolving,\n       aes(y = dissolving_time)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n# load the data\ndissolving_py = pd.read_csv(\"data/CS1-gastric_juices.csv\")\n\n# have a look at the data\ndissolving_py.head()\n\n   id  dissolving_time\n0   1             42.7\n1   2             43.4\n2   3             44.6\n3   4             45.1\n4   5             45.6\n\n# summarise the data\ndissolving_py.describe()\n\n            id  dissolving_time\ncount  8.00000         8.000000\nmean   4.50000        45.212500\nstd    2.44949         1.640068\nmin    1.00000        42.700000\n25%    2.75000        44.300000\n50%    4.50000        45.350000\n75%    6.25000        46.125000\nmax    8.00000        47.600000\n\n\nWe can look at the histogram and box plot of the data:\n\n# create a histogram\np = (ggplot(dissolving_py,\n        aes(x = \"dissolving_time\")) +\n     geom_histogram(bins = 4))\n\np.show()\n\n\n\n\n\n\n\n\n\n# create a box plot\np = (ggplot(dissolving_py,\n        aes(x = 1,\n            y = \"dissolving_time\")) +\n     geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\nPython (or plotnine in particular) gets a bit cranky if you try to create a geom_boxplot but do not define the x aesthetic. Hence us putting it as 1. The value has no numerical meaning, however.\n\n\n\nThere are only 8 data points, so the histogram is rather uninformative. Thankfully the box plot is a bit more useful here. We can see:\n\nThere don’t appear to be any major errors in data entry and there aren’t any huge outliers\nThe median value in the box-plot (the thick black line) is pretty close to 45 and so I wouldn’t be surprised if the mean of the data isn’t significantly different from 45. We can confirm that by looking at the mean and median values that we calculated using the summary command from earlier.\nThe data appear to be symmetric, and so whilst we can’t tell if they’re normal they’re a least not massively skewed.\n\n\n\nAssumptions\nNormality:\n\nRPython\n\n\n\n# perform Shapiro-Wilk test\nshapiro.test(dissolving$dissolving_time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dissolving$dissolving_time\nW = 0.98023, p-value = 0.9641\n\n\n\n# create a Q-Q plot\nggplot(dissolving,\n       aes(sample = dissolving_time)) +\n  stat_qq() +\n  stat_qq_line(colour = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n# Perform Shapiro-Wilk test to check normality\npg.normality(dissolving_py.dissolving_time)\n\n                        W      pval  normal\ndissolving_time  0.980234  0.964054    True\n\n\n\n# Create a Q-Q plot\np = (ggplot(dissolving_py,\n        aes(sample = \"dissolving_time\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"blue\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe Shapiro test has a p-value of 0.964 which (given that it is bigger than 0.05) suggests that the data are normal enough.\nThe Q-Q plot isn’t perfect, with some deviation of the points away from the line but since the points aren’t accelerating away from the line and, since we only have 8 points, we can claim, with some slight reservations, that the assumption of normality appears to be adequately well met.\n\nOverall, we are somewhat confident that the assumption of normality is well-enough met for the t-test to be an appropriate method for analysing the data. Note the ridiculous number of caveats here and the slightly political/slippery language I’m using. This is intentional and reflects the ambiguous nature of assumption checking. This is an important approach to doing statistics that you need to embrace.\nIn reality, if I found myself in this situation I would also try doing a non-parametric test on the data (Wilcoxon signed-rank test) and see whether I get the same conclusion about whether the median dissolving time differs from 45s. Technically, you don’t know about the Wilcoxon test yet as you haven’t done that section of the materials. Anyway, if I get the same conclusion then my confidence in the result of the test goes up considerably; it doesn’t matter how well an assumption has been met, I get the same result. If on the other hand I get a completely different conclusion from carrying out the non-parametric test then all bets are off; I now have very little confidence in my test result as I don’t know which one to believe (in the case that the assumptions of the test are a bit unclear). In this example a Wilcoxon test also gives us a non-significant result and so all is good.\n\n\nImplement test\n\nRPython\n\n\n\n# perform one-sample t-test\ndissolving\n\n# A tibble: 8 × 2\n     id dissolving_time\n  &lt;dbl&gt;           &lt;dbl&gt;\n1     1            42.7\n2     2            43.4\n3     3            44.6\n4     4            45.1\n5     5            45.6\n6     6            45.9\n7     7            46.8\n8     8            47.6\n\nt_test(dissolving_time ~ 1,\n       mu = 45, alternative = \"two.sided\",\n       data = dissolving)\n\n# A tibble: 1 × 7\n  .y.             group1 group2         n statistic    df     p\n* &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 dissolving_time 1      null model     8     0.366     7 0.725\n\n\n\n\n\npg.ttest(x = dissolving_py.dissolving_time,\n         y = 45,\n         alternative = \"two-sided\").round(3)\n\n            T  dof alternative  p-val           CI95%  cohen-d   BF10  power\nT-test  0.366    7   two-sided  0.725  [43.84, 46.58]     0.13  0.356  0.062\n\n\n\n\n\n\nA one-sample t-test indicated that the mean dissolving time of the drug is not significantly different from 45s (\\(\\bar{x}\\) = 45.2, p = 0.725).\n\nAnd that, is that.",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#summary",
    "href": "materials/cs1_practical_one-sample.html#summary",
    "title": "4  One-sample data",
    "section": "4.11 Summary",
    "text": "4.11 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nOne-sample tests are used when you have a single sample of continuous data\nThe t-test assumes that the data are normally distributed and independent of each other\nA good way of assessing the assumption of normality is by checking the data against a Q-Q plot\nThe Wilcoxon signed rank test is used when you have a single sample of continuous data, which is not normally distributed",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html",
    "href": "materials/cs1_practical_two-samples.html",
    "title": "5  Two-sample data",
    "section": "",
    "text": "5.1 Libraries and functions",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#libraries-and-functions",
    "href": "materials/cs1_practical_two-samples.html#libraries-and-functions",
    "title": "5  Two-sample data",
    "section": "",
    "text": "NoteClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n5.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n\n\n5.1.2 Functions\n\n# Computes summary statistics                         \nrstatix::get_summary_stats() \n\n# Performs Levene's test for equality of variance\n# (non-normally distributed data)\nrstatix::levene_test()\n\n# Performs Bartlett's test for equality of variance\n# (normally distributed data)\nstats::bartlett.test()\n\n# Performs Shapiro Wilk test\nstats::shapiro.test()\n\n# Performs one- and two-sample Wilcoxon tests\n# the latter is also known as 'Mann-Whitney U' test\nrstatix::wilcox_test()\n\n# Plots a Q-Q plot for comparison with a normal distribution\nggplot2::stat_qq()\n\n# Adds a comparison line to the Q-Q plot\nggplot2::stat_qq_line()\n\n\n\n\n\n5.1.3 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n\n\n5.1.4 Functions\n\n# Summary statistics\npandas.DataFrame.describe()\n\n# Plots the first few rows of a DataFrame\npandas.DataFrame.head()\n\n# Query the columns of a DataFrame with a boolean expression\npandas.DataFrame.query()\n\n# Reads in a .csv file\npandas.DataFrame.read_csv()\n\n# Performs an analysis of variance\npingouin.anova()\n\n# Tests for equality of variance\npingouin.homoscedasticity()\n\n# Performs a Mann-Whitney U test\npingouin.mwu()\n\n# Performs the Shapiro-Wilk test for normality\npingouin.normality()\n\n# Performs a t-test\npingouin.ttest()\n\n# Plots a Q-Q plot for comparison with a normal distribution\nplotnine.stats.stat_qq()\n\n# Adds a comparison line to the Q-Q plot\nplotnine.stats.stat_qq_line()",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#purpose-and-aim",
    "href": "materials/cs1_practical_two-samples.html#purpose-and-aim",
    "title": "5  Two-sample data",
    "section": "5.2 Purpose and aim",
    "text": "5.2 Purpose and aim\nThese two-sample Student’s t-test is used when we have two samples of continuous data where we are trying to find out if the samples came from the same parent distribution or not. This essentially boils down to finding out if there is a difference in the means of the two samples.",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#data-and-hypotheses",
    "href": "materials/cs1_practical_two-samples.html#data-and-hypotheses",
    "title": "5  Two-sample data",
    "section": "5.3 Data and hypotheses",
    "text": "5.3 Data and hypotheses\nFor example, suppose we now measure the body lengths of male guppies (in mm) collected from two rivers in Trinidad; the Aripo and the Guanapo. We want to test whether the mean body length differs between samples. We form the following null and alternative hypotheses:\n\n\\(H_0\\): The mean body length does not differ between the two groups \\((\\mu A = \\mu G)\\)\n\\(H_1\\): The mean body length does differ between the two groups \\((\\mu A \\neq \\mu G)\\)\n\nWe use a two-sample, two-tailed t-test to see if we can reject the null hypothesis.\n\nWe use a two-sample test because we now have two samples.\nWe use a two-tailed t-test because we want to know if our data suggest that the true (population) means are different from one another rather than that one mean is specifically bigger or smaller than the other.\nWe’re using Student’s t-test because the sample sizes are big and because we’re assuming that the parent populations have equal variance (We can check this later).\n\nThe data are stored in the file data/CS1-twosample.csv.\nLet’s read in the data and have a quick look at the first rows to see how the data is structured.\nMake sure you have downloaded the data and placed it within your working directory.\n\nRPython\n\n\nFirst we load the relevant libraries:\n\n# load tidyverse\nlibrary(tidyverse)\n\n# load rstatix, a tidyverse-friendly stats package\nlibrary(rstatix)\n\nWe then read in the data and create a table containing the data.\n\nrivers &lt;- read_csv(\"data/CS1-twosample.csv\")\n\nrivers\n\n# A tibble: 68 × 2\n   river   length\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 Guanapo   19.1\n 2 Guanapo   23.3\n 3 Guanapo   18.2\n 4 Guanapo   16.4\n 5 Guanapo   19.7\n 6 Guanapo   16.6\n 7 Guanapo   17.5\n 8 Guanapo   19.9\n 9 Guanapo   19.1\n10 Guanapo   18.8\n# ℹ 58 more rows\n\n\n\n\n\nrivers_py = pd.read_csv(\"data/CS1-twosample.csv\")\n\nrivers_py.head()\n\n     river  length\n0  Guanapo    19.1\n1  Guanapo    23.3\n2  Guanapo    18.2\n3  Guanapo    16.4\n4  Guanapo    19.7",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#cs1-students-sumvisual",
    "href": "materials/cs1_practical_two-samples.html#cs1-students-sumvisual",
    "title": "5  Two-sample data",
    "section": "5.4 Summarise and visualise",
    "text": "5.4 Summarise and visualise\nLet’s first summarise the data.\n\nRPython\n\n\n\nsummary(rivers)\n\n    river               length     \n Length:68          Min.   :11.20  \n Class :character   1st Qu.:18.40  \n Mode  :character   Median :19.30  \n                    Mean   :19.46  \n                    3rd Qu.:20.93  \n                    Max.   :26.40  \n\n\nThis gives us the standard summary statistics, but in this case we have more than one group (Aripo and Guanapo), so it might be helpful to get summary statistics per group. One way of doing this is by using the get_summary_stats() function from the rstatix library.\n\n# get common summary stats for the length column\nrivers %&gt;% \n  group_by(river) %&gt;% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 2 × 11\n  river   variable     n   min   max median   iqr  mean    sd    se    ci\n  &lt;chr&gt;   &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Aripo   length      39  17.5  26.4   20.1   2.2  20.3  1.78 0.285 0.577\n2 Guanapo length      29  11.2  23.3   18.8   2.2  18.3  2.58 0.48  0.983\n\n\nNumbers might not always give you the best insight into your data, so we also visualise our data:\n\nggplot(rivers,\n       aes(x = river, y = length)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nrivers_py.describe()\n\n          length\ncount  68.000000\nmean   19.463235\nstd     2.370081\nmin    11.200000\n25%    18.400000\n50%    19.300000\n75%    20.925000\nmax    26.400000\n\n\nThis gives us the standard summary statistics, but in this case we have more than one group (Aripo and Guanapo), so it might be helpful to get summary statistics per group. Here we use the pd.groupby() function to group by river. We only want to have summary statistics for the length variable, so we specify that as well:\n\nrivers_py.groupby(\"river\")[\"length\"].describe()\n\n         count       mean       std   min   25%   50%   75%   max\nriver                                                            \nAripo     39.0  20.330769  1.780620  17.5  19.1  20.1  21.3  26.4\nGuanapo   29.0  18.296552  2.584636  11.2  17.5  18.8  19.7  23.3\n\n\nNumbers might not always give you the best insight into your data, so we also visualise our data:\n\np = (ggplot(rivers_py,\n        aes(x = \"river\", y = \"length\")) + \n     geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nThe box plot does appear to suggest that the two samples have different means, and moreover that the guppies in Guanapo may be smaller than the guppies in Aripo. It isn’t immediately obvious that the two populations don’t have equal variances though (box plots are not quite the right tool for this), so we plough on. Who ever said statistics would be glamorous?",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#assumptions",
    "href": "materials/cs1_practical_two-samples.html#assumptions",
    "title": "5  Two-sample data",
    "section": "5.5 Assumptions",
    "text": "5.5 Assumptions\nIn order to use a Student’s t-test (and for the results to be strictly valid) we have to make three assumptions:\n\nThe parent distributions from which the samples are taken are both normally distributed (which would lead to the sample data being normally distributed too).\nEach data point in the samples is independent of the others.\nThe parent distributions should have the same variance.\n\nIn this example the first assumption can be ignored as the sample sizes are large enough (because of maths, with Aripo containing 39 and Guanapo 29 samples). If the samples were smaller then we would use the tests from the previous section.\nThe second point we can do nothing about unless we know how the data were collected, so again we ignore it.\nThe third point regarding equality of variance can be tested using either Bartlett’s test (if the samples are normally distributed) or Levene’s test (if the samples are not normally distributed).\nThis is where it gets a bit trickier. Although we don’t care if the samples are normally distributed for the t-test to be valid (because the sample size is big enough to compensate), we do need to know if they are normally distributed in order to decide which variance test to use.\nSo we perform a Shapiro-Wilk test on both samples separately.\n\nRPython\n\n\nWe can use the filter() function to filter the data by river, then we perform the Shapiro-Wilk test on the length measurement. The shapiro.test() function needs the data in a vector format. We get these by using the pull() function.\n\n\n\n\n\n\nTip\n\n\n\nIt’s good practice to check what kind of data is going into these functions. Run the code line-by-line to see what data is passed on from the filter() and pull() functions.\n\n\n\n# filter data by river and perform test\nrivers %&gt;% \n    filter(river == \"Aripo\") %&gt;% \n    pull(length) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.93596, p-value = 0.02802\n\nrivers %&gt;% \n    filter(river == \"Guanapo\") %&gt;% \n    pull(length) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.94938, p-value = 0.1764\n\n\n\n\nTo perform a Shapiro-Wilk test we can use the normality() function from pingouin. We can give it the data in the original ‘long’ format, where we specify:\n\ndv = dependent variable, length\ngroup = grouping variable, river\ndata = data frame\n\n\npg.normality(dv = \"length\",\n             group = \"river\",\n             data = rivers_py)\n\n                W      pval  normal\nriver                              \nGuanapo  0.949384  0.176423    True\nAripo    0.935958  0.028023   False\n\n\n\n\n\nWe can see that whilst the Guanapo data is probably normally distributed (p = 0.1764 &gt; 0.05), the Aripo data is unlikely to be normally distributed (p = 0.02802 &lt; 0.05). Remember that the p-value gives the probability of observing each sample if the parent population is actually normally distributed.\nThe Shapiro-Wilk test is quite sensitive to sample size. This means that if you have a large sample then even small deviations from normality will cause the sample to fail the test, whereas smaller samples are allowed to pass with much larger deviations. Here the Aripo data has nearly 40 points in it compared with the Guanapo data and so it is much easier for the Aripo sample to fail compared with the Guanapo data.\n\n\n\n\n\n\nImportant\n\n\n\nComplete Exercise 5.9.1.\n\n\nThe Q-Q plots show the opposite of what we found with the Shapiro-Wilk tests: the data for Aripo look pretty normally distributed apart from one data point, whereas the assumption of normality for the Guanapo data is less certain.\nWhat to do? Well, you could be conservative and state that you are not confident that the data in either group are normally distributed. That would be a perfectly reasonable conclusion.\nI would personally not have issues with stating that the Aripo data are probably normally distributed enough.",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#equality-of-variance",
    "href": "materials/cs1_practical_two-samples.html#equality-of-variance",
    "title": "5  Two-sample data",
    "section": "5.6 Equality of variance",
    "text": "5.6 Equality of variance\n\n\n\n\n\n\nTip\n\n\n\nRemember that statistical tests do not provide answers, they merely suggest patterns. Human interpretation is still a crucial aspect to what we do.\n\n\nThe reason why we’re checking for equality of variance (also referred to as homogeneity of variance) is because many statistical tests assume that the spread of the data within different parental populations (in this case, two) is the same.\nIf that is indeed the case, then the data themselves should have equal spread as well.\nThe Shapiro-Wilk test and the Q-Q plots have shown that some of the data might not be normal enough (although in opposite directions!) and so in order to test for equality of variance we will use Levene’s test.\n\nRPython\n\n\nThe function we use is levene_test() from the rstatix library.\nIt takes the data in the form of a formula as follows:\n\nlevene_test(data = rivers,\n            formula = length ~ river)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1    66      1.77 0.188\n\n\nOr shortened:\n\nlevene_test(rivers,\n            length ~ river)\n\nThe key bit of information is the p column. This is the p-value 0.1876 for this test.\n\n\nTo test for equality of variance, we can use the homoscedasticity() function from pingouin.\nNote that, contrary to R, we specify the type of test in the method argument. The default is \"levene\", assuming that data are not normally distributed.\n\npg.homoscedasticity(dv = \"length\",\n                    group = \"river\",\n                    method = \"levene\",\n                    data = rivers_py)\n\n               W      pval  equal_var\nlevene  1.773184  0.187569       True\n\n\n\n\n\nThe p-value tells us the probability of observing these two samples if they come from distributions with the same variance. As this probability is greater than our arbitrary significance level of 0.05 then we can be somewhat confident that the necessary assumptions for carrying out Student’s t-test on these two samples was valid. (Once again woohoo!)\n\n5.6.1 Bartlett’s test\nIf we had wanted to carry out Bartlett’s test (i.e. if the data had been sufficiently normally distributed) then we would have done:\n\nRPython\n\n\nHere we use bartlett.test() function.\n\nbartlett.test(length ~ river, data = rivers)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  length by river\nBartlett's K-squared = 4.4734, df = 1, p-value = 0.03443\n\n\nThe relevant p-value is given on the 3rd line.\n\n\n\npg.homoscedasticity(dv = \"length\",\n                    group = \"river\",\n                    method = \"bartlett\",\n                    data = rivers_py)\n\n                 T      pval  equal_var\nbartlett  4.473437  0.034426      False",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#implement-and-interpret-the-test",
    "href": "materials/cs1_practical_two-samples.html#implement-and-interpret-the-test",
    "title": "5  Two-sample data",
    "section": "5.7 Implement and interpret the test",
    "text": "5.7 Implement and interpret the test\nIn this case we’re ignoring the fact that the data are not normal enough, according to the Shapiro-Wilk test. However, this is not entirely naughty, because the sample sizes are pretty large and the t-test is also pretty robust in this case, we can perform a t-test. Remember, this is only allowed because the variances of the two groups (Aripo and Guanapo) are equal.\nPerform a two-sample, two-tailed, t-test:\n\nRPython\n\n\n\n# two-sample, two-tailed t-test\nt_test(length ~ river,\n       alternative = \"two.sided\",\n       var.equal = TRUE,\n       data = rivers)\n\n# A tibble: 1 × 8\n  .y.    group1 group2     n1    n2 statistic    df        p\n* &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 length Aripo  Guanapo    39    29      3.84    66 0.000275\n\n\nHere we do the following:\n\nThe first argument must be in the formula format: response ~ predictor\nThe alternative argument gives the type of alternative hypothesis and must be one of two.sided, greater or less\nThe var.equal argument says whether the variance of the two samples can be assumed to be equal (Student’s t-test) or unequal (Welch’s t-test)\n\nSo, how do we interpret these results?\n\n.y. gives you the name of the response variable, or variable of interest (length in our case)\ngroup1 and group2 gives you the names of the groups being compared\nn1 and n2 gives you the number of observations in each group\nstatistic gives you the value of the statistic (t = 3.8432667)\ndf gives you the number of degrees of freedom (66)\np gives you the p-value (2.75^{-4})\n\n\n\nThe ttest() function in pingouin needs two vectors as input, so we split the data as follows:\n\naripo = rivers_py.query('river == \"Aripo\"')[\"length\"]\nguanapo = rivers_py.query('river == \"Guanapo\"')[\"length\"]\n\nNext, we perform the t-test. We specify that the variance are equal by setting correction = False. We also transpose() the data, so we can actually see the entire output.\n\npg.ttest(aripo, guanapo,\n         correction = False).transpose()\n\n                   T-test\nT                3.843267\ndof                    66\nalternative     two-sided\np-val            0.000275\nCI95%        [0.98, 3.09]\ncohen-d          0.942375\nBF10               92.191\npower            0.966135\n\n\n\n\n\nAgain, the p-value is what we’re most interested in. Since the p-value is very small (much smaller than the standard significance level) we choose to say “that it is very unlikely that these two samples came from the same parent distribution and as such we can reject our null hypothesis” and state that:\n\nA Student’s t-test indicated that the mean body length of male guppies in the Guanapo river (\\(\\bar{x}\\) = 18.29 mm) differs significantly from the mean body length of male guppies in the Aripo river (\\(\\bar{x}\\) = 20.33 mm, p = 0.0003).\n\nNow there’s a conversation starter.\n\n\n\n\n\n\nImportant\n\n\n\nComplete Exercise 5.9.2.",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#dealing-with-non-normal-data",
    "href": "materials/cs1_practical_two-samples.html#dealing-with-non-normal-data",
    "title": "5  Two-sample data",
    "section": "5.8 Dealing with non-normal data",
    "text": "5.8 Dealing with non-normal data\nIf we’re not sure that the data we are dealing with may come from a parent distribution that is normal, then we can’t use a Student’s t-test. Instead we use the Mann-Whitney U test. This test does not assume that the parent distributions are normally distributed. It does however assume that both have the same shape and variance. With this test we check if the medians of the two parent distributions differ significantly from each other.\n\n5.8.1 Data and hypotheses\nAgain, we use the rivers data set. We want to test whether the median body length of male guppies differs between samples. We form the following null and alternative hypotheses:\n\n\\(H_0\\): The difference in median body length between the two groups is 0 \\((\\mu A - \\mu G = 0)\\)\n\\(H_1\\): The difference in median body length between the two groups is not 0 \\((\\mu A - \\mu G \\neq 0)\\)\n\nWe use a two-tailed Mann-Whitney U test to see if we can reject the null hypothesis.\n\n\n5.8.2 Summarise and visualise\nWe did this in the previous section.\n\n\n5.8.3 Assumptions\nWe have checked these previously.\n\n\n5.8.4 Implement and interpret the test\nCalculate the median for each group (for reference) and perform a two-tailed, Mann-Whitney U test:\n\nRPython\n\n\nWe group the data using group_by() for each river and then use the summarise() the data.\n\nrivers %&gt;% \n    group_by(river) %&gt;% \n    summarise(median_length = median(length))\n\n# A tibble: 2 × 2\n  river   median_length\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Aripo            20.1\n2 Guanapo          18.8\n\n\nPerform the Mann-Whitney U test:\n\nwilcox_test(length ~ river,\n            alternative = \"two.sided\",\n            data = rivers)\n\n# A tibble: 1 × 7\n  .y.    group1 group2     n1    n2 statistic        p\n* &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 length Aripo  Guanapo    39    29       841 0.000646\n\n\n\nThe first argument must be in the formula format: response ~ predictor\nThe alternative argument gives the type of alternative hypothesis and must be one of two.sided, greater or less\n\nThe output:\n\n.y. gives you the name of the response variable, or variable of interest (length in our case)\ngroup1 and group2 gives you the names of the groups being compared\nn1 and n2 gives you the number of observations in each group\nstatistic gives you the value of the statistic (t = 841)\np gives you the p-value (6.46^{-4})\n\n\n\nBefore we can implement the Mann-Whitney U test, we need to reformat our data a bit.\nThe pg.mwu() function requires the numerical input for the two groups it needs to compare.\nThe easiest way is to reformat our data from the long format where all the data are stacked on top of one another to the wide format, where the length values are in separate columns for the two rivers.\nWe can do this with the pd.pivot() function. We save the output in a new object and then access the values as required. It keeps all the data separate, meaning that there will be missing values NaN in this format. The pg.mwu() function ignores missing values by default.\n\n# reformat the data into a 'wide' format\nrivers_py_wide = pd.pivot(rivers_py,\n                          columns = 'river',\n                          values = 'length')\n      \n# have a look at the format\nrivers_py_wide.head()\n\nriver  Aripo  Guanapo\n0        NaN     19.1\n1        NaN     23.3\n2        NaN     18.2\n3        NaN     16.4\n4        NaN     19.7\n\n\nNext, we can calculate the median values for each river:\n\nrivers_py_wide['Aripo'].median()\n\nnp.float64(20.1)\n\nrivers_py_wide['Guanapo'].median()\n\nnp.float64(18.8)\n\n\nFinally, we can perform the Mann-Whitney U test:\n\n# perform the Mann-Whitney U test\n# ignoring the missing values\npg.mwu(rivers_py_wide['Aripo'],\n       rivers_py_wide['Guanapo'])\n\n     U-val alternative     p-val       RBC     CLES\nMWU  841.0   two-sided  0.000646  0.487179  0.74359\n\n\n\n\n\nGiven that the p-value is less than 0.05 we can reject the null hypothesis at this confidence level. Again, the p-value on the 3rd line is what we’re most interested in. Since the p-value is very small (much smaller than the standard significance level) we choose to say “that it is very unlikely that these two samples came from the same parent distribution and as such we can reject our null hypothesis”.\nTo put it more completely, we can state that:\n\nA Mann-Whitney test indicated that the median body length of male guppies in the Guanapo river (\\(\\tilde{x}\\) = 18.8 mm) differs significantly from the median body length of male guppies in the Aripo river (\\(\\tilde{x}\\) = 20.1 mm, p = 0.0006).",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#exercises",
    "href": "materials/cs1_practical_two-samples.html#exercises",
    "title": "5  Two-sample data",
    "section": "5.9 Exercises",
    "text": "5.9 Exercises\n\n5.9.1 Q-Q plots rivers\n\n\n\n\n\n\nExerciseExercise 1\n\n\n\n\n\n\nLevel: \nCreate the Q-Q plots for the two rivers in the data/CS1-twosample.csv file and discuss with your neighbour what you see in light of the results from the above Shapiro-Wilk test.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nRPython\n\n\n\n# we group the data by river\n# then create a panel per river\n# containing the Q-Q plot for that river\nggplot(rivers,\n       aes(sample = length)) +\n  stat_qq() +\n  stat_qq_line(colour = \"blue\") +\n  facet_wrap(facets = vars(river))\n\n\n\n\n\n\n\n\n\n\n\np = (ggplot(rivers_py,\n        aes(sample = \"length\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"red\") +\n     facet_wrap(\"river\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.9.2 Turtles\n\n\n\n\n\n\nExerciseExercise 2\n\n\n\n\n\n\nLevel: \nThis exercise explores serum cholesterol concentrations in turtles.\nUsing the data in data/CS1-turtle.csv, test the null hypothesis that male and female turtles have the same mean serum cholesterol concentrations.\n\nLoad the data\nWrite down the null and alternative hypotheses\nImport the data\nSummarise and visualise the data\nCheck your assumptions (normality and variance) using appropriate tests and plots\nPerform a two-sample t-test\nWrite down a sentence that summarises the results that you have found\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n5.10 Answer\n\nData\nOnce you’ve loaded the data, have a look at the structure. The data is in a tidy data format, where each variable (the thing that you measure) is in its own column. Each observation has its own row.\n\n\nHypotheses\n\\(H_0\\) : male mean \\(=\\) female mean\n\\(H_1\\) : male mean \\(\\neq\\) female mean\n\n\nLoad, summarise and visualise data\nLet’s load the data and explore our data a bit more before we dive into the statistics.\n\nRPython\n\n\n\n# load the data\nturtle &lt;- read_csv(\"data/CS1-turtle.csv\")\n\n# and have a look\nturtle\n\n# A tibble: 13 × 2\n   serum sex   \n   &lt;dbl&gt; &lt;chr&gt; \n 1  220. Male  \n 2  219. Male  \n 3  230. Male  \n 4  229. Male  \n 5  222  Male  \n 6  224. Male  \n 7  226. Male  \n 8  223. Female\n 9  222. Female\n10  230. Female\n11  224. Female\n12  224. Female\n13  231. Female\n\n\nLet’s summarise the data (although a visualisation is probably much easier to work with):\n\n# create summary statistics for each group\nturtle %&gt;% \n  group_by(sex) %&gt;% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 2 × 11\n  sex    variable     n   min   max median   iqr  mean    sd    se    ci\n  &lt;chr&gt;  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Female serum        6  222.  231.   224.  5.22  226.  3.87  1.58  4.06\n2 Male   serum        7  219.  230.   224.  6.6   224.  4.26  1.61  3.94\n\n\nand visualise the data:\n\n# visualise the data\nggplot(turtle,\n       aes(x = sex, y = serum)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nturtle_py = pd.read_csv(\"data/CS1-turtle.csv\")\n\nturtle_py.describe()\n\n            serum\ncount   13.000000\nmean   224.900000\nstd      3.978274\nmin    218.600000\n25%    222.000000\n50%    224.100000\n75%    228.800000\nmax    230.800000\n\n\nand visualise the data:\n\np = (ggplot(turtle_py,\n        aes(x = \"sex\", y = \"serum\")) +\n     geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nAs always we use the plot and summary to assess three things:\n\nDoes it look like we’ve loaded the data in correctly?\n\nWe have two groups and the extreme values of our plots seem to match with our data set, so I’m happy that we haven’t done anything massively wrong here.\n\nDo we think that there is a difference between the two groups?\n\nWe need the result of the formal test to make sense given the data, so it’s important to develop a sense of what we think is going to happen here. Whilst the ranges of the two groups suggests that the Female serum levels might be higher than the males when we look at things more closely we realise that isn’t the case. The box plot shows that the median values of the two groups is virtually identical and this is backed up by the summary statistics we calculated: the medians are both about 224.1, and the means are fairly close too (225.7 vs 224.2). Based on this, and the fact that there are only 13 observations in total I would be very surprised if any test came back showing that there was a difference between the groups.\n\nWhat do we think about assumptions?\n\nNormality looks a bit worrying: whilst the Male group appears nice and symmetric (and so might be normal), the Female group appears to be quite skewed (since the median is much closer to the bottom than the top). We’ll have to look carefully at the more formal checks to decided whether we think the data are normal enough for us to use a t-test.\nHomogeneity of variance. At this stage the spread of the data within each group looks similar, but because of the potential skew in the Female group we’ll again want to check the assumptions carefully.\n\n\n\n\nAssumptions\nNormality\nLet’s look at the normality of each of the groups separately. There are several ways of getting at the serum values for Male and Female groups separately. All of them come down to splitting the data. Afterwards we use the Shapiro-Wilk (‘formal’ test), followed by Q-Q plots (much more informative).\n\nRPython\n\n\n\n# perform Shapiro-Wilk test on each group\nturtle %&gt;% \n    filter(sex == \"Female\") %&gt;% \n    pull(serum) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.84178, p-value = 0.1349\n\nturtle %&gt;% \n    filter(sex == \"Male\") %&gt;% \n    pull(serum) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.94392, p-value = 0.6743\n\n\n\n\n\npg.normality(dv = \"serum\",\n             group = \"sex\",\n             data = turtle_py)\n\n               W      pval  normal\nsex                               \nMale    0.943924  0.674275    True\nFemale  0.841785  0.134869    True\n\n\n\n\n\nThe p-values for both Shapiro-Wilk tests are non-significant which suggests that the data are normal enough. This is a bit surprising given what we saw in the box plot but there are two bits of information that we can use to reassure us.\n\nThe p-value for the Female group is smaller than for the Male group (suggesting that the Female group is closer to being non-normal than the Male group) which makes sense based on our visual observations.\nThe Shapiro-Wilk test is generally quite relaxed about normality for small sample sizes (and notoriously strict for very large sample sizes). For a group with only 6 data points in it, the data would actually have to have a really, really skewed distribution. Given that the Female group only has 6 data points in it, it’s not too surprising that the Shapiro-Wilk test came back saying everything is OK.\n\nGiven these caveats of the Shapiro-Wilk test (I’ll stop mentioning them now, I think I’ve made my opinion clear ;)), let’s look at the Q-Q plots.\n\nRPython\n\n\n\n# create Q-Q plots for both groups\nggplot(turtle,\n       aes(sample = serum)) +\n  stat_qq() +\n  stat_qq_line(colour = \"blue\") +\n  facet_wrap(facets = vars(sex))\n\n\n\n\n\n\n\n\n\n\n\n# create Q-Q plots for both groups\np = (ggplot(turtle_py,\n        aes(sample = \"serum\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"red\") +\n     facet_wrap(\"sex\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nThe results from the Q-Q plots echo what we’ve already seen from the Shapiro-Wilk analyses. The normality of the data in the Male group doesn’t look too bad whereas the those in the Female group looks somewhat dodgy.\nOverall, the assumption of normality of the data doesn’t appear to be very well met at all, but we do have to bear in mind that there are only a few data points in each group and we might just be seeing this pattern in the data due to random chance rather than because the underlying populations are actually not normally distributed. Personally, though I’d edge towards non-normal here.\nHomogeneity of Variance\nIt’s not clear whether the data are normal or not, so it isn’t clear which test to use here. The sensible approach is to do both and hope that they agree (fingers crossed!). Or err on the side of caution and assume they are not normal, but potentially throwing away statistical power (more on that later).\n\nRPython\n\n\nBartlett’s test gives us:\n\n# perform Bartlett's test\nbartlett.test(serum ~ sex,\n              data = turtle)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  serum by sex\nBartlett's K-squared = 0.045377, df = 1, p-value = 0.8313\n\n\nand Levene’s test gives us:\n\n# perform Levene's test\nlevene_test(serum ~ sex,\n              data = turtle)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1    11     0.243 0.631\n\n\n\n\nBartlett’s test gives us:\n\npg.homoscedasticity(dv = \"serum\",\n                    group = \"sex\",\n                    method = \"bartlett\",\n                    data = turtle_py)\n\n                 T      pval  equal_var\nbartlett  0.045377  0.831312       True\n\n\nand Levene’s test gives us:\n\npg.homoscedasticity(dv = \"serum\",\n                    group = \"sex\",\n                    method = \"levene\",\n                    data = turtle_py)\n\n               W     pval  equal_var\nlevene  0.243418  0.63145       True\n\n\n\n\n\nThe good news is that both Levene and Bartlett agree that there is homogeneity of variance between the two groups (thank goodness, that’s one less thing to worry about!).\nOverall, what this means is that we’re not too sure about normality, but that homogeneity of variance is pretty good.\n\n\nImplement two-sample t-test\nBecause of the result of the Bartlett test I know that I can carry out a two-sample Student’s t-test. If the variances between the two groups were not equal, then we’d have to perform Welch’s t-test.\n\nRPython\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the t_test() function the default option for dealing with equality of variance is to assume there there isn’t any. If you look at the help page with ?t_test() then you’ll see that the default for var.equal = FALSE. Here we do assume equality of variance, so we set it to var.equal = TRUE.\n\n\n\n# perform two-sample t-test\nt_test(serum ~ sex,\n       alternative = \"two.sided\",\n       var.equal = TRUE,\n       data = turtle)\n\n# A tibble: 1 × 8\n  .y.   group1 group2    n1    n2 statistic    df     p\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 serum Female Male       6     7     0.627    11 0.544\n\n\n\n\nTo test if the two groups are different from one another, we can use the pg.ttest() function.\nThis function requires the data to be separated into individual groups, so we need to split the serum data by sex. One of the ways we can do this is:\n\nturtle_male = turtle_py.query('sex == \"Male\"')[\"serum\"]\nturtle_female = turtle_py.query('sex == \"Female\"')[\"serum\"]\n\nNext, we use these data to test for differences:\n\npg.ttest(turtle_female, turtle_male,\n                alternative = \"two-sided\",\n                correction = False).transpose()\n\n                    T-test\nT                 0.626811\ndof                     11\nalternative      two-sided\np-val             0.543573\nCI95%        [-3.58, 6.42]\ncohen-d           0.348725\nBF10                 0.519\npower             0.088495\n\n\n\n\n\nWith a p-value of 0.544, this test tells us that there is insufficient evidence to suggest that the means of the two groups are different. A suitable summary sentence would be:\n\nA Student’s two-sample t-test indicated that the mean serum cholesterol level did not differ significantly between male and female turtles (p = 0.544).\n\n\n\nDiscussion\nIn reality, because of the ambiguous normality assumption assessment, for this data set I would actually carry out two different tests; the two-sample t-test with equal variance and the Mann-Whitney U test. If both of them agreed then it wouldn’t matter too much which one I reported (I’d personally report both with a short sentence to say that I’m doing that because it wasn’t clear whether the assumption of normality had or had not been met), but it would be acceptable to report just one.\n\n\n\n\n\n\n\n\n\n\n\n5.10.1 Turtles (revisited)\n\n\n\n\n\n\nExerciseExercise 3\n\n\n\n\n\n\nLevel: \nAnalyse the turtle data set from before using a Mann-Whitney U test.\nWe follow the same process as with Student’s t-test.\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\n\nHypotheses\n\\(H_0\\) : male median \\(=\\) female median\n\\(H_1\\) : male median \\(\\neq\\) female median\n\n\nSummarise and visualise\nThis is the same as before.\n\n\nAssumptions\nWe’ve already checked that the variances of the two groups are similar, so we’re OK there. Whilst the Mann-Whitney U test doesn’t require normality or symmetry of distributions it does require that the distributions have the same shape. In this example, with just a handful of data points in each group, it’s quite hard to make this call one way or another. My advice in this case would be say that unless it’s obvious that the distributions are very different we can just allow this assumption to pass, and you’re only going see obvious differences in distribution shape when you have considerably more data points than we have here.\n\n\nCarry out a Mann-Whitney U test\n\nRPython\n\n\n\nwilcox_test(serum ~ sex,\n            alternative = \"two.sided\",\n            data = turtle)\n\n# A tibble: 1 × 7\n  .y.   group1 group2    n1    n2 statistic     p\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 serum Female Male       6     7        26 0.534\n\n\n\n\n\n# reformat the data into a 'wide' format\nturtle_py_wide = pd.pivot(turtle_py,\n                          columns = 'sex',\n                          values = 'serum')\n      \n# have a look at the format\nturtle_py_wide.head()\n\nsex  Female   Male\n0       NaN  220.1\n1       NaN  218.6\n2       NaN  229.6\n3       NaN  228.8\n4       NaN  222.0\n\n\n\n# perform the Mann-Whitney U test\n# ignoring the missing values\npg.mwu(turtle_py_wide['Male'],\n       turtle_py_wide['Female'])\n\n     U-val alternative   p-val       RBC      CLES\nMWU   16.0   two-sided  0.5338 -0.238095  0.380952\n\n\n\n\n\nThis gives us exactly the same conclusion that we got from the two-sample t-test i.e. that there isn’t any significant difference between the two groups.\n\nA Mann-Whitney U test indicated that there wasn’t a significant difference in the median serum cholesterol levels between male and female turtles (p = 0.534)",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#answer",
    "href": "materials/cs1_practical_two-samples.html#answer",
    "title": "5  Two-sample data",
    "section": "5.10 Answer",
    "text": "5.10 Answer\n\nData\nOnce you’ve loaded the data, have a look at the structure. The data is in a tidy data format, where each variable (the thing that you measure) is in its own column. Each observation has its own row.\n\n\nHypotheses\n\\(H_0\\) : male mean \\(=\\) female mean\n\\(H_1\\) : male mean \\(\\neq\\) female mean\n\n\nLoad, summarise and visualise data\nLet’s load the data and explore our data a bit more before we dive into the statistics.\n\nRPython\n\n\n\n# load the data\nturtle &lt;- read_csv(\"data/CS1-turtle.csv\")\n\n# and have a look\nturtle\n\n# A tibble: 13 × 2\n   serum sex   \n   &lt;dbl&gt; &lt;chr&gt; \n 1  220. Male  \n 2  219. Male  \n 3  230. Male  \n 4  229. Male  \n 5  222  Male  \n 6  224. Male  \n 7  226. Male  \n 8  223. Female\n 9  222. Female\n10  230. Female\n11  224. Female\n12  224. Female\n13  231. Female\n\n\nLet’s summarise the data (although a visualisation is probably much easier to work with):\n\n# create summary statistics for each group\nturtle %&gt;% \n  group_by(sex) %&gt;% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 2 × 11\n  sex    variable     n   min   max median   iqr  mean    sd    se    ci\n  &lt;chr&gt;  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Female serum        6  222.  231.   224.  5.22  226.  3.87  1.58  4.06\n2 Male   serum        7  219.  230.   224.  6.6   224.  4.26  1.61  3.94\n\n\nand visualise the data:\n\n# visualise the data\nggplot(turtle,\n       aes(x = sex, y = serum)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nturtle_py = pd.read_csv(\"data/CS1-turtle.csv\")\n\nturtle_py.describe()\n\n            serum\ncount   13.000000\nmean   224.900000\nstd      3.978274\nmin    218.600000\n25%    222.000000\n50%    224.100000\n75%    228.800000\nmax    230.800000\n\n\nand visualise the data:\n\np = (ggplot(turtle_py,\n        aes(x = \"sex\", y = \"serum\")) +\n     geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nAs always we use the plot and summary to assess three things:\n\nDoes it look like we’ve loaded the data in correctly?\n\nWe have two groups and the extreme values of our plots seem to match with our data set, so I’m happy that we haven’t done anything massively wrong here.\n\nDo we think that there is a difference between the two groups?\n\nWe need the result of the formal test to make sense given the data, so it’s important to develop a sense of what we think is going to happen here. Whilst the ranges of the two groups suggests that the Female serum levels might be higher than the males when we look at things more closely we realise that isn’t the case. The box plot shows that the median values of the two groups is virtually identical and this is backed up by the summary statistics we calculated: the medians are both about 224.1, and the means are fairly close too (225.7 vs 224.2). Based on this, and the fact that there are only 13 observations in total I would be very surprised if any test came back showing that there was a difference between the groups.\n\nWhat do we think about assumptions?\n\nNormality looks a bit worrying: whilst the Male group appears nice and symmetric (and so might be normal), the Female group appears to be quite skewed (since the median is much closer to the bottom than the top). We’ll have to look carefully at the more formal checks to decided whether we think the data are normal enough for us to use a t-test.\nHomogeneity of variance. At this stage the spread of the data within each group looks similar, but because of the potential skew in the Female group we’ll again want to check the assumptions carefully.\n\n\n\n\nAssumptions\nNormality\nLet’s look at the normality of each of the groups separately. There are several ways of getting at the serum values for Male and Female groups separately. All of them come down to splitting the data. Afterwards we use the Shapiro-Wilk (‘formal’ test), followed by Q-Q plots (much more informative).\n\nRPython\n\n\n\n# perform Shapiro-Wilk test on each group\nturtle %&gt;% \n    filter(sex == \"Female\") %&gt;% \n    pull(serum) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.84178, p-value = 0.1349\n\nturtle %&gt;% \n    filter(sex == \"Male\") %&gt;% \n    pull(serum) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.94392, p-value = 0.6743\n\n\n\n\n\npg.normality(dv = \"serum\",\n             group = \"sex\",\n             data = turtle_py)\n\n               W      pval  normal\nsex                               \nMale    0.943924  0.674275    True\nFemale  0.841785  0.134869    True\n\n\n\n\n\nThe p-values for both Shapiro-Wilk tests are non-significant which suggests that the data are normal enough. This is a bit surprising given what we saw in the box plot but there are two bits of information that we can use to reassure us.\n\nThe p-value for the Female group is smaller than for the Male group (suggesting that the Female group is closer to being non-normal than the Male group) which makes sense based on our visual observations.\nThe Shapiro-Wilk test is generally quite relaxed about normality for small sample sizes (and notoriously strict for very large sample sizes). For a group with only 6 data points in it, the data would actually have to have a really, really skewed distribution. Given that the Female group only has 6 data points in it, it’s not too surprising that the Shapiro-Wilk test came back saying everything is OK.\n\nGiven these caveats of the Shapiro-Wilk test (I’ll stop mentioning them now, I think I’ve made my opinion clear ;)), let’s look at the Q-Q plots.\n\nRPython\n\n\n\n# create Q-Q plots for both groups\nggplot(turtle,\n       aes(sample = serum)) +\n  stat_qq() +\n  stat_qq_line(colour = \"blue\") +\n  facet_wrap(facets = vars(sex))\n\n\n\n\n\n\n\n\n\n\n\n# create Q-Q plots for both groups\np = (ggplot(turtle_py,\n        aes(sample = \"serum\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"red\") +\n     facet_wrap(\"sex\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nThe results from the Q-Q plots echo what we’ve already seen from the Shapiro-Wilk analyses. The normality of the data in the Male group doesn’t look too bad whereas the those in the Female group looks somewhat dodgy.\nOverall, the assumption of normality of the data doesn’t appear to be very well met at all, but we do have to bear in mind that there are only a few data points in each group and we might just be seeing this pattern in the data due to random chance rather than because the underlying populations are actually not normally distributed. Personally, though I’d edge towards non-normal here.\nHomogeneity of Variance\nIt’s not clear whether the data are normal or not, so it isn’t clear which test to use here. The sensible approach is to do both and hope that they agree (fingers crossed!). Or err on the side of caution and assume they are not normal, but potentially throwing away statistical power (more on that later).\n\nRPython\n\n\nBartlett’s test gives us:\n\n# perform Bartlett's test\nbartlett.test(serum ~ sex,\n              data = turtle)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  serum by sex\nBartlett's K-squared = 0.045377, df = 1, p-value = 0.8313\n\n\nand Levene’s test gives us:\n\n# perform Levene's test\nlevene_test(serum ~ sex,\n              data = turtle)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1    11     0.243 0.631\n\n\n\n\nBartlett’s test gives us:\n\npg.homoscedasticity(dv = \"serum\",\n                    group = \"sex\",\n                    method = \"bartlett\",\n                    data = turtle_py)\n\n                 T      pval  equal_var\nbartlett  0.045377  0.831312       True\n\n\nand Levene’s test gives us:\n\npg.homoscedasticity(dv = \"serum\",\n                    group = \"sex\",\n                    method = \"levene\",\n                    data = turtle_py)\n\n               W     pval  equal_var\nlevene  0.243418  0.63145       True\n\n\n\n\n\nThe good news is that both Levene and Bartlett agree that there is homogeneity of variance between the two groups (thank goodness, that’s one less thing to worry about!).\nOverall, what this means is that we’re not too sure about normality, but that homogeneity of variance is pretty good.\n\n\nImplement two-sample t-test\nBecause of the result of the Bartlett test I know that I can carry out a two-sample Student’s t-test. If the variances between the two groups were not equal, then we’d have to perform Welch’s t-test.\n\nRPython\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the t_test() function the default option for dealing with equality of variance is to assume there there isn’t any. If you look at the help page with ?t_test() then you’ll see that the default for var.equal = FALSE. Here we do assume equality of variance, so we set it to var.equal = TRUE.\n\n\n\n# perform two-sample t-test\nt_test(serum ~ sex,\n       alternative = \"two.sided\",\n       var.equal = TRUE,\n       data = turtle)\n\n# A tibble: 1 × 8\n  .y.   group1 group2    n1    n2 statistic    df     p\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 serum Female Male       6     7     0.627    11 0.544\n\n\n\n\nTo test if the two groups are different from one another, we can use the pg.ttest() function.\nThis function requires the data to be separated into individual groups, so we need to split the serum data by sex. One of the ways we can do this is:\n\nturtle_male = turtle_py.query('sex == \"Male\"')[\"serum\"]\nturtle_female = turtle_py.query('sex == \"Female\"')[\"serum\"]\n\nNext, we use these data to test for differences:\n\npg.ttest(turtle_female, turtle_male,\n                alternative = \"two-sided\",\n                correction = False).transpose()\n\n                    T-test\nT                 0.626811\ndof                     11\nalternative      two-sided\np-val             0.543573\nCI95%        [-3.58, 6.42]\ncohen-d           0.348725\nBF10                 0.519\npower             0.088495\n\n\n\n\n\nWith a p-value of 0.544, this test tells us that there is insufficient evidence to suggest that the means of the two groups are different. A suitable summary sentence would be:\n\nA Student’s two-sample t-test indicated that the mean serum cholesterol level did not differ significantly between male and female turtles (p = 0.544).\n\n\n\nDiscussion\nIn reality, because of the ambiguous normality assumption assessment, for this data set I would actually carry out two different tests; the two-sample t-test with equal variance and the Mann-Whitney U test. If both of them agreed then it wouldn’t matter too much which one I reported (I’d personally report both with a short sentence to say that I’m doing that because it wasn’t clear whether the assumption of normality had or had not been met), but it would be acceptable to report just one.",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#summary",
    "href": "materials/cs1_practical_two-samples.html#summary",
    "title": "5  Two-sample data",
    "section": "5.11 Summary",
    "text": "5.11 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nStudent’s t tests are used when you have two samples of continuous data, which are normally distributed, independent of each other and have equal variance\nA good way of assessing the assumption of normality is by checking the data against a Q-Q plot\nWe can check equality of variance (homoscedasticity) with Bartlett’s (normal data) or Levene’s (non-normal data) test\nThe Mann-Whitney U test is used when you have two samples of continuous data, which are not normally distributed, but are independent of each other, have equal variance and similar distributional shape",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples-paired.html",
    "href": "materials/cs1_practical_two-samples-paired.html",
    "title": "6  Paired data",
    "section": "",
    "text": "6.1 Libraries and functions\nA paired t-test is used when we have two samples of continuous data that can be paired (examples of these sort of data would be weights of individuals before and after a diet). This test is applicable if the number of paired points within the samples is large (&gt;30) or, if the number of points is small, then this test also works when the parent distributions are normally distributed.\nThere is the assumption that each pair within the data is independent of each other.",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Paired data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples-paired.html#libraries-and-functions",
    "href": "materials/cs1_practical_two-samples-paired.html#libraries-and-functions",
    "title": "6  Paired data",
    "section": "",
    "text": "NoteClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n6.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n\n\n6.1.2 Functions\n\n# Performs a one-sample t-test, Student's t-test\n# and Welch's t-test in later sections\nrstatix::t_test()\n\n# Performs a Shapiro-Wilk test for normality\nstats::shapiro.test()\n\n# Performs one and two sample Wilcoxon tests\nrstatix::wilcox_test()\n\n# Plots a Q-Q plot for comparison with a normal distribution\nggplot2::stat_qq()\n\n# Adds a comparison line to the Q-Q plot\nggplot2::stat_qq_line()\n\n# Plots jittered points by adding a small amount of random\n# variation to each point, to handle overplotting\nggplot2::geom_jitter()\n\n# Computes summary statistics                         \nrstatix::get_summary_stats() \n\n# \"Widens\" the data, increasing the number of columns\ntidyr::pivot_wider()\n\n\n\n\n\n6.1.3 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n\n\n6.1.4 Functions\n\n# Return reshaped DataFrame organised by given index / column values\npandas.DataFrame.pivot()\n\n# Reads in a .csv file\npandas.DataFrame.read_csv()\n\n# Performs the Shapiro-Wilk test for normality\npingouin.normality()\n\n# Performs a t-test\npingouin.ttest()\n\n# Performs Wilcoxon signed rank test\npingouin.wilcoxon()\n\n# Plots a Q-Q plot for comparison with a normal distribution\nplotnine.stats.stat_qq()\n\n# Adds a comparison line to the Q-Q plot\nplotnine.stats.stat_qq_line()\n\n                             |",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Paired data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples-paired.html#data-and-hypotheses",
    "href": "materials/cs1_practical_two-samples-paired.html#data-and-hypotheses",
    "title": "6  Paired data",
    "section": "6.2 Data and hypotheses",
    "text": "6.2 Data and hypotheses\nFor example, suppose we measure the cortisol levels in 20 adult females (nmol/l) first thing in the morning and again in the evening. We want to test whether the cortisol levels differs between the two measurement times. We will initially form the following null and alternative hypotheses:\n\n\\(H_0\\): There is no difference in cortisol level between times (\\(\\mu M = \\mu E\\))\n\\(H_1\\): There is a difference in cortisol levels between times (\\(\\mu M \\neq \\mu E\\))\n\nWe use a two-sample, two-tailed paired t-test to see if we can reject the null hypothesis.\n\nWe use a two-sample test because we now have two samples\nWe use a two-tailed t-test because we want to know if our data suggest that the true (population) means are different from one another rather than that one mean is specifically bigger or smaller than the other\nWe use a paired test because each data point in the first sample can be linked to another data point in the second sample by a connecting factor\nWe’re using a t-test because we assume that the underlying population is normally distributed (We’ll check this in a bit)\n\nThe data are stored in a tidy format in the file data/CS1-twopaired.csv.\n\nRPython\n\n\n\n# load the data\ncortisol &lt;- read_csv(\"data/CS1-twopaired.csv\")\n\nRows: 40 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): time\ndbl (2): patient_id, cortisol\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# have a look at the data\ncortisol\n\n# A tibble: 40 × 3\n   patient_id time    cortisol\n        &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1          1 morning     311.\n 2          2 morning     146.\n 3          3 morning     297 \n 4          4 morning     271.\n 5          5 morning     268.\n 6          6 morning     264.\n 7          7 morning     358.\n 8          8 morning     316.\n 9          9 morning     336.\n10         10 morning     221.\n# ℹ 30 more rows\n\n\n\n\n\n# load the data\ncortisol_py = pd.read_csv('data/CS1-twopaired.csv')\n\n# inspect the data\ncortisol_py.head()\n\n   patient_id     time  cortisol\n0           1  morning     310.6\n1           2  morning     146.1\n2           3  morning     297.0\n3           4  morning     270.9\n4           5  morning     267.5\n\n\n\n\n\nWe can see that the data frame consists of three columns:\n\npatient_id, a unique ID for each patient\ntime when the cortisol level was measured\ncortisol, which contains the measured value.\n\nFor each patient_id there are two measurements: one in the morning and one in the afternoon.",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Paired data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples-paired.html#summarise-and-visualise",
    "href": "materials/cs1_practical_two-samples-paired.html#summarise-and-visualise",
    "title": "6  Paired data",
    "section": "6.3 Summarise and visualise",
    "text": "6.3 Summarise and visualise\nIt’s always a good idea to visualise your data, so let’s do that.\n\nRPython\n\n\n\n# create a boxplot\nggplot(cortisol,\n       aes(x = time, y = cortisol)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.05) +\n  ylab(\"Cortisol level (nmol/l)\")\n\n\n\n\n\n\n\n\nHere we use also visualise the actual data points, to get a sense of how these data are spread out. To avoid overlapping the data points (try using geom_point() instead of geom_jitter()), we jitter the data points. What geom_jitter() does is add a small amount of variation to each point.\n\n\n\np = (ggplot(cortisol_py,\n        aes(x = \"time\",\n            y = \"cortisol\")) +\n     geom_boxplot() +\n     geom_jitter(width = 0.05) +\n     ylab(\"Cortisol level (nmol/l)\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nHowever, this plot does not capture how the cortisol level of each individual subject has changed though. We can explore the individual changes between morning and evening by looking at the differences between the two times of measurement for each patient.\nTo do this, we need to put our data into a wide format, so we can calculate the change in cortisol level for each patient.\n\nRPython\n\n\nIn tidyverse we can use the pivot_wider() function.\n\n# calculate the difference between evening and morning values\ncortisol_diff &lt;- cortisol %&gt;%\n  pivot_wider(id_cols = patient_id,\n              names_from = time,\n              values_from = cortisol) %&gt;% \n  mutate(cortisol_change = evening - morning)\n\ncortisol_diff\n\n# A tibble: 20 × 4\n   patient_id morning evening cortisol_change\n        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;\n 1          1    311.   273.            -37.4\n 2          2    146.    65.7           -80.4\n 3          3    297    257.            -40.4\n 4          4    271.   321              50.1\n 5          5    268.    80.3          -187. \n 6          6    264.   379.            116. \n 7          7    358.   163.           -195. \n 8          8    316.   294.            -22  \n 9          9    336.   140.           -196. \n10         10    221.   231.             10.4\n11         11    366    131.           -235. \n12         12    256.   114.           -142. \n13         13    432.   217.           -215. \n14         14    208.    60.1          -148. \n15         15    324.   199.           -125. \n16         16    388.   170.           -218. \n17         17    332    160.           -172. \n18         18    414.   179.           -235. \n19         19    405.   286            -119. \n20         20    356.   226.           -130. \n\n\nThere are three arguments in pivot_wider():\n\nid_cols = patient_id tells it that each observational unit is determined by patient_id\nnames_from = time says that there will be new columns, with names from the time column (in this case, there are two values in there, morning and evening)\nvalues_from = cortisol populates the new columns with the values coming from the cortisol column\n\nLastly, we create a new column cortisol_change that contains the difference between the evening and morning measurements.\nAfter this we can plot our data:\n\n# plot the data\nggplot(cortisol_diff,\n       aes(y = cortisol_change)) +\n  geom_boxplot() +\n  ylab(\"Change in cortisol (nmol/l)\")\n\n\n\n\n\n\n\n\nThe differences in cortisol levels appear to be very much less than zero, meaning that the evening cortisol levels appear to be much lower than the morning ones. As such we would expect that the test would give a pretty significant result.\nAn alternative representation would be to plot the data points for both evening and morning and connect them by patient:\n\n# plot cortisol levels by patient\nggplot(cortisol,\n       aes(x = time,\n           y = cortisol,\n           group = patient_id)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\n\nThis gives a similar picture to what the boxplot was telling us, that for most patients the cortisol levels are higher in the morning than in the evening.\n\n\n\n# reformat the data into a 'wide' format\ncortisol_diff_py = pd.pivot(cortisol_py, index = \"patient_id\", columns = \"time\", values = \"cortisol\")\n\n# add a new column with difference between\n# evening and morning cortisol levels\ncortisol_diff_py[\"cortisol_change\"] = cortisol_diff_py[\"evening\"].subtract(cortisol_diff_py[\"morning\"])\n      \n# have a look at the format\ncortisol_diff_py.head()\n\ntime        evening  morning  cortisol_change\npatient_id                                   \n1             273.2    310.6            -37.4\n2              65.7    146.1            -80.4\n3             256.6    297.0            -40.4\n4             321.0    270.9             50.1\n5              80.3    267.5           -187.2\n\n\nAfter this we can plot our data:\n\n# plot the data\np = (ggplot(cortisol_diff_py,\n        aes(x = \"1\",\n            y = \"cortisol_change\")) +\n     geom_boxplot() +\n     ylab(\"Change in cortisol (nmol/l)\"))\n\np.show()\n\n\n\n\n\n\n\n\nThe differences in cortisol levels appear to be very much less than zero, meaning that the evening cortisol levels appear to be much lower than the morning ones. As such we would expect that the test would give a pretty significant result.\nAn alternative representation would be to plot the data points for both evening and morning and connect them by patient:\n\n# plot cortisol levels by patient\np = (ggplot(cortisol_py,\n        aes(x = \"time\",\n            y = \"cortisol\",\n            group = \"patient_id\")) +\n     geom_point() +\n     geom_line())\n\np.show()\n\n\n\n\n\n\n\n\nThis gives a similar picture to what the boxplot was telling us, that for most patients the cortisol levels are higher in the morning than in the evening.",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Paired data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples-paired.html#assumptions",
    "href": "materials/cs1_practical_two-samples-paired.html#assumptions",
    "title": "6  Paired data",
    "section": "6.4 Assumptions",
    "text": "6.4 Assumptions\nYou will do this in the exercise!",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Paired data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples-paired.html#implement-and-interpret-the-test",
    "href": "materials/cs1_practical_two-samples-paired.html#implement-and-interpret-the-test",
    "title": "6  Paired data",
    "section": "6.5 Implement and interpret the test",
    "text": "6.5 Implement and interpret the test\nPerform a two-sample, two-tailed, paired t-test:\n\nRPython\n\n\n\n# perform the test\nt_test(cortisol ~ time,\n       alternative = \"two.sided\",\n       paired = TRUE,\n       data = cortisol)\n\n\nThe first argument must be in the formula format: response ~ predictor\nThe alternative argument gives the type of alternative hypothesis and must be one of two.sided, greater or less\nThe paired = TRUE argument indicates that the data are paired\n\nFrom our perspective the value of interest in the output is the p (5.29^{-5}).\n\n\nTo perform a paired t-test we can use the same pg.ttest() as before, but set the argument paired = True.\nAnnoyingly, the output is not entirely visible because the data frame is too wide. To deal with that, we can simply transpose it with transpose()\n\npg.ttest(cortisol_diff_py[\"evening\"],\n         cortisol_diff_py[\"morning\"],\n         alternative = \"two-sided\",\n         paired = True).transpose()\n\n                        T-test\nT                     -5.18329\ndof                         19\nalternative          two-sided\np-val                 0.000053\nCI95%        [-162.96, -69.21]\ncohen-d               1.434359\nBF10                   491.599\npower                  0.99998\n\n\nFrom our perspective the value of interest is the p-val.\n\n\n\nSince the p-value = 5.29 \\(\\times\\) 10-5) and thus substantially less than 0.05 we can reject the null hypothesis and state:\n\nA two-tailed, paired t-test indicated that the average cortisol level in adult females differed significantly between the morning (313.5 nmol/l) and the evening (197.4 nmol/l, p = 5.3 * 10-5).",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Paired data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples-paired.html#dealing-with-non-normal-data",
    "href": "materials/cs1_practical_two-samples-paired.html#dealing-with-non-normal-data",
    "title": "6  Paired data",
    "section": "6.6 Dealing with non-normal data",
    "text": "6.6 Dealing with non-normal data\nThe example above assumes that the paired data come from parent distributions that are normal. As we’ve seen before, we may have data where we can’t rely on that assumption. Fortunately, there is very little that we need to change in our approach if we want to analyse paired data that violate the assumption of normality.\n\n6.6.1 Data and hypotheses\nUsing the cortisol data from before we form the following null and alternative hypotheses:\n\n\\(H_0\\): The median of the difference in cortisol levels between the two groups is 0 \\((\\mu M = \\mu E)\\)\n\\(H_1\\): The median of the difference in cortisol levels between the two groups is not 0 \\((\\mu M \\neq \\mu E)\\)\n\nWe use a two-tailed Wilcoxon signed rank test to see if we can reject the null hypothesis.\n\n\n6.6.2 Summarise and visualise\nAlready implemented previously.\n\n\n6.6.3 Assumptions\nThese have been checked previously.\n\n\n6.6.4 Implement and interpret the test\nPerform a two-tailed, Wilcoxon signed rank test:\n\nRPython\n\n\n\nwilcox_test(cortisol ~ time,\n            alternative = \"two.sided\",\n            paired = TRUE,\n            data = cortisol)\n\n# A tibble: 1 × 7\n  .y.      group1  group2     n1    n2 statistic        p\n* &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 cortisol evening morning    20    20        13 0.000168\n\n\n\nThe first argument must be in the formula format: response ~ predictor\nThe alternative argument gives the type of alternative hypothesis and must be one of two.sided, greater or less\nThe paired = TRUE argument indicates that the data are paired\n\n\n\nWe’ll use the wide format data set that we created previously:\n\npg.wilcoxon(x = cortisol_diff_py[\"evening\"],\n            y = cortisol_diff_py[\"morning\"],\n            alternative = \"two-sided\",\n            correction = True)\n\n          W-val alternative     p-val      RBC  CLES\nWilcoxon   13.0   two-sided  0.000168 -0.87619  0.16\n\n\n\n\n\nThe p-value is given in the p column (p-value = 0.000168). Given that this is less than 0.05 we can still reject the null hypothesis.\n\nA two-tailed, Wilcoxon signed rank test indicated that the median cortisol level in adult females differed significantly between the morning (320.5 nmol/l) and the evening (188.9 nmol/l, p = 0.00017).",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Paired data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples-paired.html#exercises",
    "href": "materials/cs1_practical_two-samples-paired.html#exercises",
    "title": "6  Paired data",
    "section": "6.7 Exercises",
    "text": "6.7 Exercises\n\n6.7.1 Cortisol levels\n\n\n\n\n\n\nExerciseExercise 1\n\n\n\n\n\n\nLevel: \nCheck the assumptions necessary for this this paired t-test. Was a paired t-test an appropriate test?\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nWe actually don’t care too much about the distributions of the individual groups. Instead we care about the properties of the differences. So for a paired t-test to be valid for this data set, we need the differences between the morning and evening values to be normally distributed.\nLet’s check this with the Shapiro-Wilk test and Q-Q plots, using the wide data frames we created earlier.\n\nRPython\n\n\nPerform Shapiro-Wilk test:\n\n# perform Shapiro-Wilk test on cortisol differences\nshapiro.test(cortisol_diff$cortisol_change)\n\n\n    Shapiro-Wilk normality test\n\ndata:  cortisol_diff$cortisol_change\nW = 0.92362, p-value = 0.1164\n\n\nCreate Q-Q plot:\n\n# create the Q-Q plot\nggplot(cortisol_diff,\n       aes(sample = cortisol_change)) +\n  stat_qq() +\n  stat_qq_line(colour = \"blue\")\n\n\n\n\n\n\n\n\n\n\nPerform Shapiro-Wilk test:\n\n# perform Shapiro-Wilk test on cortisol differences\npg.normality(cortisol_diff_py[\"cortisol_change\"])\n\n                        W      pval  normal\ncortisol_change  0.923622  0.116355    True\n\n\nCreate Q-Q plot:\n\n# create the Q-Q plot\np = (ggplot(cortisol_diff_py,\n        aes(sample = \"cortisol_change\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"red\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nThe Shapiro-Wilk test says that the data are normal enough and whilst the Q-Q plot is mostly fine, there is some suggestion of snaking at the bottom left. I’m actually OK with this because the suggestion of snaking is actually only due to a single point (the last point on the left). If you cover that point up with your thumb (or finger of your choice) then the remaining points in the Q-Q plot look pretty darn good, and so the suggestion of snaking is actually driven by only a single point (which can happen by chance). As such I’m happy that the assumption of normality is well-met in this case. This single point check is a useful thing to remember when assessing diagnostic plots.\nSo, yep, a paired t-test is appropriate for this data set.\n\n\n\n\n\n\n\n\n\n6.7.2 Deer legs\n\n\n\n\n\n\nExerciseExercise 2\n\n\n\n\n\n\nLevel: \nUsing the following data on deer legs (yes, really!), test the null hypothesis that the fore and hind legs of the deer in this data set are the same length.\n\n\n# A tibble: 10 × 2\n   hindleg foreleg\n     &lt;dbl&gt;   &lt;dbl&gt;\n 1     142     138\n 2     140     136\n 3     144     147\n 4     144     139\n 5     142     143\n 6     146     141\n 7     149     143\n 8     150     145\n 9     142     136\n10     148     146\n\n\nDo these results provide any evidence to suggest that fore- and hind-leg length differ in deer?\n\nWrite down the null and alternative hypotheses\nImport the data from data/CS1-deer.csv\nSummarise and visualise the data\nCheck normality\nDiscuss with your (virtual) neighbour which test is most appropriate?\nPerform the test\nWrite down a sentence that summarises the results that you have found\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\n\nHypotheses\n\\(H_0\\) : foreleg average (mean or median) \\(=\\) hindleg average (mean or median)\n\\(H_1\\) : foreleg average \\(\\neq\\) hindleg average\n\n\nImport data, summarise and visualise\nFirst of all, we need to load in the data.\n\nRPython\n\n\n\n# load the data\ndeer &lt;- read_csv(\"data/CS1-deer.csv\")\n\n# have a look\ndeer\n\n# A tibble: 20 × 3\n      id leg     length\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 hindleg    142\n 2     2 hindleg    140\n 3     3 hindleg    144\n 4     4 hindleg    144\n 5     5 hindleg    142\n 6     6 hindleg    146\n 7     7 hindleg    149\n 8     8 hindleg    150\n 9     9 hindleg    142\n10    10 hindleg    148\n11     1 foreleg    138\n12     2 foreleg    136\n13     3 foreleg    147\n14     4 foreleg    139\n15     5 foreleg    143\n16     6 foreleg    141\n17     7 foreleg    143\n18     8 foreleg    145\n19     9 foreleg    136\n20    10 foreleg    146\n\n\nThe ordering of the data is important here; the first hind leg row corresponds to the first fore leg row, the second to the second and so on. To indicate this we use an id column, where each observation has a unique ID.\nLet’s look at the data and see what it tells us.\n\n# summarise the data\nsummary(deer)\n\n       id           leg                length     \n Min.   : 1.0   Length:20          Min.   :136.0  \n 1st Qu.: 3.0   Class :character   1st Qu.:140.8  \n Median : 5.5   Mode  :character   Median :143.0  \n Mean   : 5.5                      Mean   :143.1  \n 3rd Qu.: 8.0                      3rd Qu.:146.0  \n Max.   :10.0                      Max.   :150.0  \n\n\nWe can also summarise some of the main summary statistics for each type of leg. We don’t need summary statistics for the id column, so we unselect it with select(-id).\nTo make life easy we use the get_summary_stats() function from the rstatix package. Have a look at the help function to see what kind of summary statistics it can produce. In this case I’m using the type = \"common\" option to specify that I want to find commonly used statistics (e.g. sample number, min, max, median, mean etc.)\n\n# or even summarise by leg type\ndeer %&gt;% \n  select(-id) %&gt;% \n  group_by(leg) %&gt;% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 2 × 11\n  leg     variable     n   min   max median   iqr  mean    sd    se    ci\n  &lt;chr&gt;   &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 foreleg length      10   136   147    142  6.25  141.  4.03  1.27  2.88\n2 hindleg length      10   140   150    144  5.5   145.  3.40  1.08  2.43\n\n\nVisualising the data is often more useful:\n\n# we can also visualise the data\nggplot(deer,\n       aes(x = leg, y = length)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\nAll of this suggests that there might be a difference between the legs, with hind legs being longer than forelegs. However, this representation obscures the fact that we have paired data. What we really need to look at is the difference in leg length for each observation:\n\n# create a data set that contains the difference in leg length\nleg_diff &lt;- deer %&gt;% \n  pivot_wider(id_cols = id,\n              names_from = leg,\n              values_from = length) %&gt;% \n  mutate(leg_diff = hindleg - foreleg)\n\n\n# plot the difference in leg length\nggplot(leg_diff,\n       aes(y = leg_diff)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nAdditionally, we can also plot the data by observation:\n\n# plot the data by observation\nggplot(deer,\n       aes(x = leg, y = length, group = id)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n# load the data\ndeer_py = pd.read_csv(\"data/CS1-deer.csv\")\n\n# have a look\ndeer_py.head()\n\n   id      leg  length\n0   1  hindleg     142\n1   2  hindleg     140\n2   3  hindleg     144\n3   4  hindleg     144\n4   5  hindleg     142\n\n\nThe ordering of the data is important here; the first hind leg row corresponds to the first fore leg row, the second to the second and so on. To indicate this we use an id column, where each observation has a unique ID.\nLet’s look at the data and see what we can see.\n\n# summarise the data\ndeer_py.describe()\n\n              id      length\ncount  20.000000   20.000000\nmean    5.500000  143.050000\nstd     2.946898    4.006245\nmin     1.000000  136.000000\n25%     3.000000  140.750000\n50%     5.500000  143.000000\n75%     8.000000  146.000000\nmax    10.000000  150.000000\n\n\nWe can also summarise by leg type:\n\ndeer_py.groupby(\"leg\")[\"length\"].describe()\n\n         count   mean       std    min     25%    50%    75%    max\nleg                                                                \nforeleg   10.0  141.4  4.033196  136.0  138.25  142.0  144.5  147.0\nhindleg   10.0  144.7  3.400980  140.0  142.00  144.0  147.5  150.0\n\n\nIt might be more helpful to look at the difference in leg length. In order to calculate that, we need to reformat our data into a ‘wide’ format first:\n\n# reformat the data into a 'wide' format\nleg_diff_py = pd.pivot(deer_py,\n                       index = \"id\",\n                       columns = \"leg\",\n                       values = \"length\")\n\n# have a look at the format\nleg_diff_py.head()\n\nleg  foreleg  hindleg\nid                   \n1        138      142\n2        136      140\n3        147      144\n4        139      144\n5        143      142\n\n\nNext, we can add a new column leg_diff that contains the leg difference:\n\n# add a new column with difference between\n# hind and fore leg length\nleg_diff_py[\"leg_diff\"] = leg_diff_py[\"hindleg\"].subtract(leg_diff_py[\"foreleg\"])\n \n\nFinally, we can visualise this:\n\n# we can also visualise the data\np = (ggplot(leg_diff_py,\n        aes(x = \"1\",\n            y = \"leg_diff\")) +\n     geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\nAll of this suggests that there might be a difference between the legs, with hind legs being longer than forelegs. However, this representation obscures the fact that we have paired data. What we really need to look at is the difference in leg length for each observation:\n\n# plot paired observations\np = (ggplot(deer_py,\n        aes(x = \"leg\",\n            y = \"length\",\n            group = \"id\")) +\n     geom_point() +\n     geom_line())\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nAll of this gives us a much clearer picture. It looks as though the hindlegs are about 4 cm longer than the forelegs, on average. It also suggests that our leg differences might not be normally distributed (the data look a bit skewed in the boxplot).\n\n\nAssumptions\nWe need to consider the distribution of the difference in leg lengths rather than the individual distributions.\n\nRPython\n\n\nShapiro-Wilk test:\n\n# perform Shapiro-Wilk test on leg differences\nshapiro.test(leg_diff$leg_diff)\n\n\n    Shapiro-Wilk normality test\n\ndata:  leg_diff$leg_diff\nW = 0.81366, p-value = 0.02123\n\n\nQ-Q plot:\n\n# create a Q-Q plot\nggplot(leg_diff,\n       aes(sample = leg_diff)) +\n  stat_qq() +\n  stat_qq_line(colour = \"blue\")\n\n\n\n\n\n\n\n\n\n\nShapiro-Wilk test:\n\n# perform Shapiro-Wilk test on leg length differences\npg.normality(leg_diff_py[\"leg_diff\"])\n\n                 W      pval  normal\nleg_diff  0.813656  0.021235   False\n\n\nCreate the Q-Q plot:\n\n# create the Q-Q plot\np = (ggplot(leg_diff_py,\n        aes(sample = \"leg_diff\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"red\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nBoth our Shapiro-Wilk test and our Q-Q plot suggest that the difference data aren’t normally distributed, which rules out a paired t-test. We should therefore consider a paired Wilcoxon signed rank test next. Remember that this test requires that the distribution of differences be of a similar shape, whereas our box plot from before suggested that the data were very much skewed.\nThis means that we’re not able to perform a paired Wilcoxon signed rank test either!\n\n\nConclusions\nSo, frustratingly, neither of the tests at our disposal are appropriate for this data set. The differences in fore leg and hind leg lengths are neither normal enough for a paired t-test nor are they symmetric enough for a Wilcoxon signed rank test. We also don’t have enough data to just use the t-test (we’d need more than 30 points or so). So what do we do in this situation? Well, the answer is that there aren’t actually any traditional statistical tests that are valid for this data set as it stands!\nThere are two options available to someone:\n\ntry transforming the raw data (take logs, square root, reciprocals) and hope that one of them leads to a modified data set that satisfies the assumptions of one of the tests we’ve covered, or\nuse a permutation test approach (which would work but is beyond the scope of this course).\n\nThe reason I included this example in the first practical is purely to illustrate how a very simple data set with an apparently clear message (leg lengths differ within deer) can be intractable. You don’t need to have very complex data sets before you go beyond the capabilities of classical statistics.\nAs Jeremy Clarkson would put it:\n\nAnd on that bombshell, it’s time to end. Goodnight!",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Paired data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples-paired.html#summary",
    "href": "materials/cs1_practical_two-samples-paired.html#summary",
    "title": "6  Paired data",
    "section": "6.8 Summary",
    "text": "6.8 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nPaired t-tests are used when you have two paired samples of continuous data, which are normally distributed\nA good way of assessing the assumption of normality is by checking the data against a Q-Q plot\nThe Wilcoxon signed rank test is used when you have two paired samples of continuous data, which are not normally distributed.",
    "crumbs": [
      "CS1: Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Paired data</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html",
    "href": "materials/cs2_practical_anova.html",
    "title": "7  ANOVA",
    "section": "",
    "text": "7.1 Purpose and aim\nAnalysis of variance or ANOVA is a test than can be used when we have multiple samples of continuous response data. Whilst it is possible to use ANOVA with only two samples, it is generally used when we have three or more groups. It is used to find out if the samples came from parent distributions with the same mean. It can be thought of as a generalisation of the two-sample Student’s t-test.",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html#libraries-and-functions",
    "href": "materials/cs2_practical_anova.html#libraries-and-functions",
    "title": "7  ANOVA",
    "section": "7.2 Libraries and functions",
    "text": "7.2 Libraries and functions\n\n\n\n\n\n\nNoteClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n7.2.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n\n\n7.2.2 Functions\n\n# Computes summary statistics\nrstatix::get_summary_stats()\n\n# Perform Tukey's range test\nrstatix::tukey_hsd()\n\n# Creates diagnostic plots\nggResidpanel::resid_panel()\n\n# Fits a linear model  \nstats::lm()\n\n# Carries out an ANOVA on a linear model \nstats::anova()\n\n# Performs a Shapiro-Wilk test for normality\nstats::shapiro.test()\n\n\n\n\n\n7.2.3 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n# Custom Python script for diagnostic plots, load from base working directory\nfrom dgplots import dgplots\n\n\n\n7.2.4 Functions\n\n# Summary statistics\npandas.DataFrame.describe()\n\n# Plots the first few rows of a DataFrame\npandas.DataFrame.head()\n\n# Query the columns of a DataFrame with a boolean expression\npandas.DataFrame.query()\n\n# Reads in a .csv file\npandas.read_csv\n\n# Performs an analysis of variance\npingouin.anova()\n\n# Tests for equality of variance\npingouin.homoscedasticity()\n\n# Performs the Shapiro-Wilk test for normality\npingouin.normality()\n\n# Creates a model from a formula and data frame\nstatsmodels.formula.api.ols\n\n# Creates an ANOVA table for one or more fitted linear models\nstatsmodels.stats.anova.anova_lm\n\n# Custom function to create diagnostic plots\ndgplots()\n\nNote: you can download the dgplots script here.",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html#data-and-hypotheses",
    "href": "materials/cs2_practical_anova.html#data-and-hypotheses",
    "title": "7  ANOVA",
    "section": "7.3 Data and hypotheses",
    "text": "7.3 Data and hypotheses\nFor example, suppose we measure the feeding rate of oyster catchers (shellfish per hour) at three sites characterised by their degree of shelter from the wind, imaginatively called exposed (E), partially sheltered (P) and sheltered (S). We want to test whether the data support the hypothesis that feeding rates don’t differ between locations. We form the following null and alternative hypotheses:\n\n\\(H_0\\): The mean feeding rates at all three sites is the same \\(\\mu E = \\mu P = \\mu S\\)\n\\(H_1\\): The mean feeding rates are not all equal.\n\nWe will use a one-way ANOVA test to check this.\n\nWe use a one-way ANOVA test because we only have one predictor variable (the categorical variable location).\nWe’re using ANOVA because we have more than two groups and we don’t know any better yet with respect to the exact assumptions.\n\nThe data are stored in the file data/CS2-oystercatcher-feeding.csv.",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html#summarise-and-visualise",
    "href": "materials/cs2_practical_anova.html#summarise-and-visualise",
    "title": "7  ANOVA",
    "section": "7.4 Summarise and visualise",
    "text": "7.4 Summarise and visualise\n\nRPython\n\n\nFirst we read in the data.\n\n# load data\noystercatcher &lt;- read_csv(\"data/CS2-oystercatcher-feeding.csv\")\n\n# and have a look\noystercatcher\n\n# A tibble: 120 × 2\n   site    feeding\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 exposed    12.2\n 2 exposed    13.1\n 3 exposed    17.9\n 4 exposed    13.9\n 5 exposed    14.1\n 6 exposed    18.4\n 7 exposed    15.0\n 8 exposed    10.3\n 9 exposed    11.8\n10 exposed    12.5\n# ℹ 110 more rows\n\n\nThe oystercatcher data set contains two columns:\n\na site column with information on the amount of shelter of the feeding location\na feeding column containing feeding rates\n\nNext, we get some basic descriptive statistics:\n\n# get some basic descriptive statistics\noystercatcher %&gt;% \n  group_by(site) %&gt;% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 3 × 11\n  site      variable     n   min   max median   iqr  mean    sd    se    ci\n  &lt;chr&gt;     &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 exposed   feeding     40  8.35  18.6   13.9  3.40  13.8  2.44 0.386 0.781\n2 partial   feeding     40 10.8   23.0   16.9  2.82  17.1  2.62 0.414 0.838\n3 sheltered feeding     40 18.9   28.5   23.2  3.79  23.4  2.42 0.383 0.774\n\n\nFinally, we plot the data by site:\n\n# plot the data\nggplot(oystercatcher,\n       aes(x = site, y = feeding)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nFirst, we read in the data.\n\n# load the data\noystercatcher_py = pd.read_csv(\"data/CS2-oystercatcher-feeding.csv\")\n\n# and have a look\noystercatcher_py.head()\n\n      site    feeding\n0  exposed  12.175506\n1  exposed  13.073917\n2  exposed  17.939687\n3  exposed  13.891783\n4  exposed  14.051663\n\n\nThe oystercatcher_py data set contains two columns:\n\na site column with information on the amount of shelter of the feeding location\na feeding column containing feeding rates\n\nNext, we get some basic descriptive statistics per group. Here we use the pd.groupby() function to group by site. We only want to have summary statistics for the feeding variable, so we specify that as well:\n\noystercatcher_py.groupby(\"site\")[\"feeding\"].describe()\n\n           count       mean       std  ...        50%        75%        max\nsite                                   ...                                 \nexposed     40.0  13.822899  2.441974  ...  13.946420  15.581748  18.560404\npartial     40.0  17.081666  2.619906  ...  16.927683  18.416708  23.021250\nsheltered   40.0  23.355503  2.419825  ...  23.166246  25.197096  28.451252\n\n[3 rows x 8 columns]\n\n\nFinally, we plot the data:\n\n# plot the data\np = (ggplot(oystercatcher_py,\n        aes(x = \"site\",\n            y = \"feeding\")) +\n     geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nLooking at the data, there appears to be a noticeable difference in feeding rates between the three sites. We would probably expect a reasonably significant statistical result here.",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html#assumptions",
    "href": "materials/cs2_practical_anova.html#assumptions",
    "title": "7  ANOVA",
    "section": "7.5 Assumptions",
    "text": "7.5 Assumptions\nTo use an ANOVA test, we have to make three assumptions:\n\nThe parent distributions from which the samples are taken are normally distributed\nEach data point in the samples is independent of the others\nThe parent distributions should have the same variance\n\nIn a similar way to the two-sample tests we will consider the normality and equality of variance assumptions both using tests and by graphical inspection (and ignore the independence assumption).\n\n7.5.1 Normality\nFirst we perform a Shapiro-Wilk test on each site separately.\n\nRPython\n\n\nWe take the data, filter for each type of site, extract the feeding rates and send those data to the shapiro.test() function.\n\n# Shapiro-Wilk test on each site\noystercatcher %&gt;% \n    filter(site == \"exposed\") %&gt;% \n    pull(feeding) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.98859, p-value = 0.953\n\noystercatcher %&gt;% \n    filter(site == \"partial\") %&gt;% \n    pull(feeding) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.98791, p-value = 0.9398\n\noystercatcher %&gt;% \n    filter(site == \"sheltered\") %&gt;% \n    pull(feeding) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.97511, p-value = 0.5136\n\n\n\n\nWe use the pg.normality() function to calculate the statistic. This requires:\n\nthe dv dependent variable (feeding in our case)\nthe group variable (site)\nand some data\n\n\npg.normality(dv = \"feeding\",\n             group = \"site\",\n             data = oystercatcher_py)\n\n                  W      pval  normal\nsite                                 \nexposed    0.988586  0.953033    True\npartial    0.987907  0.939829    True\nsheltered  0.975106  0.513551    True\n\n\n\n\n\nWe can see that all three groups appear to be normally distributed which is good.\n\n\n\n\n\n\nImportant\n\n\n\nFor ANOVA however, considering each group in turn is often considered quite excessive and, in most cases, it is sufficient to consider the normality of the combined set of residuals from the data. We’ll explain residuals properly in the next session, but effectively they are the difference between each data point and its group mean.\nTo get hold of these residuals, we need to create a linear model. Again, this will be explained in more detail in the next section. For now, we see it as a way to describe the relationship between the feeding rate and the site.\n\n\nSo, we create a linear model, extract the residuals and check their normality:\n\nRPython\n\n\nWe use the lm() function to define the linear model that describes the relationship between feeding and site. The notation is similar to what we used previously when we were dealing with two samples of data.\n\n# define the model\nlm_oystercatcher &lt;- lm(feeding ~ site,\n                       data = oystercatcher)\n\nWe can read this as “create a linear model (lm) where the feeding rate (feeding) depends on the site (site), using the oystercatcher data”.\nWe store the output of that in an object called lm_oystercatcher. We’ll look into what this object contains in more detail in later sections.\nFor now, we extract the residuals from this object using the residuals() function and then use this in the shapiro.test() function.\n\n# extract the residuals\nresid_oyster &lt;- residuals(lm_oystercatcher)\n\n# perform Shapiro-Wilk test on residuals\nshapiro.test(resid_oyster)\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid_oyster\nW = 0.99355, p-value = 0.8571\n\n\n\n\nUnfortunately pingouin does not have a straightforward way of extracting residuals (if you know more, please let me know!).\nTo get our residuals we use statsmodels, a module that provides functions for statistical models. We’ll be using this in upcoming sessions, so you’ll have a head start!\nAt this point you shouldn’t concern yourself too much with the exact syntax, just run it an have a look.\n\n\n\n\n\n\nNoteTechnical details (optional)\n\n\n\n\n\nWe need to import a few extra modules. First, we load the statsmodels.api module, which contains an OLS() function (Ordinary Least Squares - the equivalent of the lm() function in R).\nWe also import stats.models.formula.api so we can use the formula notation in our linear model. We define the formula as formula = \"feeding ~ C(site)\" with C conveying that the site variable is a category. Lastly we can .fit() the model.\nIf you’re familiar with this stuff then you can look at the model itself by running summary(lm_oystercatcher_py). But we’ll cover all of this in later sessions.\n\n\n\nWe load the modules, define a linear model, create a fit() and we get the residuals from the linear model fit with .resid.\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\n# create a linear model\nmodel = smf.ols(formula = \"feeding ~ C(site)\", data = oystercatcher_py)\n# and get the fitted parameters of the model\nlm_oystercatcher_py = model.fit()\n\n\n# get the residuals from the model fit\n# and perform Shapiro-Wilk test\npg.normality(lm_oystercatcher_py.resid)\n\n          W     pval  normal\n0  0.993546  0.85707    True\n\n\n\n\n\nAgain, we can see that the combined residuals from all three groups appear to be normally distributed (which is as we would have expected given that they were all normally distributed individually!)\n\n\n7.5.2 Equality of variance\nWe now test for equality of variance using Bartlett’s test (since we’ve just found that all of the individual groups are normally distributed).\nPerform Bartlett’s test on the data:\n\nRPython\n\n\n\n# check equality of variance\nbartlett.test(feeding ~ site,\n              data = oystercatcher)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  feeding by site\nBartlett's K-squared = 0.29598, df = 2, p-value = 0.8624\n\n\nWhere the relevant p-value is given on the 3rd line. Here we see that each group appears to have comparable variance.\n\n\nWe use the homoscedasticity() function from pingouin (homoscedasticity is another way of describing equality of variance). The default method is levene, so we need to specify that we want to use bartlett.\n\npg.homoscedasticity(dv = \"feeding\",\n                    group = \"site\",\n                    method = \"bartlett\",\n                    data = oystercatcher_py)\n\n                 T      pval  equal_var\nbartlett  0.295983  0.862439       True\n\n\nWhere the relevant p-value is given in the pval column. Here we see that each group appears to have the same variance.\n\n\n\n\n\n7.5.3 Graphical interpretation and diagnostic plots\nAssessing assumptions via these tests can be cumbersome, but also a bit misleading at times. It reduces the answer to the question “is the assumption met?” to a yes/no, based on some statistic and associated p-value.\nThis does not convey that things aren’t always so clear-cut and that there is a lot of grey area that we need to navigate. As such, assessing assumptions through graphical means - using diagnostic plots - is often preferred.\n\nRPython\n\n\nIn the first session we already created diagnostic Q-Q plots directly from our data, using stat_qq() and stat_qq_line(). For more specific plots this becomes a bit cumbersome. There is an option to create ggplot-friendly diagnostic plots, using the ggResidPanel package.\nIf you haven’t got ggResidpanel installed, please run the following code:\n\n# install package\ninstall.packages(\"ggResidpanel\")\n\n# load library\nlibrary(ggResidpanel)\n\nLet’s create the diagnostic plots we’re interested in using ggResidPanel. It takes a linear model object as input (lm_oystercatcher) and you can define which plots you want to see using the plots = argument. I have also added a smoother line (smoother = TRUE) to the plots, which we’ll use to compare against.\n\nresid_panel(lm_oystercatcher,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\nThe top left graph plots the Residuals plot. If the data are best explained by a linear line then the points should be uniformly distributed above and below the horizontal blue line. If that’s the case then the red line (a smoother line) should overlay the blue line. This plot looks pretty good.\nThe top right graph shows the Q-Q plot which allows a visual inspection of normality. If the residuals are normally distributed, then the points should lie on the diagonal blue line. This plot looks good.\nThe bottom left Location-Scale graph allows us to investigate whether there is any correlation between the residuals and the predicted values and whether the variance of the residuals changes significantly. If not, then the red line should be horizontal. If there is any correlation or change in variance then the red line will not be horizontal. This plot is fine.\nThe last graph shows the Cook’s distance and tests if any one point has an unnecessarily large effect on the fit. A rule of thumb is that if any value is larger than 1.0, then it might have a large effect on the model. If not, then no point has undue influence. This plot is good. There are different ways to determine the threshold (apart from simply setting it to 1) and in this plot the blue dashed line is at 4/n, with n being the number of samples. At this threshold there are some data points that may be influential, but I personally find this threshold rather strict.\n\n\n\nUnfortunately Python doesn’t provide a convenient way of displaying the same diagnostic plots as R does.\nI created a function dgplots() (which stands for Diagnostic Plots - very original I know…) that does this for you. All you need to do is create a linear model, get the fit and feed that to the dgplots() function.\nYou can find the script for this here.\nYou can of course plot the model values yourself by extracting them from the linear model fit, but this should provide a convenient way to avoid that kind of stuff.\n\ndgplots(lm_oystercatcher_py)\n\n\n\n\n\nThe top left graph plots the Residuals plot. If the data are best explained by a linear line then the points should be uniformly distributed above and below the horizontal blue line. If that’s the case then the red line (a smoother line) should overlay the blue line. This plot looks pretty good.\nThe top right graph shows the Q-Q plot which allows a visual inspection of normality. If the residuals are normally distributed, then the points should lie on the diagonal blue line. This plot looks good.\nThe bottom left Location-Scale graph allows us to investigate whether there is any correlation between the residuals and the predicted values and whether the variance of the residuals changes significantly. If not, then the red line should be horizontal. If there is any correlation or change in variance then the red line will not be horizontal. This plot is fine.\nThe last graph shows the Influential points and tests if any one point has an unnecessarily large effect on the fit. Here we’re using the Cook’s distance as a measure. A rule of thumb is that if any value is larger than 1.0, then it might have a large effect on the model. If not, then no point has undue influence. This plot is good. There are different ways to determine the threshold (apart from simply setting it to 1) and in this plot the blue dashed line is at 4/n, with n being the number of samples. At this threshold there are some data points that may be influential, but I personally find this threshold rather strict.\n\n\n\n\nWe can see that these graphs are very much in line with what we’ve just looked at using the test, which is reassuring. The groups all appear to have the same spread of data, and the Q-Q plot shows that the assumption of normality is alright.\n\n\n\n\n\n\nImportantAssessing assumptions\n\n\n\nAt this stage, I should point out that I nearly always stick with the graphical method for assessing the assumptions of a test. Assumptions are rarely either completely met or not met and there is always some degree of personal assessment.\nWhilst the formal statistical tests (like Shapiro-Wilk) are technically fine, they can often create a false sense of things being absolutely right or wrong in spite of the fact that they themselves are still probabilistic statistical tests. In these exercises we are using both approaches whilst you gain confidence and experience in interpreting the graphical output and whilst it is absolutely fine to use both in the future I would strongly recommend that you don’t rely solely on the statistical tests in isolation.",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html#implement-and-interpret-the-test",
    "href": "materials/cs2_practical_anova.html#implement-and-interpret-the-test",
    "title": "7  ANOVA",
    "section": "7.6 Implement and interpret the test",
    "text": "7.6 Implement and interpret the test\nAs is often the case, performing the actual statistical test is the least of your efforts.\n\nRPython\n\n\nIn R we perform the ANOVA on the linear model object, lm_oystercatcher in this case. We do this with the anova() function:\n\nanova(lm_oystercatcher)\n\nAnalysis of Variance Table\n\nResponse: feeding\n           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nsite        2 1878.02  939.01  150.78 &lt; 2.2e-16 ***\nResiduals 117  728.63    6.23                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis takes the linear model (i.e. finds the means of the three groups and calculates a load of intermediary data that we need for the statistical analysis) and stores this information in an R object (which we’ve called lm_oystercatcher, but which you can call what you like).\nIn the output:\n\nThe 1st line just tells you the that this is an ANOVA test\nThe 2nd line tells you what the response variable is (in this case feeding)\nThe 3rd, 4th and 5th lines are an ANOVA table which contain some useful values:\n\nThe Df column contains the degrees of freedom values on each row, 2 and 117 (which we can use for the reporting)\nThe F value column contains the F statistic, 150.78 (which again we’ll need for reporting).\nThe p-value is 2.2e-16 and is the number directly under the Pr(&gt;F) on the 4th line (to be precise, it is 4.13e-33 but anything smaller than 2.2e-16 gets reported as &lt; 2.2e-16).\nThe other values in the table (in the Sum Sq and Mean Sq) columns are used to calculate the F statistic itself and we don’t need to know these.\n\nThe 6th line has some symbolic codes to represent how big (small) the p-value is; so, a p-value smaller than 0.001 would have a *** symbol next to it (which ours does). Whereas if the p-value was between 0.01 and 0.05 then there would simply be a * character next to it, etc. Thankfully we can all cope with actual numbers and don’t need a short-hand code to determine the reporting of our experiments (please tell me that’s true…!)\n\n\n\nThere are different ways of conducting an ANOVA in Python, with scipy.stats proving an option. However, I find that the anova() function in pingouin provides the easiest and most-detailed option to do this.\nIt takes the following arguments:\n\ndv: dependent variable (response variable; in our case feeding)\nbetween: between-subject factor (predictor variable; in our case site)\ndata: which function doesn’t!?\ndetailed: optional True or False, we’re setting it to True because we like to know what we’re doing!\n\n\npg.anova(dv = \"feeding\",\n         between = \"site\",\n         data = oystercatcher_py,\n         detailed = True)\n\n   Source           SS   DF          MS           F         p-unc       np2\n0    site  1878.015371    2  939.007685  150.782449  4.128088e-33  0.720473\n1  Within   728.625249  117    6.227566         NaN           NaN       NaN\n\n\nThis creates a linear model based on the data, i.e. finds the means of the three groups and calculates a load of intermediary data that we need for the statistical analysis.\nIn the output:\n\nSource: Factor names - in our case these are the different sites (site)\nSS: Sums of squares (we’ll get to that in a bit)\nDF: Degrees of freedom (at the moment only used for reporting)\nMS: Mean squares\nF: Our F-statistic\np-unc: p-value (unc stands for “uncorrected” - more on multiple testing correction later)\nnp2: Partial eta-square effect sizes (more on this later)\n\nAlternatively, and we’ll be using this method later on in the course, you can perform an ANOVA on the lm_oystercatcher_py object we created earlier.\nThis uses the sm.stats.anova_lm() function from statsmodels. As you’ll see, the output is very similar:\n\nsm.stats.anova_lm(lm_oystercatcher_py)\n\n             df       sum_sq     mean_sq           F        PR(&gt;F)\nC(site)     2.0  1878.015371  939.007685  150.782449  4.128088e-33\nResidual  117.0   728.625249    6.227566         NaN           NaN\n\n\n\n\n\nAgain, the p-value is what we’re most interested in here and shows us the probability of getting samples such as ours if the null hypothesis were actually true.\nSince the p-value is very small (much smaller than the standard significance level of 0.05) we can say “that it is very unlikely that these three samples came from the same parent distribution” and as such we can reject our null hypothesis and state that:\n\nA one-way ANOVA showed that the mean feeding rate of oystercatchers differed significantly between locations (p = 4.13e-33).",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html#post-hoc-testing-tukeys-range-test",
    "href": "materials/cs2_practical_anova.html#post-hoc-testing-tukeys-range-test",
    "title": "7  ANOVA",
    "section": "7.7 Post-hoc testing (Tukey’s range test)",
    "text": "7.7 Post-hoc testing (Tukey’s range test)\nOne drawback with using an ANOVA test is that it only tests to see if all of the means are the same. If we get a significant result using ANOVA then all we can say is that not all of the means are the same, rather than anything about how the pairs of groups differ. For example, consider the following box plot for three samples.\n\n\n\n\n\n\n\n\n\nEach group is a random sample of 20 points from a normal distribution with variance 1. Groups 1 and 2 come from a parent population with mean 0 whereas group 3 come from a parent population with mean 2. The data clearly satisfy the assumptions of an ANOVA test.\nHow do we find out if there are any differences between these groups and, if so, which groups are different from each other?\n\n7.7.1 Read in data and plot\n\nRPython\n\n\n\n# load the data\ntukey &lt;- read_csv(\"data/CS2-tukey.csv\")\n\nRows: 60 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): group\ndbl (1): response\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# have a look at the data\ntukey\n\n# A tibble: 60 × 2\n   response group  \n      &lt;dbl&gt; &lt;chr&gt;  \n 1    1.58  sample1\n 2    0.380 sample1\n 3   -0.997 sample1\n 4   -0.771 sample1\n 5    0.169 sample1\n 6   -0.698 sample1\n 7   -0.167 sample1\n 8    1.38  sample1\n 9   -0.839 sample1\n10   -1.05  sample1\n# ℹ 50 more rows\n\n\n\n# plot the data\nggplot(tukey,\n       aes(x = group, y = response)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n# load the data\ntukey_py = pd.read_csv(\"data/CS2-tukey.csv\")\n\n# have a look at the data\ntukey_py.head()\n\n   response    group\n0  1.580048  sample1\n1  0.379544  sample1\n2 -0.996505  sample1\n3 -0.770799  sample1\n4  0.169046  sample1\n\n\n\n# plot the data\np = (ggplot(tukey_py,\n        aes(x = \"group\",\n            y = \"response\")) +\n     geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.7.2 Test for a significant difference in group means\n\nRPython\n\n\n\n# create a linear model\nlm_tukey &lt;- lm(response ~ group,\n               data = tukey)\n\n# perform an ANOVA\nanova(lm_tukey)\n\nAnalysis of Variance Table\n\nResponse: response\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ngroup      2 33.850 16.9250   20.16 2.392e-07 ***\nResiduals 57 47.854  0.8395                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\npg.anova(dv = \"response\",\n         between = \"group\",\n         data = tukey_py,\n         detailed = True)\n\n   Source         SS  DF         MS          F         p-unc       np2\n0   group  33.850044   2  16.925022  20.159922  2.391626e-07  0.414302\n1  Within  47.853670  57   0.839538        NaN           NaN       NaN\n\n\n\n\n\nHere we have a p-value of 2.39 \\(\\times\\) 10-7 and so the test has very conclusively rejected the hypothesis that all means are equal.\nHowever, this was not due to all of the sample means being different, but rather just because one of the groups is very different from the others. In order to drill down and investigate this further we use a new test called Tukey’s range test (or Tukey’s honest significant difference test – this always makes me think of some terrible cowboy/western dialogue).\nThis will compare all of the groups in a pairwise fashion and reports on whether a significant difference exists.\n\n\n7.7.3 Performing Tukey’s test\n\nRPython\n\n\nTo perform Tukey’s range test we can use the tukey_hsd() function from the rstatix package. Note, there is a TukeyHSD() function in base R as well, but the tukey_hsd() function can take a linear model object as input, whereas the TukeyHSD() function cannot. This makes the tukey_hsd() function a bit easier to work with.\n\n# perform Tukey's range test on linear model\ntukey_hsd(lm_tukey)\n\n# A tibble: 3 × 9\n  term  group1  group2  null.value estimate conf.low conf.high       p.adj\n* &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 group sample1 sample2          0    0.304   -0.393      1.00 0.55       \n2 group sample1 sample3          0    1.72     1.03       2.42 0.000000522\n3 group sample2 sample3          0    1.42     0.722      2.12 0.0000246  \n# ℹ 1 more variable: p.adj.signif &lt;chr&gt;\n\n\nThe tukey_hsd() function takes our linear model (lm_tukey) as its input. The output is a pair-by-pair comparison between the different groups (samples 1 to 3). We are interested in the p.adj column, which gives us the adjusted p-value. The null hypothesis in each case is that there is no difference in the mean between the two groups.\n\n\n\npg.pairwise_tukey(dv = \"response\",\n                  between = \"group\",\n                  data = tukey_py).transpose()\n\n                0         1         2\nA         sample1   sample1   sample2\nB         sample2   sample3   sample3\nmean(A) -0.049878 -0.049878  0.253878\nmean(B)  0.253878  1.673481  1.673481\ndiff    -0.303756 -1.723359 -1.419603\nse       0.289748  0.289748  0.289748\nT       -1.048347 -5.947789 -4.899442\np-tukey  0.549801  0.000001  0.000025\nhedges  -0.353155 -1.746521 -1.489547\n\n\nThe dv argument is the response variable, whereas the between argument defines the explanatory variable.\nWe .transpose() the data, so we can look at the output a bit easier. Doing so, we focus on the p-tukey values.\n\n\n\nAs we can see that there isn’t a significant difference between sample1 and sample2 but there is a significant difference between sample1 and sample3, as well as sample2 and sample3. This matches with what we expected based on the box plot.\n\n\n7.7.4 Assumptions\nWhen to use Tukey’s range test is a matter of debate (strangely enough a lot of statistical analysis techniques are currently matters of opinion rather than mathematical fact – it does explain a little why this whole field appears so bloody confusing!)\n\nSome people claim that we should only perform Tukey’s range test (or any other post-hoc tests) if the preceding ANOVA test showed that there was a significant difference between the groups and that if the ANOVA test had not shown any significant differences between groups then we would have to stop there.\nOther people say that this is rubbish and we can do whatever we like as long as we tell people what we did.\n\nThe background to this is rather involved but one of the reasons for this debate is to prevent so-called data-dredging or p-hacking. This is where scientists/analysts are so fixated on getting a “significant” result that they perform a huge variety of statistical techniques until they find one that shows that their data is significant (this was a particular problem in psychological studies for while – not to point fingers though, they are working hard to sort their stuff out. Kudos!).\nWhether you should use post-hoc testing or not will depend on your experimental design and the questions that you’re attempting to answer.\nTukey’s range test, when we decide to use it, requires the same three assumptions as an ANOVA test:\n\nNormality of distributions\nEquality of variance between groups\nIndependence of observations",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html#exercises",
    "href": "materials/cs2_practical_anova.html#exercises",
    "title": "7  ANOVA",
    "section": "7.8 Exercises",
    "text": "7.8 Exercises\n\n\n\n\n\n\nExerciseExercise 1 - Lobster weight\n\n\n\n\n\n\nLevel: \nJuvenile lobsters in aquaculture were grown on three different diets (fresh mussels, semi-dry pellets and dry flakes). After nine weeks, their wet weight was:\n\n\n# A tibble: 7 × 3\n  Mussels Pellets Flakes\n    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1    152.    118.  102. \n2    132.    111.  103. \n3    104.    129.   90.4\n4    154.    110.  133. \n5    132     175.  129. \n6    119      NA   129. \n7    162.     NA    NA  \n\n\nIs there any evidence that diet affects the growth rate of lobsters?\n\nWrite down the null and alternative hypotheses\nImport the data from data/CS2-lobsters.csv\nSummarise and visualise the data\nCheck the assumptions using appropriate tests and graphical analyses\nPerform an ANOVA\nWrite down a sentence that summarises the results that you have found\nPerform a post-hoc test and report the findings\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nHypotheses\n\\(H_0\\) : all means are equal\n\\(H_1\\) : not all means are equal\n\n\nImport Data, summarise and visualise\n\nRPython\n\n\n\n# load the data\nlobsters &lt;- read_csv(\"data/CS2-lobsters.csv\")\n\n# look at the data\nlobsters\n\n# A tibble: 18 × 3\n      id weight diet   \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  \n 1     1  152.  Mussels\n 2     2  132.  Mussels\n 3     3  104.  Mussels\n 4     4  154.  Mussels\n 5     5  132   Mussels\n 6     6  119   Mussels\n 7     7  162.  Mussels\n 8     8  118.  Pellets\n 9     9  111.  Pellets\n10    10  129.  Pellets\n11    11  110.  Pellets\n12    12  175.  Pellets\n13    13  102.  Flakes \n14    14  103.  Flakes \n15    15   90.4 Flakes \n16    16  133.  Flakes \n17    17  129.  Flakes \n18    18  129.  Flakes \n\n\nThe data have a unique id column, which we don’t need any summary statistics for, so we deselect it:\n\n# create some summary statistics\nlobsters %&gt;% \n  select(-id) %&gt;% \n  group_by(diet) %&gt;% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 3 × 11\n  diet    variable     n   min   max median   iqr  mean    sd    se    ci\n  &lt;chr&gt;   &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Flakes  weight       6  90.4  133.   116.  27.3  114.  18.2  7.42  19.1\n2 Mussels weight       7 104.   162.   132.  27.0  136.  20.6  7.79  19.1\n3 Pellets weight       5 110.   175.   118.  17.8  128.  27.2 12.1   33.7\n\n\nNext, we visualise the data:\n\nlobsters %&gt;% \n  ggplot(aes(x = diet, y = weight)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nlobsters_py = pd.read_csv(\"data/CS2-lobsters.csv\")\n\nNext, we visualise the data:\n\np = (ggplot(lobsters_py,\n        aes(x = \"diet\",\n            y = \"weight\")) +\n     geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nAs always we use the plot and summary to assess three things:\n\nDid we load the data in properly?\n\n\nWe see three groups with reasonable values. There aren’t any data points that are obviously wrong (negative, zero or massively big) and we have the right number of groups. So it looks as if we didn’t do anything obviously wrong.\n\n\nWhat do we expect as a result of a statistical test?\n\n\nWhilst the Mussels group does look higher than the other two groups, Pellets and Flakes appear almost identical in terms of average values, and there’s quite a bit of overlap with the Mussels group. A non-significant result is the most likely answer, and I would be surprised to see a significant p-value - especially given the small sample size that we have here.\n\n\nWhat do we think about assumptions?\n\n\nThe groups appear mainly symmetric (although Pellets is a bit weird) and so we’re not immediately massively worried about lack of normality. Again, Flakes and Mussels appear to have very similar variances but it’s a bit hard to decide what’s going on with Pellets. It’s hard to say what’s going on with the assumptions and so I’ll wait and see what the other tests say.\n\n\n\nExplore Assumptions\nWe’ll explore the assumption of normality and equality of variance, assuming that the data are independent.\n\nRPython\n\n\nNormality\nWe’ll be really thorough here and consider the normality of each group separately and jointly using the Shapiro-Wilk test, as well as looking at the Q-Q plot. In reality, and after these examples , we’ll only use the Q-Q plot.\nFirst, we perform the Shapiro-Wilk test on the individual groups:\n\n# Shapiro-Wilk on lobster groups\nlobsters %&gt;% \n    filter(diet == \"Flakes\") %&gt;% \n    pull(weight) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.84368, p-value = 0.1398\n\nlobsters %&gt;% \n    filter(diet == \"Mussels\") %&gt;% \n    pull(weight) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.94784, p-value = 0.71\n\nlobsters %&gt;% \n    filter(diet == \"Pellets\") %&gt;% \n    pull(weight) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.76706, p-value = 0.0425\n\n\nFlakes and Mussels are fine, but, as we suspected from earlier, Pellets appears to have a marginally significant Normality test result.\nLet’s look at the Shapiro-Wilk test for all of the data together:\n\n# create a linear model\nlm_lobsters &lt;- lm(weight ~ diet,\n                  data = lobsters)\n\n# extract the residuals\nresid_lobsters &lt;- residuals(lm_lobsters)\n\n# and perform the Shapiro-Wilk test on the residuals\nresid_lobsters %&gt;% \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.94779, p-value = 0.3914\n\n\nThis on the other hand says that everything is fine. Let’s look at Q-Q plot.\n\n# Q-Q plots\nresid_panel(lm_lobsters,\n            plots = \"qq\")\n\n\n\n\n\n\n\n\nHere, I’ve used an extra argument to the normal diagnostic plots call. The default option is to plot 4 diagnostic plots. You can tell resid_panel() to only plot a specific one, using the plots = arguments. If you want to know more about this have a look at the help documentation or by using ?resid_panel.\nThe Q-Q plot looks OK, not perfect, but more than good enough for us to have confidence in the normality of the data.\nOverall, I’d be happy that the assumption of normality has been adequately well met here. The suggested lack of normality in the Pellets was only just significant and we have to take into account that there are only 5 data points in that group. If there had been a lot more points in that group, or if the Q-Q plot was considerably worse then I wouldn’t be confident.\nEquality of Variance\nWe’ll consider the Bartlett test and we’ll look at some diagnostic plots too.\n\n# perform Bartlett's test\nbartlett.test(weight ~ diet,\n              data = lobsters)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  weight by diet\nBartlett's K-squared = 0.71273, df = 2, p-value = 0.7002\n\n# plot the residuals and scale-location plots\nresid_panel(lm_lobsters,\n            plots = c(\"resid\", \"ls\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\nIn the above code I’ve specified which diagnostic plots I wanted. I have also added a smoother line (smoother = TRUE) to the plots.\n\nThe Residuals Plot. What we’re looking for there is that the points are evenly spread on either side of the line. Looks good.\nThe Location-Scale Plot (this is displayed by default in base R’s diagnostic plots). Here we’re looking at the red line. If that line is more or less horizontal, then the equality of variance assumption is met.\n\nHere all three methods agree that there aren’t any issues with equality of variance:\n\nthe Bartlett test p-value is large and non-significant\nthe spread of points in all three groups in the residuals vs fitted graph are roughly the same\nthe red line in the scale-location graph is pretty horizontal\n\nOverall, this assumption is pretty well met.\n\n\nNormality\nWe’ll be really thorough here and consider the normality of each group separately and jointly using the Shapiro-Wilk test, as well as looking at the Q-Q plot. In reality, and after these examples , we’ll only use the Q-Q plot.\nFirst, we perform the Shapiro-Wilk test on the individual groups:\n\n# Shapiro-Wilk on lobster groups\npg.normality(dv = \"weight\",\n             group = \"diet\",\n             data = lobsters_py)\n\n                W      pval  normal\ndiet                               \nMussels  0.947836  0.709968    True\nPellets  0.767059  0.042495   False\nFlakes   0.843678  0.139796    True\n\n\nFlakes and Mussels are fine, but, as we suspected from earlier, Pellets appears to have a marginally significant Normality test result.\nLet’s look at the Shapiro-Wilk test for all of the data together:\n\n# create a linear model\nmodel = smf.ols(formula = \"weight ~ C(diet)\", data = lobsters_py)\n# and get the fitted parameters of the model\nlm_lobsters_py = model.fit()\n\n\n# get the residuals from the model fit\n# and perform Shapiro-Wilk test\npg.normality(lm_lobsters_py.resid)\n\n          W      pval  normal\n0  0.947787  0.391366    True\n\n\nThis on the other hand says that everything is fine. Let’s look at Q-Q plot.\n\n# Q-Q plots\np = (ggplot(lobsters_py,\n        aes(sample = \"weight\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"red\"))\n\np.show()\n\n\n\n\n\n\n\n\nThe Q-Q plot looks OK, not perfect, but more than good enough for us to have confidence in the normality of the data.\nOverall, I’d be happy that the assumption of normality has been adequately well met here. The suggested lack of normality in the Pellets was only just significant and we have to take into account that there are only 5 data points in that group. If there had been a lot more points in that group, or if the Q-Q plot was considerably worse then I wouldn’t be confident.\nEquality of Variance\nWe’ll consider the Bartlett test and we’ll look at the diagnostic plots too.\n\npg.homoscedasticity(dv = \"weight\",\n                    group = \"diet\",\n                    method = \"bartlett\",\n                    data = lobsters_py)\n\n                 T      pval  equal_var\nbartlett  0.712733  0.700216       True\n\n\n\ndgplots(lm_lobsters_py)\n\n\n\n\nWe’ll just focus on the following:\n\nThe Residuals Plot. What we’re looking for there is that the points are evenly spread on either side of the line. Looks good.\nThe Location-Scale Plot (this is displayed by default in base R’s diagnostic plots). Here we’re looking at the red line. If that line is more or less horizontal, then the equality of variance assumption is met.\n\nHere all three methods agree that there aren’t any issues with equality of variance:\n\nthe Bartlett test p-value is large and non-significant\nthe spread of points in all three groups in the residuals vs fitted graph are roughly the same\nthe red line in the scale-location graph is pretty horizontal\n\nOverall, this assumption is pretty well met.\n\n\n\n\n\nCarry out one-way ANOVA\nWith our assumptions of normality and equality of variance met we can be confident that a one-way ANOVA is an appropriate test.\n\nRPython\n\n\n\nanova(lm_lobsters)\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\ndiet       2 1567.2  783.61  1.6432 0.2263\nResiduals 15 7153.1  476.87               \n\n\n\n\n\npg.anova(dv = \"weight\",\n         between = \"diet\",\n         data = lobsters_py,\n         detailed = True)\n\n   Source           SS  DF          MS        F     p-unc       np2\n0    diet  1567.229381   2  783.614690  1.64324  0.226313  0.179722\n1  Within  7153.075619  15  476.871708      NaN       NaN       NaN\n\n\n\n\n\n\nA one-way ANOVA test indicated that the mean weight of juvenile lobsters did not differ significantly between diets (p = 0.23).\n\n\n\nPost-hoc testing with Tukey\nIn this case we did not find any significant differences between the different diets. So that is a good time for me to reiterate that carrying out the post-hoc test after getting a non-significant result with ANOVA is something that you have to think very carefully about and it all depends on what your research question it.\nIf your research question was:\n\nDoes diet affect lobster weight?\n\nor\n\nIs there any effect of diet on lobster weight?\n\nThen when we got the non-significant result from the ANOVA test we should have just stopped there as we have our answer. Going digging for “significant” results by running more tests is a main factor that contributes towards lack of reproducibility in research.\nIf on the other hand your research question was:\n\nAre any specific diets better or worse for lobster weight than others?\n\nThen we should probably have just skipped the one-way ANOVA test entirely and just jumped straight in with the Tukey’s range test. The important point here is that the result of the one-way ANOVA test doesn’t stop you from carrying out the Tukey test - but it’s up to you to decide whether it is sensible.",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html#summary",
    "href": "materials/cs2_practical_anova.html#summary",
    "title": "7  ANOVA",
    "section": "7.9 Summary",
    "text": "7.9 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nWe use an ANOVA to test if there is a difference in means between multiple continuous response variables\nWe check assumptions with diagnostic plots and check if the residuals are normally distributed\nWe use post-hoc testing to check for significant differences between the group means, for example using Tukey’s range test",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html",
    "href": "materials/cs2_practical_kruskal-wallis.html",
    "title": "8  Kruskal-Wallis",
    "section": "",
    "text": "8.1 Purpose and aim\nThe Kruskal-Wallis one-way analysis of variance test is an analogue of ANOVA that can be used when the assumption of normality cannot be met. In this way it is an extension of the Mann-Whitney test for two groups.",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html#libraries-and-functions",
    "href": "materials/cs2_practical_kruskal-wallis.html#libraries-and-functions",
    "title": "8  Kruskal-Wallis",
    "section": "8.2 Libraries and functions",
    "text": "8.2 Libraries and functions\n\n\n\n\n\n\nNoteClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n8.2.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n\n\n8.2.2 Functions\n\n# Performs a Kruskal-Wallis test\nstats::kruskal.test()\n\n# Performs Dunn's test for pairwise multiple comparisons of the ranked data\nrstatix::dunn_test()\n\n\n\n\n\n8.2.3 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Post-hoc tests\nimport scikit_posthocs as sp\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n\n\n8.2.4 Functions\n\n# Summary statistics\npandas.DataFrame.describe()\n\n# Plots the first few rows of a DataFrame\npandas.DataFrame.head()\n\n# Reads in a .csv file\npandas.DataFrame.read_csv()\n\n# Performs an analysis of variance\n#pingouin.anova()\n\n# Tests for equality of variance\npingouin.homoscedasticity()\n\n# Performs the Kruskal-Wallis test\npingouin.kruskal\n\n# Performs the Shapiro-Wilk test for normality\npingouin.normality()\n\n# Creates a model from a formula and data frame\nstatsmodels.formula.api.ols\n\n# Creates an ANOVA table for one or more fitted linear models\nstatsmodels.stats.anova.anova_lm",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html#data-and-hypotheses",
    "href": "materials/cs2_practical_kruskal-wallis.html#data-and-hypotheses",
    "title": "8  Kruskal-Wallis",
    "section": "8.3 Data and hypotheses",
    "text": "8.3 Data and hypotheses\nFor example, suppose a behavioural ecologist records the rate at which spider monkeys behaved aggressively towards one another, as a function of how closely related the monkeys are. The familiarity of the two monkeys involved in each interaction is classified as high, low or none. We want to test if the data support the hypothesis that aggression rates differ according to strength of relatedness. We form the following null and alternative hypotheses:\n\n\\(H_0\\): The median aggression rates for all types of familiarity are the same\n\\(H_1\\): The median aggression rates are not all equal\n\nWe will use a Kruskal-Wallis test to check this.\nThe data are stored in the file data/CS2-spidermonkey.csv.",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html#summarise-and-visualise",
    "href": "materials/cs2_practical_kruskal-wallis.html#summarise-and-visualise",
    "title": "8  Kruskal-Wallis",
    "section": "8.4 Summarise and visualise",
    "text": "8.4 Summarise and visualise\n\nRPython\n\n\nFirst we read the data in:\n\nspidermonkey &lt;- read_csv(\"data/CS2-spidermonkey.csv\")\n\n\n# look at the data\nspidermonkey\n\n# A tibble: 21 × 3\n      id aggression familiarity\n   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;      \n 1     1        0.2 high       \n 2     2        0.1 high       \n 3     3        0.4 high       \n 4     4        0.8 high       \n 5     5        0.3 high       \n 6     6        0.5 high       \n 7     7        0.2 high       \n 8     8        0.5 low        \n 9     9        0.4 low        \n10    10        0.3 low        \n# ℹ 11 more rows\n\n# summarise the data\nspidermonkey %&gt;% \n  select(-id) %&gt;% \n  group_by(familiarity) %&gt;% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 3 × 11\n  familiarity variable       n   min   max median   iqr  mean    sd    se    ci\n  &lt;chr&gt;       &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 high        aggression     7   0.1   0.8    0.3  0.25 0.357 0.237 0.09  0.219\n2 low         aggression     7   0.3   1.2    0.5  0.3  0.629 0.315 0.119 0.291\n3 none        aggression     7   0.9   1.6    1.2  0.25 1.26  0.23  0.087 0.213\n\n# create boxplot\nggplot(spidermonkey,\n       aes(x = familiarity, y = aggression)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nFirst we read the data in:\n\nspidermonkey_py = pd.read_csv(\"data/CS2-spidermonkey.csv\")\n\n\n# look at the data\nspidermonkey_py.head()\n\n   id  aggression familiarity\n0   1         0.2        high\n1   2         0.1        high\n2   3         0.4        high\n3   4         0.8        high\n4   5         0.3        high\n\n# summarise the data\nspidermonkey_py.describe()[\"aggression\"]\n\ncount    21.000000\nmean      0.747619\nstd       0.460021\nmin       0.100000\n25%       0.400000\n50%       0.600000\n75%       1.200000\nmax       1.600000\nName: aggression, dtype: float64\n\n\n\n# create boxplot\np = (ggplot(spidermonkey_py,\n        aes(x = \"familiarity\",\n            y = \"aggression\")) +\n     geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nThe data appear to show a very significant difference in aggression rates between the three types of familiarity. We would probably expect a reasonably significant result here.",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html#assumptions",
    "href": "materials/cs2_practical_kruskal-wallis.html#assumptions",
    "title": "8  Kruskal-Wallis",
    "section": "8.5 Assumptions",
    "text": "8.5 Assumptions\nTo use the Kruskal-Wallis test we have to make three assumptions:\n\nThe parent distributions from which the samples are drawn have the same shape (if they’re normal then we should use a one-way ANOVA)\nEach data point in the samples is independent of the others\nThe parent distributions should have the same variance\n\nIndependence we’ll ignore as usual. Similar shape is best assessed from the earlier visualisation of the data. That means that we only need to check equality of variance.\n\n8.5.1 Equality of variance\nWe test for equality of variance using Levene’s test (since we can’t assume normal parent distributions which rules out Bartlett’s test).\n\nRPython\n\n\n\n# perform Levene's test\nlevene_test(aggression ~ familiarity,\n            data = spidermonkey)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     2    18     0.114 0.893\n\n\nThe relevant p-value is given in the p column (0.893). As it is quite large we see that each group do appear to have the same variance.\nThere is also a warning about group coerced to factor. There is no need to worry about this - Levene’s test needs to compare different groups and because familiarity is encoded as a character value, it converts it to a categorical one before running the test.\n\n\nWe can run Levene’s test with the pg.homoscedasticity() function. We previously used this for Bartlett’s test, but it allows us to define Levene’s instead.\n\npg.homoscedasticity(dv = \"aggression\",\n                    group = \"familiarity\",\n                    method = \"levene\",\n                    data = spidermonkey_py)\n\n               W      pval  equal_var\nlevene  0.113924  0.892964       True",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html#implement-and-interpret-the-test",
    "href": "materials/cs2_practical_kruskal-wallis.html#implement-and-interpret-the-test",
    "title": "8  Kruskal-Wallis",
    "section": "8.6 Implement and interpret the test",
    "text": "8.6 Implement and interpret the test\nPerform a Kruskal-Wallis test on the data:\n\nRPython\n\n\n\n# implement Kruskal-Wallis test\nkruskal.test(aggression ~ familiarity,\n             data = spidermonkey)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  aggression by familiarity\nKruskal-Wallis chi-squared = 13.597, df = 2, p-value = 0.001115\n\n\n\nThe first argument must be in the formula format: variable ~ category\nThe second argument must be the name of the data frame\n\nThe p-value is given in the 3rd line. This shows us the probability of getting samples such as ours if the null hypothesis were actually true.\n\n\nWe can use the kruskal() function from pingouin to perform the Kruskal-Wallis test:\n\npg.kruskal(dv = \"aggression\",\n           between = \"familiarity\",\n           data = spidermonkey_py)\n\n              Source  ddof1          H     p-unc\nKruskal  familiarity      2  13.597156  0.001115\n\n\n\n\n\nSince the p-value is very small (much smaller than the standard significance level of 0.05) we can say “that it is very unlikely that these three samples came from the same parent distribution and as such we can reject our null hypothesis” and state that:\n\nA Kruskal-Wallis test showed that aggression rates between spidermonkeys depends upon the degree of familiarity between them (p = 0.0011).",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html#post-hoc-testing-dunns-test",
    "href": "materials/cs2_practical_kruskal-wallis.html#post-hoc-testing-dunns-test",
    "title": "8  Kruskal-Wallis",
    "section": "8.7 Post-hoc testing (Dunn’s test)",
    "text": "8.7 Post-hoc testing (Dunn’s test)\nThe equivalent of Tukey’s range test for non-normal data is Dunn’s test.\nDunn’s test is used to check for significant differences in group medians:\n\nRPython\n\n\nThe dunn_test() function comes from the rstatix package, so make sure you have that loaded.\n\n# perform Dunn's test\ndunn_test(aggression ~ familiarity,\n          data = spidermonkey)\n\n# A tibble: 3 × 9\n  .y.        group1 group2    n1    n2 statistic        p    p.adj p.adj.signif\n* &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       \n1 aggression high   low        7     7      1.41 0.160    0.160    ns          \n2 aggression high   none       7     7      3.66 0.000257 0.000771 ***         \n3 aggression low    none       7     7      2.25 0.0245   0.0490   *           \n\n\nThe dunn_test() function performs a Kruskal-Wallis test on the data, followed by a post-hoc pairwise multiple comparison.\nThe comparison between the pairs of groups is reported in the table at the bottom. Each row contains a single comparison. We are interested in the p and p.adj columns, which contain the the p-values that we want. This table shows that there isn’t a significant difference between the high and low groups, as the p-value (0.1598) is too high. The other two comparisons between the high familiarity and no familiarity groups and between the low and no groups are significant though.\nThe dunn_test() function has several arguments, of which the p.adjust.method is likely to be of interest. Here you can define which method needs to be used to account for multiple comparisons. The default is \"holm\". We’ll cover more about this in the chapter on Power analysis.\n\n\nUnfortunately pingouin does not seem to have function that can perform Dunn’s test, so we need to import this from elsewhere.\nThere is a series of post-hocs tests available via scikit_posthocs. You’ll need to install this by running:\n\npip install scikit-posthocs\n\nAfter installation, load it with:\n\nimport scikit_posthocs as sp\n\nFinally, we can perform Dunn’s test as follows:\n\nsp.posthoc_dunn(spidermonkey_py,\n                val_col = \"aggression\",\n                group_col = \"familiarity\")\n\n          high       low      none\nhigh  1.000000  0.159777  0.000257\nlow   0.159777  1.000000  0.024493\nnone  0.000257  0.024493  1.000000\n\n\nThe p-values of the pairwise comparisons are reported in the table. This table shows that there isn’t a significant difference between the high and low groups, as the p-value (0.1598) is too high. The other two comparisons between the high familiarity and no familiarity groups and between the low and no groups are significant though.\nThe sp.posthoc_dunn() function has several arguments, of which the p_adjust is likely to be of interest. Here you can define which method needs to be used to account for multiple comparisons. We’ll cover more about this in the chapter on Power analysis.",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html#exercises",
    "href": "materials/cs2_practical_kruskal-wallis.html#exercises",
    "title": "8  Kruskal-Wallis",
    "section": "8.8 Exercises",
    "text": "8.8 Exercises\n\n8.8.1 Lobster weight (revisited)\n\n\n\n\n\n\nExerciseExercise 1\n\n\n\n\n\n\nLevel: \nPerform a Kruskal-Wallis test and do a post-hoc test on the lobster data set.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n8.9 Answer\n\nHypothesis\n\n\\(H_0\\) : all medians are equal\n\\(H_1\\) : not all medians are equal\n\n\n\nImport data, summarise and visualise\nAll done previously.\n\n\nAssumptions\nFrom before, since the data are normal enough they are definitely similar enough for a Kruskal-Wallis test and they do all have equality of variance from out assessment of the diagnostic plots. For completeness though we will look at Levene’s test.\n\nRPython\n\n\n\nlevene_test(weight ~ diet,\n            data = lobsters)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     2    15   0.00280 0.997\n\n\n\n\n\npg.homoscedasticity(dv = \"weight\",\n                    group = \"diet\",\n                    method = \"levene\",\n                    data = lobsters_py)\n\n               W      pval  equal_var\nlevene  0.002796  0.997209       True\n\n\n\n\n\nGiven that the p-value is so high, this again agrees with our previous assessment that the equality of variance assumption is well met. Rock on.\n\n\nKruskal-Wallis test\nSo, we perform the Kruskall-Wallis test.\n\nRPython\n\n\n\n# implement Kruskal-Wallis test\nkruskal.test(weight ~ diet,\n             data = lobsters)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  weight by diet\nKruskal-Wallis chi-squared = 3.2565, df = 2, p-value = 0.1963\n\n\n\n\n\npg.kruskal(dv = \"weight\",\n           between = \"diet\",\n           data = lobsters_py)\n\n        Source  ddof1         H     p-unc\nKruskal   diet      2  3.256475  0.196275\n\n\n\n\n\n\nA Kruskal-Wallis test indicated that the median weight of juvenile lobsters did not differ significantly between diets (p = 0.20).\n\n\n\nPost-hoc testing\nIn this case we should not be doing any post-hoc testing, because we did not detect any statistically significant differences. Doing so anyway and then reporting any incidental groups that would differ, would be p-hacking. And naughty.",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html#answer",
    "href": "materials/cs2_practical_kruskal-wallis.html#answer",
    "title": "8  Kruskal-Wallis",
    "section": "8.9 Answer",
    "text": "8.9 Answer\n\nHypothesis\n\n\\(H_0\\) : all medians are equal\n\\(H_1\\) : not all medians are equal\n\n\n\nImport data, summarise and visualise\nAll done previously.\n\n\nAssumptions\nFrom before, since the data are normal enough they are definitely similar enough for a Kruskal-Wallis test and they do all have equality of variance from out assessment of the diagnostic plots. For completeness though we will look at Levene’s test.\n\nRPython\n\n\n\nlevene_test(weight ~ diet,\n            data = lobsters)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     2    15   0.00280 0.997\n\n\n\n\n\npg.homoscedasticity(dv = \"weight\",\n                    group = \"diet\",\n                    method = \"levene\",\n                    data = lobsters_py)\n\n               W      pval  equal_var\nlevene  0.002796  0.997209       True\n\n\n\n\n\nGiven that the p-value is so high, this again agrees with our previous assessment that the equality of variance assumption is well met. Rock on.\n\n\nKruskal-Wallis test\nSo, we perform the Kruskall-Wallis test.\n\nRPython\n\n\n\n# implement Kruskal-Wallis test\nkruskal.test(weight ~ diet,\n             data = lobsters)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  weight by diet\nKruskal-Wallis chi-squared = 3.2565, df = 2, p-value = 0.1963\n\n\n\n\n\npg.kruskal(dv = \"weight\",\n           between = \"diet\",\n           data = lobsters_py)\n\n        Source  ddof1         H     p-unc\nKruskal   diet      2  3.256475  0.196275\n\n\n\n\n\n\nA Kruskal-Wallis test indicated that the median weight of juvenile lobsters did not differ significantly between diets (p = 0.20).\n\n\n\nPost-hoc testing\nIn this case we should not be doing any post-hoc testing, because we did not detect any statistically significant differences. Doing so anyway and then reporting any incidental groups that would differ, would be p-hacking. And naughty.",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html#summary",
    "href": "materials/cs2_practical_kruskal-wallis.html#summary",
    "title": "8  Kruskal-Wallis",
    "section": "8.10 Summary",
    "text": "8.10 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nWe use a Kruskal-Wallis test to see if there is a difference in medians between multiple continuous response variables\nWe assume parent distributions have the same shape; each data point is independent and the parent distributions have the same variance\nWe test for equality of variance using Levene’s test\nPost-hoc testing to check for significant differences in the group medians is done with Dunn’s test",
    "crumbs": [
      "CS2: Categorical predictors",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html",
    "href": "materials/cs3_practical_linear-regression.html",
    "title": "9  Linear regression",
    "section": "",
    "text": "9.1 Libraries and functions",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#libraries-and-functions",
    "href": "materials/cs3_practical_linear-regression.html#libraries-and-functions",
    "title": "9  Linear regression",
    "section": "",
    "text": "NoteClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n9.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n\n\n9.1.2 Functions\n\n# Creates diagnostic plots\nggResidpanel::resid_panel()\n\n# Creates a linear model\nstats::lm()\n\n# Creates an ANOVA table for a linear model\nstats::anova()\n\n\n\n\n\n9.1.3 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n# Custom Python script for diagnostic plots, load from base working directory\nfrom dgplots import dgplots\n\n\n\n9.1.4 Functions\n\n# Summary statistics\npandas.DataFrame.describe()\n\n# Plots the first few rows of a DataFrame\npandas.DataFrame.head()\n\n# Query the columns of a DataFrame with a boolean expression\npandas.DataFrame.query()\n\n# Reads in a .csv file\npandas.DataFrame.read_csv()\n\n# Creates a model from a formula and data frame\nstatsmodels.formula.api.ols()\n\n# Creates an ANOVA table for one or more fitted linear models\nstatsmodels.stats.anova.anova_lm()\n\n# Custom function to create diagnostic plots\ndgplots()\n\nNote: you can download the dgplots script here.",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#purpose-and-aim",
    "href": "materials/cs3_practical_linear-regression.html#purpose-and-aim",
    "title": "9  Linear regression",
    "section": "9.2 Purpose and aim",
    "text": "9.2 Purpose and aim\nRegression analysis not only tests for an association between two or more variables, but also allows you to investigate quantitatively the nature of any relationship which is present. This can help you determine if one variable may be used to predict values of another. Simple linear regression essentially models the dependence of a scalar dependent variable (\\(y\\)) on an independent (or explanatory) variable (\\(x\\)) according to the relationship:\n\\[\\begin{equation*}\ny = \\beta_0 + \\beta_1 x\n\\end{equation*}\\]\nwhere \\(\\beta_0\\) is the value of the intercept and \\(\\beta_1\\) is the slope of the fitted line. A linear regression analysis assesses if the coefficient of the slope, \\(\\beta_1\\), is actually different from zero. If it is different from zero then we can say that \\(x\\) has a significant effect on \\(y\\) (since changing \\(x\\) leads to a predicted change in \\(y\\)). If it isn’t significantly different from zero, then we say that there isn’t sufficient evidence of such a relationship. To assess whether the slope is significantly different from zero we first need to calculate the values of \\(\\beta_0\\) and \\(\\beta_1\\).",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#data-and-hypotheses",
    "href": "materials/cs3_practical_linear-regression.html#data-and-hypotheses",
    "title": "9  Linear regression",
    "section": "9.3 Data and hypotheses",
    "text": "9.3 Data and hypotheses\nWe will perform a simple linear regression analysis on the two variables murder and assault from the USArrests data set. This rather bleak data set contains statistics on arrests per 100,000 residents for assault, murder and robbery in each of the 50 US states in 1973, alongside the proportion of the population who lived in urban areas at that time. We wish to determine whether the assault variable is a significant predictor of the murder variable. This means that we will need to find the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) that best fit the following macabre equation:\n\\[\\begin{equation*}\nMurder  = \\beta_0 + \\beta_1 \\times Assault\n\\end{equation*}\\]\nAnd then will be testing the following null and alternative hypotheses:\n\n\\(H_0\\): assault is not a significant predictor of murder, \\(\\beta_1 = 0\\)\n\\(H_1\\): assault is a significant predictor of murder, \\(\\beta_1 \\neq 0\\)",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#summarise-and-visualise",
    "href": "materials/cs3_practical_linear-regression.html#summarise-and-visualise",
    "title": "9  Linear regression",
    "section": "9.4 Summarise and visualise",
    "text": "9.4 Summarise and visualise\n\nRPython\n\n\nFirst, we read in the data:\n\nUSArrests &lt;- read_csv(\"data/CS3-usarrests.csv\")\n\nYou can visualise the data with:\n\n# create scatterplot of the data\nggplot(USArrests,\n       aes(x = assault, y = murder)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nFirst, we read in the data:\n\nUSArrests_py = pd.read_csv(\"data/CS3-usarrests.csv\")\n\nYou can visualise the data with:\n\n# create scatterplot of the data\np = (ggplot(USArrests_py,\n         aes(x = \"assault\",\n             y = \"murder\")) +\n     geom_point())\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nPerhaps unsurprisingly, there appears to be a relatively strong positive relationship between these two variables. Whilst there is a reasonable scatter of the points around any trend line, we would probably expect a significant result in this case.",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#assumptions",
    "href": "materials/cs3_practical_linear-regression.html#assumptions",
    "title": "9  Linear regression",
    "section": "9.5 Assumptions",
    "text": "9.5 Assumptions\nIn order for a linear regression analysis to be valid 4 key assumptions need to be met:\n\n\n\n\n\n\nImportant\n\n\n\n\nThe data must be linear (it is entirely possible to calculate a straight line through data that is not straight - it doesn’t mean that you should!)\nThe residuals must be normally distributed\nThe residuals must not be correlated with their fitted values (i.e. they should be independent)\nThe fit should not depend overly much on a single point (no point should have high leverage).\n\n\n\nWhether these assumptions are met can easily be checked visually by producing four key diagnostic plots.\n\nRPython\n\n\nFirst we need to define the linear model:\n\nlm_1 &lt;- lm(murder ~ assault,\n           data = USArrests)\n\n\nThe first argument to lm is a formula saying that murder depends on assault. As we have seen before, the syntax is generally dependent variable ~ independent variable.\nThe second argument specifies which data to use.\n\nNext, we can create diagnostic plots for the model:\n\nresid_panel(lm_1,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\nThe top left graph plots the Residuals plot. If the data are best explained by a linear line then the points should be uniformly distributed above and below the horizontal blue line. If that’s the case then the red line (a smoother line) should overlay the blue line. This plot is pretty good.\nThe top right graph shows the Q-Q plot which allows a visual inspection of normality. If the residuals are normally distributed, then the points should lie on the diagonal dotted line. This isn’t too bad but there is some slight snaking towards the upper end and there appears to be an outlier.\nThe bottom left Location-scale graph allows us to investigate whether there is any correlation between the residuals and the predicted values and whether the variance of the residuals changes significantly. If not, then the red line should be horizontal. If there is any correlation or change in variance then the red line will not be horizontal. This plot is fine.\nThe last graph shows the Cook’s distance and tests if any one point has an unnecessarily large effect on the fit. The important aspect here is to see if any points are larger than 0.5 (meaning you’d have to be careful) or 1.0 (meaning you’d definitely have to check if that point has an large effect on the model). If not, then no point has undue influence. This plot is good.\n\n\n\nIf you haven’t loaded statsmodels yet, run the following:\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nNext, we create a linear model and get the .fit():\n\n# create a linear model\nmodel = smf.ols(formula= \"murder ~ assault\", data = USArrests_py)\n# and get the fitted parameters of the model\nlm_USArrests_py = model.fit()\n\nThen we use dgplots() to create the diagnostic plots:\n\ndgplots(lm_USArrests_py)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFormally, if there is any concern after looking at the diagnostic plots then a linear regression is not valid. However, disappointingly, very few people ever check whether the linear regression assumptions have been met before quoting the results.\nLet’s change this through leading by example!",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#implement-and-interpret-test",
    "href": "materials/cs3_practical_linear-regression.html#implement-and-interpret-test",
    "title": "9  Linear regression",
    "section": "9.6 Implement and interpret test",
    "text": "9.6 Implement and interpret test\nWe have already defined the linear model, so we can have a closer look at it:\n\nRPython\n\n\n\n# show the linear model\nlm_1\n\n\nCall:\nlm(formula = murder ~ assault, data = USArrests)\n\nCoefficients:\n(Intercept)      assault  \n    0.63168      0.04191  \n\n\nThe lm() function returns a linear model object which is essentially a list containing everything necessary to understand and analyse a linear model. However, if we just type the model name (as we have above) then it just prints to the screen the actual coefficients of the model i.e. the intercept and the slope of the line.\n\n\n\n\n\n\nNoteThe linear model object: would you like to know more?\n\n\n\n\n\nIf you wanted to know more about the lm object we created, then type in:\n\nView(lm_1)\n\nThis shows a list (a type of object in R), containing all of the information associated with the linear model. The most relevant ones at the moment are:\n\ncoefficients contains the values of the coefficients we found earlier.\nresiduals contains the residual associated for each individual data point.\nfitted.values contains the values that the linear model predicts for each individual data point.\n\n\n\n\n\n\n\nprint(lm_USArrests_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 murder   R-squared:                       0.643\nModel:                            OLS   Adj. R-squared:                  0.636\nMethod:                 Least Squares   F-statistic:                     86.45\nDate:                Sun, 23 Nov 2025   Prob (F-statistic):           2.60e-12\nTime:                        11:21:26   Log-Likelihood:                -118.26\nNo. Observations:                  50   AIC:                             240.5\nDf Residuals:                      48   BIC:                             244.4\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.6317      0.855      0.739      0.464      -1.087       2.350\nassault        0.0419      0.005      9.298      0.000       0.033       0.051\n==============================================================================\nOmnibus:                        4.799   Durbin-Watson:                   1.796\nProb(Omnibus):                  0.091   Jarque-Bera (JB):                3.673\nSkew:                           0.598   Prob(JB):                        0.159\nKurtosis:                       3.576   Cond. No.                         436.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nA rather large table, but the values we’re interested in can be found more or less in the middle. We are after the coef values, where the intercept is 0.6317 and the slope is 0.0419.\n\n\n\nSo here we have found that the line of best fit is given by:\n\\[\\begin{equation*}\nMurder = 0.63 + 0.042 \\times Assault\n\\end{equation*}\\]\nNext we can assess whether the slope is significantly different from zero:\n\nRPython\n\n\n\nanova(lm_1)\n\nAnalysis of Variance Table\n\nResponse: murder\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nassault    1 597.70  597.70  86.454 2.596e-12 ***\nResiduals 48 331.85    6.91                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere, we again use the anova() command to assess significance. This shouldn’t be too surprising at this stage if the introductory lectures made any sense. From a mathematical perspective, one-way ANOVA and simple linear regression are exactly the same as each other and it makes sense that we should use the same command to analyse them in R.\nThis is exactly the same format as the table we saw for one-way ANOVA:\n\nThe 1st line just tells you the that this is an ANOVA test\nThe 2nd line tells you what the response variable is (in this case Murder)\nThe 3rd, 4th and 5th lines are an ANOVA table which contain some useful values:\n\nThe Df column contains the degrees of freedom values on each row, 1 and 48\nThe F value column contains the F statistic, 86.454\nThe p-value is 2.596e-12 and is the number directly under the Pr(&gt;F) on the 4th line.\nThe other values in the table (in the Sum Sq and Mean Sq) column are used to calculate the F statistic itself and we don’t need to know these.\n\n\n\n\nWe can perform an ANOVA on the lm_USArrests_py object using the anova_lm() function from the statsmodels package.\n\nsm.stats.anova_lm(lm_USArrests_py, typ = 2)\n\n              sum_sq    df          F        PR(&gt;F)\nassault   597.703202   1.0  86.454086  2.595761e-12\nResidual  331.849598  48.0        NaN           NaN\n\n\n\n\n\nAgain, the p-value is what we’re most interested in here and shows us the probability of getting data such as ours if the null hypothesis were actually true and the slope of the line were actually zero. Since the p-value is excruciatingly tiny we can reject our null hypothesis and state that:\n\nA simple linear regression showed that the assault rate in US states was a significant predictor of the number of murders (p = 2.59x10-12).\n\n\n9.6.1 Plotting the regression line\nIt can be very helpful to plot the regression line with the original data to see how far the data are from the predicted linear values. We can do this as follows:\n\nRPython\n\n\n\n# plot the data\nggplot(USArrests,\n       aes(x = assault, y = murder)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\nWe plot all the data using geom_point()\nNext, we add the linear model using geom_smooth(method = \"lm\"), hiding the confidence intervals (se = FALSE)\n\n\n\n\np = (ggplot(USArrests_py,\n        aes(x = \"assault\", y = \"murder\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 se = False,\n                 colour = \"blue\"))\n\np.show()",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#exercises",
    "href": "materials/cs3_practical_linear-regression.html#exercises",
    "title": "9  Linear regression",
    "section": "9.7 Exercises",
    "text": "9.7 Exercises\n\n9.7.1 State data: Life expectancy and murder\n\n\n\n\n\n\nExerciseExercise 1\n\n\n\n\n\n\nLevel: \nWe will use the data from the file data/CS3-statedata.csv data set for this exercise. This rather more benign data set contains information on more general properties of each US state, such as population (1975), per capita income (1974), illiteracy proportion (1970), life expectancy (1969), murder rate per 100,000 people (there’s no getting away from it), percentage of the population who are high-school graduates, average number of days where the minimum temperature is below freezing between 1931 and 1960, and the state area in square miles. The data set contains 50 rows and 8 columns, with column names: population, income, illiteracy, life_exp, murder, hs_grad, frost and area.\nPerform a linear regression on the variables life_exp and murder and do the following:\n\nFind the value of the slope and intercept coefficients.\nDetermine if the slope is significantly different from zero. In other words, is there a relationship between the two variables? (hint: think about which variable is your response and predictor)\nProduce a scatter plot of the data with the line of best fit superimposed on top.\nProduce diagnostic plots and discuss with your (virtual) neighbour if you should have carried out a simple linear regression in each case\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n9.8 Answer\n\nLoad and visualise the data\n\nRPython\n\n\nFirst, we read in the data:\n\nUSAstate &lt;- read_csv(\"data/CS3-statedata.csv\")\n\nNext, we visualise the murder variable against the life_exp variable. We also add a regression line.\n\n# plot the data and add the regression line\nggplot(USAstate,\n       aes(x = murder, y = life_exp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\nFirst, we read in the data:\n\nUSAstate_py = pd.read_csv(\"data/CS3-statedata.csv\")\n\nNext, we visualise the murder variable against the life_exp variable. We also add a regression line.\n\np = (ggplot(USAstate_py,\n        aes(x = \"life_exp\", y = \"murder\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 se = False,\n                 colour = \"blue\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nWe visualise for the same reasons as before:\n\nWe check that the data aren’t obviously wrong. Here we have sensible values for life expectancy (nothing massively large or small), and plausible values for murder rates (not that I’m that au fait with US murder rates in 1973 but small positive numbers seem plausible).\nWe check to see what we would expect from the statistical analysis. Here there does appear to be a reasonable downward trend to the data. I would be surprised if we didn’t get a significant result given the amount of data and the spread of the data about the line\nWe check the assumptions (only roughly though as we’ll be doing this properly in a minute). Nothing immediately gives me cause for concern; the data appear linear, the spread of the data around the line appears homogeneous and symmetrical. No outliers either.\n\n\n\nCheck assumptions\nNow, let’s check the assumptions with the diagnostic plots.\n\nRPython\n\n\n\n# create a linear model\nlm_murder &lt;- lm(life_exp ~ murder,\n           data = USAstate)\n\n# create the diagnostic plots\nresid_panel(lm_murder,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\nThe Residuals plot appears symmetric enough (similar distribution of points above and below the horizontal blue line) for me to happy with linearity. Similarly the red line in the Location-Scale plot looks horizontal enough for me to be happy with homogeneity of variance. There aren’t any influential points in the Cook’s distance plot. The only plot that does give me a bit of concern is the Q-Q plot. Here we see clear evidence of snaking, although the degree of snaking isn’t actually that bad. This just means that we can be pretty certain that the distribution of residuals isn’t normal, but also that it isn’t very non-normal.\n\n\nFirst, we create a linear model:\n\n# create a linear model\nmodel = smf.ols(formula= \"life_exp ~ murder\", data = USAstate_py)\n# and get the fitted parameters of the model\nlm_USAstate_py = model.fit()\n\nNext, we can create the diagnostic plots:\n\ndgplots(lm_USAstate_py)\n\n\n\n\nThe Residuals plot appears symmetric enough (similar distribution of points above and below the horizontal blue line) for me to happy with linearity. Similarly the red line in the Location-Scale plot looks horizontal enough for me to be happy with homogeneity of variance. There aren’t any influential points in the Influential points plot. The only plot that does give me a bit of concern is the Q-Q plot. Here we see clear evidence of snaking, although the degree of snaking isn’t actually that bad. This just means that we can be pretty certain that the distribution of residuals isn’t normal, but also that it isn’t very non-normal.\n\n\n\nWhat do we do in this situation? Well, there are three possible options:\n\nAppeal to the Central Limit Theorem. This states that if we have a large enough sample size we don’t have to worry about whether the distribution of the residuals are normally distributed. Large enough is a bit of a moving target here and to be honest it depends on how non-normal the underlying data are. If the data are only a little bit non-normal then we can get away with using a smaller sample than if the data are massively skewed (for example). This is not an exact science, but anything over 30 data points is considered a lot for mild to moderate non-normality (as we have in this case). If the data were very skewed then we would be looking for more data points (50-100). So, for this example we can legitimately just carry on with our analysis without worrying.\nTry transforming the data. Here we would try applying some mathematical functions to the response variable (life_exp) in the hope that repeating the analysis with this transformed variable would make things better. To be honest with you it might not work and we won’t know until we try. Dealing with transformed variables is legitimate as an approach but it can make interpreting the model a bit more challenging. In this particular example none of the traditional transformations (log, square-root, reciprocal) do anything to fix the slight lack of normality.\nGo with permutation methods / bootstrapping. This approach would definitely work. I don’t have time to explain it here (it’s the subject of an entire other practical). This approach also requires us to have a reasonably large sample size to work well as we have to assume that the distribution of the sample is a good approximation for the distribution of the entire data set.\n\nSo in this case, because we have a large enough sample size and our deviation from normality isn’t too bad, we can just crack on with the standard analysis.\n\n\nImplement and interpret test\nSo, let’s actually do the analysis:\n\nRPython\n\n\n\nanova(lm_murder)\n\nAnalysis of Variance Table\n\nResponse: life_exp\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nmurder     1 53.838  53.838  74.989 2.26e-11 ***\nResiduals 48 34.461   0.718                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nsm.stats.anova_lm(lm_USAstate_py, typ = 2)\n\n             sum_sq    df          F        PR(&gt;F)\nmurder    53.837675   1.0  74.988652  2.260070e-11\nResidual  34.461327  48.0        NaN           NaN\n\n\n\n\n\nAnd after all of that we find that the murder rate is a statistically significant predictor of life expectancy in US states. Woohoo!\n\n\n\n\n\n\n\n\n\n\n\n9.8.1 State data: Graduation and frost days\n\n\n\n\n\n\nExerciseExercise 2\n\n\n\n\n\n\nLevel: \nNow let’s investigate the relationship between the proportion of High School Graduates a state has (hs_grad) and the mean number of days below freezing (frost) within each state.\nAgain, think about which variable is your response and predictor.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n9.9 Answer\nWe’ll run through this a bit quicker:\n\nRPython\n\n\n\n# plot the data\nggplot(USAstate,\n       aes(x = frost, y = hs_grad)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\np = (ggplot(USAstate_py,\n        aes(x = \"hs_grad\", y = \"frost\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 se = False,\n                 colour = \"blue\"))\n\n\n\n\nOnce again, we look at the data.\n\nThere doesn’t appear to be any ridiculous errors with the data; High School graduation proportions are in the 0-100% range and the mean number of sub-zero days for each state are between 0 and 365, so these numbers are plausible.\nWhilst there is a trend upwards, which wouldn’t surprise me if it came back as being significant, I’m a bit concerned about…\nThe assumptions. I’m mainly concerned that the data aren’t very linear. There appears to be a noticeable pattern to the data with some sort of minimum around 50-60 Frost days. This means that it’s hard to assess the other assumptions.\n\nLet’s check these out properly\nNow, let’s check the assumptions with the diagnostic plots.\n\nRPython\n\n\n\n# create a linear model\nlm_frost &lt;- lm(hs_grad ~ frost,\n               data = USAstate)\n\n# create the diagnostic plots\nresid_panel(lm_frost,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\n\nFirst, we create a linear model:\n\n# create a linear model\nmodel = smf.ols(formula= \"hs_grad ~ frost\", data = USAstate_py)\n# and get the fitted parameters of the model\nlm_frost_py = model.fit()\n\nNext, we can create the diagnostic plots:\n\ndgplots(lm_frost_py)\n\n\n\n\n\n\n\nWe can see that what we suspected from before is backed up by the residuals plot. The data aren’t linear and there appears to be some sort of odd down-up pattern here. Given the lack of linearity it just isn’t worth worrying about the other plots because our model is misspecified: a straight line just doesn’t represent our data at all.\nJust for reference, and as practice for looking at diagnostic plots, if we ignore the lack of linearity then we can say that\n\nNormality is pretty good from the Q-Q plot\nHomogeneity of variance isn’t very good and there appears to be a noticeable drop in variance as we go from left to right (from consideration of the Location-Scale plot)\nThere don’t appear to be any influential points (by looking at the Cook’s distance plot)\n\nHowever, none of that is relevant in this particular case since the data aren’t linear and a straight line would be the wrong model to fit.\nSo what do we do in this situation?\nWell actually, this is a bit tricky as there aren’t any easy fixes here. There are two broad solutions for dealing with a misspecified model.\n\nThe most common solution is that we need more predictor variables in the model. Here we’re trying to explain/predict high school graduation only using the number of frost days. Obviously there are many more things that would affect the proportion of high school graduates than just how cold it is in a State (which is a weird potential predictor when you think about it) and so what we would need is a statistical approach that allows us to look at multiple predictor variables. We’ll cover that approach in the next two sessions.\nThe other potential solution is to say that high school graduation can in fact be predicted only by the number of frost days but that the relationship between them isn’t linear. We would then need to specify a relationship (a curve basically) and then try to fit the data to the new, non-linear, curve. This process is called, unsurprisingly, non-linear regression and we don’t cover that in this course. This process is best used when there is already a strong theoretical reason for a non-linear relationship between two variables (such as sigmoidal dose-response curves in pharmacology or exponential relationships in cell growth). In this case we don’t have any such preconceived notions and so it wouldn’t really be appropriate in this case.\n\nNeither of these solutions can be tackled with the knowledge that we have so far in the course but we can definitely say that based upon this data set, there isn’t a linear relationship (significant or otherwise) between frosty days and high school graduation rates.",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#answer",
    "href": "materials/cs3_practical_linear-regression.html#answer",
    "title": "9  Linear regression",
    "section": "9.8 Answer",
    "text": "9.8 Answer\n\nLoad and visualise the data\n\nRPython\n\n\nFirst, we read in the data:\n\nUSAstate &lt;- read_csv(\"data/CS3-statedata.csv\")\n\nNext, we visualise the murder variable against the life_exp variable. We also add a regression line.\n\n# plot the data and add the regression line\nggplot(USAstate,\n       aes(x = murder, y = life_exp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\nFirst, we read in the data:\n\nUSAstate_py = pd.read_csv(\"data/CS3-statedata.csv\")\n\nNext, we visualise the murder variable against the life_exp variable. We also add a regression line.\n\np = (ggplot(USAstate_py,\n        aes(x = \"life_exp\", y = \"murder\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 se = False,\n                 colour = \"blue\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nWe visualise for the same reasons as before:\n\nWe check that the data aren’t obviously wrong. Here we have sensible values for life expectancy (nothing massively large or small), and plausible values for murder rates (not that I’m that au fait with US murder rates in 1973 but small positive numbers seem plausible).\nWe check to see what we would expect from the statistical analysis. Here there does appear to be a reasonable downward trend to the data. I would be surprised if we didn’t get a significant result given the amount of data and the spread of the data about the line\nWe check the assumptions (only roughly though as we’ll be doing this properly in a minute). Nothing immediately gives me cause for concern; the data appear linear, the spread of the data around the line appears homogeneous and symmetrical. No outliers either.\n\n\n\nCheck assumptions\nNow, let’s check the assumptions with the diagnostic plots.\n\nRPython\n\n\n\n# create a linear model\nlm_murder &lt;- lm(life_exp ~ murder,\n           data = USAstate)\n\n# create the diagnostic plots\nresid_panel(lm_murder,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\nThe Residuals plot appears symmetric enough (similar distribution of points above and below the horizontal blue line) for me to happy with linearity. Similarly the red line in the Location-Scale plot looks horizontal enough for me to be happy with homogeneity of variance. There aren’t any influential points in the Cook’s distance plot. The only plot that does give me a bit of concern is the Q-Q plot. Here we see clear evidence of snaking, although the degree of snaking isn’t actually that bad. This just means that we can be pretty certain that the distribution of residuals isn’t normal, but also that it isn’t very non-normal.\n\n\nFirst, we create a linear model:\n\n# create a linear model\nmodel = smf.ols(formula= \"life_exp ~ murder\", data = USAstate_py)\n# and get the fitted parameters of the model\nlm_USAstate_py = model.fit()\n\nNext, we can create the diagnostic plots:\n\ndgplots(lm_USAstate_py)\n\n\n\n\nThe Residuals plot appears symmetric enough (similar distribution of points above and below the horizontal blue line) for me to happy with linearity. Similarly the red line in the Location-Scale plot looks horizontal enough for me to be happy with homogeneity of variance. There aren’t any influential points in the Influential points plot. The only plot that does give me a bit of concern is the Q-Q plot. Here we see clear evidence of snaking, although the degree of snaking isn’t actually that bad. This just means that we can be pretty certain that the distribution of residuals isn’t normal, but also that it isn’t very non-normal.\n\n\n\nWhat do we do in this situation? Well, there are three possible options:\n\nAppeal to the Central Limit Theorem. This states that if we have a large enough sample size we don’t have to worry about whether the distribution of the residuals are normally distributed. Large enough is a bit of a moving target here and to be honest it depends on how non-normal the underlying data are. If the data are only a little bit non-normal then we can get away with using a smaller sample than if the data are massively skewed (for example). This is not an exact science, but anything over 30 data points is considered a lot for mild to moderate non-normality (as we have in this case). If the data were very skewed then we would be looking for more data points (50-100). So, for this example we can legitimately just carry on with our analysis without worrying.\nTry transforming the data. Here we would try applying some mathematical functions to the response variable (life_exp) in the hope that repeating the analysis with this transformed variable would make things better. To be honest with you it might not work and we won’t know until we try. Dealing with transformed variables is legitimate as an approach but it can make interpreting the model a bit more challenging. In this particular example none of the traditional transformations (log, square-root, reciprocal) do anything to fix the slight lack of normality.\nGo with permutation methods / bootstrapping. This approach would definitely work. I don’t have time to explain it here (it’s the subject of an entire other practical). This approach also requires us to have a reasonably large sample size to work well as we have to assume that the distribution of the sample is a good approximation for the distribution of the entire data set.\n\nSo in this case, because we have a large enough sample size and our deviation from normality isn’t too bad, we can just crack on with the standard analysis.\n\n\nImplement and interpret test\nSo, let’s actually do the analysis:\n\nRPython\n\n\n\nanova(lm_murder)\n\nAnalysis of Variance Table\n\nResponse: life_exp\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nmurder     1 53.838  53.838  74.989 2.26e-11 ***\nResiduals 48 34.461   0.718                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nsm.stats.anova_lm(lm_USAstate_py, typ = 2)\n\n             sum_sq    df          F        PR(&gt;F)\nmurder    53.837675   1.0  74.988652  2.260070e-11\nResidual  34.461327  48.0        NaN           NaN\n\n\n\n\n\nAnd after all of that we find that the murder rate is a statistically significant predictor of life expectancy in US states. Woohoo!",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#answer-1",
    "href": "materials/cs3_practical_linear-regression.html#answer-1",
    "title": "9  Linear regression",
    "section": "9.9 Answer",
    "text": "9.9 Answer\nWe’ll run through this a bit quicker:\n\nRPython\n\n\n\n# plot the data\nggplot(USAstate,\n       aes(x = frost, y = hs_grad)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\np = (ggplot(USAstate_py,\n        aes(x = \"hs_grad\", y = \"frost\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 se = False,\n                 colour = \"blue\"))\n\n\n\n\nOnce again, we look at the data.\n\nThere doesn’t appear to be any ridiculous errors with the data; High School graduation proportions are in the 0-100% range and the mean number of sub-zero days for each state are between 0 and 365, so these numbers are plausible.\nWhilst there is a trend upwards, which wouldn’t surprise me if it came back as being significant, I’m a bit concerned about…\nThe assumptions. I’m mainly concerned that the data aren’t very linear. There appears to be a noticeable pattern to the data with some sort of minimum around 50-60 Frost days. This means that it’s hard to assess the other assumptions.\n\nLet’s check these out properly\nNow, let’s check the assumptions with the diagnostic plots.\n\nRPython\n\n\n\n# create a linear model\nlm_frost &lt;- lm(hs_grad ~ frost,\n               data = USAstate)\n\n# create the diagnostic plots\nresid_panel(lm_frost,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\n\nFirst, we create a linear model:\n\n# create a linear model\nmodel = smf.ols(formula= \"hs_grad ~ frost\", data = USAstate_py)\n# and get the fitted parameters of the model\nlm_frost_py = model.fit()\n\nNext, we can create the diagnostic plots:\n\ndgplots(lm_frost_py)\n\n\n\n\n\n\n\nWe can see that what we suspected from before is backed up by the residuals plot. The data aren’t linear and there appears to be some sort of odd down-up pattern here. Given the lack of linearity it just isn’t worth worrying about the other plots because our model is misspecified: a straight line just doesn’t represent our data at all.\nJust for reference, and as practice for looking at diagnostic plots, if we ignore the lack of linearity then we can say that\n\nNormality is pretty good from the Q-Q plot\nHomogeneity of variance isn’t very good and there appears to be a noticeable drop in variance as we go from left to right (from consideration of the Location-Scale plot)\nThere don’t appear to be any influential points (by looking at the Cook’s distance plot)\n\nHowever, none of that is relevant in this particular case since the data aren’t linear and a straight line would be the wrong model to fit.\nSo what do we do in this situation?\nWell actually, this is a bit tricky as there aren’t any easy fixes here. There are two broad solutions for dealing with a misspecified model.\n\nThe most common solution is that we need more predictor variables in the model. Here we’re trying to explain/predict high school graduation only using the number of frost days. Obviously there are many more things that would affect the proportion of high school graduates than just how cold it is in a State (which is a weird potential predictor when you think about it) and so what we would need is a statistical approach that allows us to look at multiple predictor variables. We’ll cover that approach in the next two sessions.\nThe other potential solution is to say that high school graduation can in fact be predicted only by the number of frost days but that the relationship between them isn’t linear. We would then need to specify a relationship (a curve basically) and then try to fit the data to the new, non-linear, curve. This process is called, unsurprisingly, non-linear regression and we don’t cover that in this course. This process is best used when there is already a strong theoretical reason for a non-linear relationship between two variables (such as sigmoidal dose-response curves in pharmacology or exponential relationships in cell growth). In this case we don’t have any such preconceived notions and so it wouldn’t really be appropriate in this case.\n\nNeither of these solutions can be tackled with the knowledge that we have so far in the course but we can definitely say that based upon this data set, there isn’t a linear relationship (significant or otherwise) between frosty days and high school graduation rates.",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#summary",
    "href": "materials/cs3_practical_linear-regression.html#summary",
    "title": "9  Linear regression",
    "section": "9.10 Summary",
    "text": "9.10 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nLinear regression tests if a linear relationship exists between two or more variables\nIf so, we can use one variable to predict another\nA linear model has an intercept and slope and we test if the slope differs from zero\nWe create linear models and perform an ANOVA to assess the slope coefficient\nWe can only use a linear regression if these four assumptions are met:\n\nThe data are linear\nResiduals are normally distributed\nResiduals are not correlated with their fitted values\nNo single point should have a large influence on the linear model\n\nWe can use diagnostic plots to evaluate these assumptions",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html",
    "href": "materials/cs3_practical_correlations.html",
    "title": "10  Correlations",
    "section": "",
    "text": "10.1 Libraries and functions",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#libraries-and-functions",
    "href": "materials/cs3_practical_correlations.html#libraries-and-functions",
    "title": "10  Correlations",
    "section": "",
    "text": "NoteClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n10.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n\n\n10.1.2 Functions\n\n# Computes the absolute value\nbase::abs()\n\n# Creates a matrix of scatter plots\ngraphics::pairs()\n\n# Computes a correlation matrix\nstats::cor()\n\n# Creates a heat map\nstats::heatmap()\n\n# Turns object into tibble\ntibble::as.tibble()\n\n# Lengthens the data\ntidyr::pivot_longer()\n\n\n\n\n\n10.1.3 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n\n\n10.1.4 Functions\n\n# Compute pairwise correlation of columns\npandas.DataFrame.corr()\n\n# Plots the first few rows of a DataFrame\npandas.DataFrame.head()\n\n# Query the columns of a DataFrame with a boolean expression\npandas.DataFrame.query()\n\n# Set the name of the axis for the index or columns\npandas.DataFrame.rename_axis()\n\n# Unpivot a DataFrame from wide to long format\npandas.DataFrame.melt()\n\n# Reads in a .csv file\npandas.DataFrame.read_csv()",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#purpose-and-aim",
    "href": "materials/cs3_practical_correlations.html#purpose-and-aim",
    "title": "10  Correlations",
    "section": "10.2 Purpose and aim",
    "text": "10.2 Purpose and aim\nCorrelation refers to the relationship of two variables (or data sets) to one another. Two data sets are said to be correlated if they are not independent from one another. Correlations can be useful because they can indicate if a predictive relationship may exist. However just because two data sets are correlated does not mean that they are causally related.",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#data-and-hypotheses",
    "href": "materials/cs3_practical_correlations.html#data-and-hypotheses",
    "title": "10  Correlations",
    "section": "10.3 Data and hypotheses",
    "text": "10.3 Data and hypotheses\nWe will use the USArrests data set for this example. This rather bleak data set contains statistics in arrests per 100,000 residents for assault, murder and robbery in each of the 50 US states in 1973, alongside the proportion of the population who lived in urban areas at that time. USArrests is a data frame with 50 observations of five variables: state, murder, assault, urban_pop and robbery.\nWe will be using these data to explore if there are correlations between these variables.\nThe data are stored in the file data/CS3-usarrests.csv.",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#summarise-and-visualise",
    "href": "materials/cs3_practical_correlations.html#summarise-and-visualise",
    "title": "10  Correlations",
    "section": "10.4 Summarise and visualise",
    "text": "10.4 Summarise and visualise\nFirst, we load the data:\n\nRPython\n\n\n\n# load the data\nUSArrests &lt;- read_csv(\"data/CS3-usarrests.csv\")\n\n# have a look at the data\nUSArrests\n\n# A tibble: 50 × 5\n   state       murder assault urban_pop robbery\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 Alabama       13.2     236        58    21.2\n 2 Alaska        10       263        48    44.5\n 3 Arizona        8.1     294        80    31  \n 4 Arkansas       8.8     190        50    19.5\n 5 California     9       276        91    40.6\n 6 Colorado       7.9     204        78    38.7\n 7 Connecticut    3.3     110        77    11.1\n 8 Delaware       5.9     238        72    15.8\n 9 Florida       15.4     335        80    31.9\n10 Georgia       17.4     211        60    25.8\n# ℹ 40 more rows\n\n\nWe can create a visual overview of the potential correlations that might exist between the variables. There are different ways of doing this, for example by creating scatter plots between variable pairs:\n\n# murder vs robbery\nggplot(USArrests,\n       aes(x = murder, y = robbery)) +\n    geom_point()\n\n\n\n\n\n\n\n# assault vs urban_pop\nggplot(USArrests,\n       aes(x = assault, y = urban_pop)) +\n    geom_point()\n\n\n\n\n\n\n\n\nThis gets a bit tedious if there are many unique variable pairs. Unfortunately ggplot() does not have a pairwise function, but we can borrow the one from base R. The pairs() function only wants numerical data, so we need to remove the state column for this. The pairs() function has a lower.panel argument that allows you to remove duplicate combinations (after all murder vs assault is the same as assault vs murder):\n\nUSArrests %&gt;% \n    select(-state) %&gt;% \n    pairs(lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\nFirst, we load the data:\n\nUSArrests_py = pd.read_csv(\"data/CS3-usarrests.csv\")\n\nUSArrests_py.head()\n\n        state  murder  assault  urban_pop  robbery\n0     Alabama    13.2      236         58     21.2\n1      Alaska    10.0      263         48     44.5\n2     Arizona     8.1      294         80     31.0\n3    Arkansas     8.8      190         50     19.5\n4  California     9.0      276         91     40.6\n\n\nWe can create a visual overview of the potential correlations that might exist between the variables. There are different ways of doing this, for example by creating scatter plots between variable pairs:\n\n# murder vs robbery\np = (ggplot(USArrests_py,\n       aes(x = \"murder\",\n           y = \"robbery\")) +\n     geom_point())\n\np.show()\n\n\n\n\n\n\n\n# assault vs urban_pop\np = (ggplot(USArrests_py,\n       aes(x = \"assault\",\n           y = \"urban_pop\")) +\n     geom_point())\n\np.show()\n\n\n\n\n\n\n\n\nThis gets a bit tedious if there are many unique variable pairs. There is an option to automatically create a matrix of scatter plots, using Seaborn. But that would involve installing the seaborn package just for this. And frankly, I don’t want to - not least because staring at tons of scatter plots is probably not the best way forward anyway!\nIf you have your heart set on creating a pairplot, then have a look at the seaborn documentation.\n\n\n\nFrom the visual inspection we can see that there appears to be a slight positive correlation between all pairs of variables, although this may be very weak in some cases (murder and urban_pop for example).",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#correlation-coefficients",
    "href": "materials/cs3_practical_correlations.html#correlation-coefficients",
    "title": "10  Correlations",
    "section": "10.5 Correlation coefficients",
    "text": "10.5 Correlation coefficients\nInstead of visualising the variables against each other in a scatter plot, we can also calculate correlation coefficients for each variable pair. There are different types of correlation coefficients, but the most well-known one is probably Pearson’s r. This is a measure of the linear correlation between two variables. It has a value between -1 and +1, where +1 means a perfect positive correlation, -1 means a perfect negative correlation and 0 means no correlation at all.\nThere are other correlation coefficients, most notably the Spearman’s rank correlation coefficient, a non-parametric measure of rank correlation and is generally less sensitive to outliers.\nSo, let’s calculate Pearson’s r for our data:\n\nRPython\n\n\nWe can do this using the cor() function. Since we can only calculate correlations between numbers, we have to remove the state column from our data before calculating the correlations:\n\nUSArrests %&gt;% \n    select(-state) %&gt;% \n    cor()\n\n              murder   assault  urban_pop   robbery\nmurder    1.00000000 0.8018733 0.06957262 0.5635788\nassault   0.80187331 1.0000000 0.25887170 0.6652412\nurban_pop 0.06957262 0.2588717 1.00000000 0.4113412\nrobbery   0.56357883 0.6652412 0.41134124 1.0000000\n\n\nThis gives us a numerical overview of the Pearson’s r correlation coefficients between each variable pair. Note that across the diagonal the correlation coefficients are 1 - this should make sense since, for example, murder is perfectly correlated with itself.\nAs before, the values are mirrored across the diagonal, since the correlation between, for example, murder and assault is the same as the one between assault and murder.\n\n10.5.1 Visualise the correlation matrix\nJust staring at a matrix of numbers might not be very useful. It would be good to create some sort of heatmap of the values, so we can visually inspect the data a bit better. There are dedicated packages that allow you to do this (for example the corrr) package).\nHere we’ll just use the standard stats::heatmap() function. The symm argument tells the function that we have a symmetric matrix and in conjunction with the Rowv = NA argument stops the plot from reordering the rows and columns. The Rowv = NA argument also stops the function from adding dendrograms to the margins of the plot.\nThe plot itself is coloured from yellow, indicating the smallest values (which in this case correspond to no difference in correlation coefficients), through orange to dark red, indicating the biggest values (which in this case correspond to the variables with the biggest difference in correlation coefficients).\nThe plot is symmetric along the leading diagonal (hopefully for obvious reasons).\n\nUSArrests %&gt;% \n  select(-state) %&gt;% \n  cor() %&gt;% \n  heatmap(symm = TRUE, Rowv = NA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteAlternative method 1: ggplot\n\n\n\n\n\nBefore we can plot the data we need to reformat the data. We’re taking the following steps:\n\nwe calculate the correlation matrix with cor() using the (default) method of method = \"pearson\"\nconvert the output to a tibble so we can use\npivot_longer() to reformat the data into pairwise variables and a column with the Pearson’s r value\nuse the mutate() and round() functions to round the Pearson’s r values\n\n\nUSArrests_pear &lt;- USArrests %&gt;% \n    select(-state) %&gt;% \n    cor(method = \"pearson\") %&gt;% \n    as_tibble(rownames = \"var1\") %&gt;% \n    pivot_longer(cols = -var1,\n                 names_to = \"var2\",\n                 values_to = \"pearson_cor\") %&gt;% \n    mutate(pearson_cor = round(pearson_cor, digits = 3))\n\nThe output of that looks like this:\n\nhead(USArrests_pear)\n\n# A tibble: 6 × 3\n  var1    var2      pearson_cor\n  &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;\n1 murder  murder          1    \n2 murder  assault         0.802\n3 murder  urban_pop       0.07 \n4 murder  robbery         0.564\n5 assault murder          0.802\n6 assault assault         1    \n\n\nAfter all that, we can visualise the data with geom_tile(), adding the Pearson’s r values as text labels:\n\nggplot(USArrests_pear,\n       aes(x = var1, y = var2, fill = pearson_cor)) +\n    geom_tile() +\n    geom_text(aes(label = pearson_cor),\n              color = \"white\",\n              size = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteAlternative method 2: rstatix\n\n\n\n\n\nAs always, there are multiple ways to skin a proverbial cat. If you’d rather use a function from the rstatix package (which we’ve loaded before), then you can run the following code, which uses the rstatix::cor_test() function:\n\nUSArrests %&gt;% \n    select(-state) %&gt;% \n    cor_test() %&gt;%\n    select(var1, var2, cor) %&gt;% \n    ggplot(aes(x = var1, y = var2, fill = cor)) +\n    geom_tile() +\n    geom_text(aes(label = cor),\n              color = \"white\",\n              size = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can do this using the pandas.DataFrame.corr() function. This function takes the default method = \"pearson\". It should ignore the state column, so we tell it to.\n\nUSArrests_py.corr(numeric_only = True)\n\n             murder   assault  urban_pop   robbery\nmurder     1.000000  0.801873   0.069573  0.563579\nassault    0.801873  1.000000   0.258872  0.665241\nurban_pop  0.069573  0.258872   1.000000  0.411341\nrobbery    0.563579  0.665241   0.411341  1.000000\n\n\nThis gives us a numerical overview of the Pearson’s r correlation coefficients between each variable pair. Note that across the diagonal the correlation coefficients are 1 - this should make sense since, for example, murder is perfectly correlated with itself.\nAs before, the values are mirrored across the diagonal, since the correlation between, for example, murder and assault is the same as the one between assault and murder.\n\n10.5.2 Visualise the correlation matrix\nJust staring at a matrix of numbers might not be very useful. It would be good to create some sort of heatmap of the values, so we can visually inspect the data a bit better.\n\n# create correlation matrix\nUSArrests_cor_py = USArrests_py.corr(numeric_only = True)\n# put the row names into a column\nUSArrests_cor_py = USArrests_cor_py.rename_axis(\"var1\").reset_index()\n\nUSArrests_cor_py.head()\n\n        var1    murder   assault  urban_pop   robbery\n0     murder  1.000000  0.801873   0.069573  0.563579\n1    assault  0.801873  1.000000   0.258872  0.665241\n2  urban_pop  0.069573  0.258872   1.000000  0.411341\n3    robbery  0.563579  0.665241   0.411341  1.000000\n\n\nNow that we have the correlation matrix in a workable format, we need to restructure it so that we can plot the data. For this, we need to create a “long” format, using the melt() function.\n\nUSArrests_pear_py = pd.melt(USArrests_cor_py,\n        id_vars=['var1'],\n        value_vars=['murder', 'assault', 'urban_pop', 'robbery'],\n        var_name='var2',\n        value_name='cor').round(3)\n\nHave a look at the structure:\n\nUSArrests_pear_py.head()\n\n        var1     var2    cor\n0     murder   murder  1.000\n1    assault   murder  0.802\n2  urban_pop   murder  0.070\n3    robbery   murder  0.564\n4     murder  assault  0.802\n\n\n\np = (ggplot(USArrests_pear_py,\n        aes(x = \"var1\", y = \"var2\", fill = \"cor\")) +\n     geom_tile() +\n     geom_text(aes(label = \"cor\"),\n               colour = \"white\",\n               size = 10))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe correlation matrix and visualisations give us the insight that we need. The most correlated variables are murder and assault with an \\(r\\) value of 0.80. This appears to agree well with the set plots that we produced earlier.",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#spearmans-rank-correlation-coefficient",
    "href": "materials/cs3_practical_correlations.html#spearmans-rank-correlation-coefficient",
    "title": "10  Correlations",
    "section": "10.6 Spearman’s rank correlation coefficient",
    "text": "10.6 Spearman’s rank correlation coefficient\nThis test first calculates the rank of the numerical data (i.e. their position from smallest (or most negative) to the largest (or most positive)) in the two variables and then calculates Pearson’s product moment correlation coefficient using the ranks. As a consequence, this test is less sensitive to outliers in the distribution.\n\nRPython\n\n\n\nUSArrests %&gt;% \n    select(-state) %&gt;% \n    cor(method = \"spearman\")\n\n             murder   assault urban_pop   robbery\nmurder    1.0000000 0.8172735 0.1067163 0.6794265\nassault   0.8172735 1.0000000 0.2752133 0.7143681\nurban_pop 0.1067163 0.2752133 1.0000000 0.4381068\nrobbery   0.6794265 0.7143681 0.4381068 1.0000000\n\n\n\n\n\nUSArrests_py.corr(method = \"spearman\", numeric_only = True)\n\n             murder   assault  urban_pop   robbery\nmurder     1.000000  0.817274   0.106716  0.679427\nassault    0.817274  1.000000   0.275213  0.714368\nurban_pop  0.106716  0.275213   1.000000  0.438107\nrobbery    0.679427  0.714368   0.438107  1.000000",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#exercises",
    "href": "materials/cs3_practical_correlations.html#exercises",
    "title": "10  Correlations",
    "section": "10.7 Exercises",
    "text": "10.7 Exercises\n\n10.7.1 Pearson’s r\n\n\n\n\n\n\nExerciseExercise 1\n\n\n\n\n\n\nLevel: \nPearson’s correlation for USA state data\nWe will again use the data from the file data/CS3-statedata.csv data set for this exercise. The data set contains 50 rows and 8 columns, with column names: population, income, illiteracy, life_exp, murder, hs_grad, frost and area.\nVisually identify 3 different pairs of variables that appear to be\n\nthe most positively correlated\nthe most negatively correlated\nnot correlated at all\n\nCalculate Pearson’s r for all variable pairs and see how well you were able to identify correlation visually.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n10.8 Answer\nVisually determining the most negative/positively and uncorrelated pairs of variables:\n\nRPython\n\n\n\nUSAstate &lt;- read_csv(\"data/CS3-statedata.csv\")\n\n# have a look at the data\nUSAstate\n\n# A tibble: 50 × 9\n   state       population income illiteracy life_exp murder hs_grad frost   area\n   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Alabama           3615   3624        2.1     69.0   15.1    41.3    20  50708\n 2 Alaska             365   6315        1.5     69.3   11.3    66.7   152 566432\n 3 Arizona           2212   4530        1.8     70.6    7.8    58.1    15 113417\n 4 Arkansas          2110   3378        1.9     70.7   10.1    39.9    65  51945\n 5 California       21198   5114        1.1     71.7   10.3    62.6    20 156361\n 6 Colorado          2541   4884        0.7     72.1    6.8    63.9   166 103766\n 7 Connecticut       3100   5348        1.1     72.5    3.1    56     139   4862\n 8 Delaware           579   4809        0.9     70.1    6.2    54.6   103   1982\n 9 Florida           8277   4815        1.3     70.7   10.7    52.6    11  54090\n10 Georgia           4931   4091        2       68.5   13.9    40.6    60  58073\n# ℹ 40 more rows\n\n\nWe basically repeat what we’ve done previously:\n\nUSAstate_pear &lt;-USAstate %&gt;% \n  select(-state) %&gt;% \n  cor(method = \"pearson\")\n\nNext, we can plot the data:\n\nheatmap(USAstate_pear, symm = TRUE, Rowv = NA)\n\n\n\n\n\n\n\n\n\n\nFirst, we load the data:\n\nUSAstate_py = pd.read_csv(\"data/CS3-statedata.csv\")\n\nUSAstate_py.head()\n\n        state  population  income  illiteracy  ...  murder  hs_grad  frost    area\n0     Alabama        3615    3624         2.1  ...    15.1     41.3     20   50708\n1      Alaska         365    6315         1.5  ...    11.3     66.7    152  566432\n2     Arizona        2212    4530         1.8  ...     7.8     58.1     15  113417\n3    Arkansas        2110    3378         1.9  ...    10.1     39.9     65   51945\n4  California       21198    5114         1.1  ...    10.3     62.6     20  156361\n\n[5 rows x 9 columns]\n\n\n\n# create correlation matrix\nUSAstate_cor_py = USAstate_py.corr(numeric_only = True)\n\n# put the row names into a column\nUSAstate_cor_py = USAstate_cor_py.rename_axis(\"var1\").reset_index()\n\nUSAstate_cor_py.head()\n\n         var1  population    income  ...   hs_grad     frost      area\n0  population    1.000000  0.208228  ... -0.098490 -0.332152  0.022544\n1      income    0.208228  1.000000  ...  0.619932  0.226282  0.363315\n2  illiteracy    0.107622 -0.437075  ... -0.657189 -0.671947  0.077261\n3    life_exp   -0.068052  0.340255  ...  0.582216  0.262068 -0.107332\n4      murder    0.343643 -0.230078  ... -0.487971 -0.538883  0.228390\n\n[5 rows x 9 columns]\n\n\nNow that we have the correlation matrix in a workable format, we need to restructure it so that we can plot the data. For this, we need to create a “long” format, using the melt() function. Note that we’re not setting the values_var argument. If not set, then it uses all but the id_vars column (which in our case is a good thing, since we don’t want to manually specify lots of column names).\n\nUSAstate_pear_py = pd.melt(USAstate_cor_py,\n        id_vars=['var1'],\n        var_name='var2',\n        value_name='cor').round(3)\n\nHave a look at the structure:\n\nUSArrests_pear_py.head()\n\n        var1     var2    cor\n0     murder   murder  1.000\n1    assault   murder  0.802\n2  urban_pop   murder  0.070\n3    robbery   murder  0.564\n4     murder  assault  0.802\n\n\n\np = (ggplot(USAstate_pear_py,\n        aes(x = \"var1\", y = \"var2\", fill = \"cor\")) +\n     geom_tile() +\n     geom_text(aes(label = \"cor\"),\n               colour = \"white\",\n               size = 10))\n\n\n\n\nIt looks like:\n\nilliteracy and murder are the most positively correlated pair\nlife_exp and murder are the most negatively correlated pair\npopulation and area are the least correlated pair\n\nWe can explore that numerically, by doing the following:\n\nRPython\n\n\nFirst, we need to create the pairwise comparisons, with the relevant Pearson’s \\(r\\) values:\n\n# build a contingency table with as.table()\n# and create a dataframe with as.data.frame()\nUSAstate_pear_cont &lt;- as.data.frame(as.table(USAstate_pear))\n    \n# and have a look\nhead(USAstate_pear_cont)\n\n        Var1       Var2        Freq\n1 population population  1.00000000\n2     income population  0.20822756\n3 illiteracy population  0.10762237\n4   life_exp population -0.06805195\n5     murder population  0.34364275\n6    hs_grad population -0.09848975\n\n\nIs this method obvious? No! Some creative Googling led to Stackoverflow and here we are. But, it does give us what we need.\nNow that we have the paired comparisons, we can extract the relevant data:\n\n# first we remove the same-pair correlations\nUSAstate_pear_cont &lt;- USAstate_pear_cont %&gt;% \n  filter(Freq != 1)\n\n# most positively correlated pair\nUSAstate_pear_cont %&gt;% \n  filter(Freq == max(Freq))\n\n        Var1       Var2      Freq\n1     murder illiteracy 0.7029752\n2 illiteracy     murder 0.7029752\n\n# most negatively correlated pair\nUSAstate_pear_cont %&gt;% \n  filter(Freq == min(Freq))\n\n      Var1     Var2       Freq\n1   murder life_exp -0.7808458\n2 life_exp   murder -0.7808458\n\n# least correlated pair\nUSAstate_pear_cont %&gt;% \n  filter(Freq == min(abs(Freq)))\n\n        Var1       Var2       Freq\n1       area population 0.02254384\n2 population       area 0.02254384\n\n\nNote that we use the minimum absolute value (with the abs() function) to find the least correlated pair.\n\n\nWe take the correlation matrix in the long format:\n\nUSAstate_pear_py.head()\n\n         var1        var2    cor\n0  population  population  1.000\n1      income  population  0.208\n2  illiteracy  population  0.108\n3    life_exp  population -0.068\n4      murder  population  0.344\n\n\nand use it to extract the relevant values:\n\n# filter out self-pairs\ndf_cor = USAstate_pear_py.query(\"cor != 1\")\n\n# filter for the maximum correlation value\ndf_cor[df_cor.cor == df_cor.cor.max()]\n\n          var1        var2    cor\n20      murder  illiteracy  0.703\n34  illiteracy      murder  0.703\n\n# filter for the minimum correlation value\ndf_cor[df_cor.cor == df_cor.cor.min()]\n\n        var1      var2    cor\n28    murder  life_exp -0.781\n35  life_exp    murder -0.781\n\n# filter for the least correlated value\n# create a column containing absolute values\ndf_cor[\"abs_cor\"] = df_cor[\"cor\"].abs()\ndf_cor[df_cor.abs_cor == df_cor.abs_cor.min()]\n\n          var1        var2    cor  abs_cor\n7         area  population  0.023    0.023\n56  population        area  0.023    0.023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.8.1 Spearman’s correlation\n\n\n\n\n\n\nExerciseExercise 2\n\n\n\n\n\n\nLevel: \nCalculate Spearman’s correlation coefficient for the data/CS3-statedata.csv data set.\nWhich variable’s correlations are affected most by the use of the Spearman’s rank compared with Pearson’s r? Hint: think of a way to address this question programmatically.\nThinking about the variables, can you explain why this might this be?\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n10.9 Answer\nIn order to determine which variables are most affected by the choice of Spearman vs Pearson you could just plot both matrices out side by side and try to spot what was going on, but one of the reasons we’re using programming languages is that we can be a bit more programmatic about these things. Also, our eyes aren’t that good at processing and parsing this sort of information display. A better way would be to somehow visualise the data.\n\nRPython\n\n\nFirst, calculate the Pearson and Spearman correlation matrices (technically, we’ve done the Pearson one already, but we’re doing it again for clarity here).\n\ncor_pear &lt;- USAstate %&gt;% \n    select(-state) %&gt;% \n    cor(method = \"pearson\")\n\ncor_spear &lt;- USAstate %&gt;% \n    select(-state) %&gt;% \n    cor(method = \"spearman\")\n\nWe can calculate the difference between two matrices by subtracting them.\n\ncor_diff &lt;- cor_pear - cor_spear\n\nAgain, we could now just look at a grid of 64 numbers and see if we can spot the biggest differences, but our eyes aren’t that good at processing and parsing this sort of information display. A better way would be to visualise the data.\n\nheatmap(abs(cor_diff), symm = TRUE, Rowv = NA)\n\n\n\n\n\n\n\n\nThe abs() function calculates the absolute value (i.e. just the magnitude) of the matrix values. This is just because I only care about situations where the two correlation coefficients are different from each other but I don’t care which is the larger. The symm argument tells the function that we have a symmetric matrix and in conjunction with the Rowv = NA argument stops the plot from reordering the rows and columns. The Rowv = NA argument also stops the function from adding dendrograms to the margins of the plot.\n\n\nFirst, calculate the Pearson and Spearman correlation matrices (technically, we’ve done the Pearson one already, but we’re doing it again for clarity here).\n\ncor_pear_py = USAstate_py.corr(method = \"pearson\", numeric_only = True)\ncor_spea_py = USAstate_py.corr(method = \"spearman\", numeric_only = True)\n\nWe can calculate the difference between two matrices by subtracting them.\n\ncor_dif_py = cor_pear_py - cor_spea_py\n\nAgain, we could now just look at a grid of 64 numbers and see if we can spot the biggest differences, but our eyes aren’t that good at processing and parsing this sort of information display. A better way would be to visualise the data.\n\n# get the row names in a column\ncor_dif_py = cor_dif_py.rename_axis(\"var1\").reset_index()\n\n# reformat the data into a long format\n# and round the values\ncor_dif_py = pd.melt(cor_dif_py,\n        id_vars=['var1'],\n        var_name='var2',\n        value_name='cor').round(3)\n        \n# create a column with absolute correlation difference values\ncor_dif_py[\"abs_cor\"] = cor_dif_py[\"cor\"].abs()\n\n# have a look at the final data frame\ncor_dif_py.head()\n\n         var1        var2    cor  abs_cor\n0  population  population  0.000    0.000\n1      income  population  0.084    0.084\n2  illiteracy  population -0.205    0.205\n3    life_exp  population  0.036    0.036\n4      murder  population -0.002    0.002\n\n\nNow we can plot the data:\n\np = (ggplot(cor_dif_py,\n        aes(x = \"var1\", y = \"var2\", fill = \"abs_cor\")) +\n     geom_tile() +\n     geom_text(aes(label = \"abs_cor\"),\n               colour = \"white\",\n               size = 10))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nAll in all there is not a huge difference in correlation coefficients, since the values are all quite small. Most of the changes occur along the area variable. One possible explanation could be that certain states with a large area have a relatively large effect on the Pearson’s r coefficient. For example, Alaska has an area that is over twice as big as the next state - Texas.\nIf, for example, we’d look a bit closer then we would find for area and income that Pearson gives a value of 0.36, a slight positive correlation, whereas Spearman gives a value of 0.057, basically uncorrelated.\nThis means that this is basically ignored by Spearman.\nWell done, Mr. Spearman.",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#answer",
    "href": "materials/cs3_practical_correlations.html#answer",
    "title": "10  Correlations",
    "section": "10.8 Answer",
    "text": "10.8 Answer\nVisually determining the most negative/positively and uncorrelated pairs of variables:\n\nRPython\n\n\n\nUSAstate &lt;- read_csv(\"data/CS3-statedata.csv\")\n\n# have a look at the data\nUSAstate\n\n# A tibble: 50 × 9\n   state       population income illiteracy life_exp murder hs_grad frost   area\n   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Alabama           3615   3624        2.1     69.0   15.1    41.3    20  50708\n 2 Alaska             365   6315        1.5     69.3   11.3    66.7   152 566432\n 3 Arizona           2212   4530        1.8     70.6    7.8    58.1    15 113417\n 4 Arkansas          2110   3378        1.9     70.7   10.1    39.9    65  51945\n 5 California       21198   5114        1.1     71.7   10.3    62.6    20 156361\n 6 Colorado          2541   4884        0.7     72.1    6.8    63.9   166 103766\n 7 Connecticut       3100   5348        1.1     72.5    3.1    56     139   4862\n 8 Delaware           579   4809        0.9     70.1    6.2    54.6   103   1982\n 9 Florida           8277   4815        1.3     70.7   10.7    52.6    11  54090\n10 Georgia           4931   4091        2       68.5   13.9    40.6    60  58073\n# ℹ 40 more rows\n\n\nWe basically repeat what we’ve done previously:\n\nUSAstate_pear &lt;-USAstate %&gt;% \n  select(-state) %&gt;% \n  cor(method = \"pearson\")\n\nNext, we can plot the data:\n\nheatmap(USAstate_pear, symm = TRUE, Rowv = NA)\n\n\n\n\n\n\n\n\n\n\nFirst, we load the data:\n\nUSAstate_py = pd.read_csv(\"data/CS3-statedata.csv\")\n\nUSAstate_py.head()\n\n        state  population  income  illiteracy  ...  murder  hs_grad  frost    area\n0     Alabama        3615    3624         2.1  ...    15.1     41.3     20   50708\n1      Alaska         365    6315         1.5  ...    11.3     66.7    152  566432\n2     Arizona        2212    4530         1.8  ...     7.8     58.1     15  113417\n3    Arkansas        2110    3378         1.9  ...    10.1     39.9     65   51945\n4  California       21198    5114         1.1  ...    10.3     62.6     20  156361\n\n[5 rows x 9 columns]\n\n\n\n# create correlation matrix\nUSAstate_cor_py = USAstate_py.corr(numeric_only = True)\n\n# put the row names into a column\nUSAstate_cor_py = USAstate_cor_py.rename_axis(\"var1\").reset_index()\n\nUSAstate_cor_py.head()\n\n         var1  population    income  ...   hs_grad     frost      area\n0  population    1.000000  0.208228  ... -0.098490 -0.332152  0.022544\n1      income    0.208228  1.000000  ...  0.619932  0.226282  0.363315\n2  illiteracy    0.107622 -0.437075  ... -0.657189 -0.671947  0.077261\n3    life_exp   -0.068052  0.340255  ...  0.582216  0.262068 -0.107332\n4      murder    0.343643 -0.230078  ... -0.487971 -0.538883  0.228390\n\n[5 rows x 9 columns]\n\n\nNow that we have the correlation matrix in a workable format, we need to restructure it so that we can plot the data. For this, we need to create a “long” format, using the melt() function. Note that we’re not setting the values_var argument. If not set, then it uses all but the id_vars column (which in our case is a good thing, since we don’t want to manually specify lots of column names).\n\nUSAstate_pear_py = pd.melt(USAstate_cor_py,\n        id_vars=['var1'],\n        var_name='var2',\n        value_name='cor').round(3)\n\nHave a look at the structure:\n\nUSArrests_pear_py.head()\n\n        var1     var2    cor\n0     murder   murder  1.000\n1    assault   murder  0.802\n2  urban_pop   murder  0.070\n3    robbery   murder  0.564\n4     murder  assault  0.802\n\n\n\np = (ggplot(USAstate_pear_py,\n        aes(x = \"var1\", y = \"var2\", fill = \"cor\")) +\n     geom_tile() +\n     geom_text(aes(label = \"cor\"),\n               colour = \"white\",\n               size = 10))\n\n\n\n\nIt looks like:\n\nilliteracy and murder are the most positively correlated pair\nlife_exp and murder are the most negatively correlated pair\npopulation and area are the least correlated pair\n\nWe can explore that numerically, by doing the following:\n\nRPython\n\n\nFirst, we need to create the pairwise comparisons, with the relevant Pearson’s \\(r\\) values:\n\n# build a contingency table with as.table()\n# and create a dataframe with as.data.frame()\nUSAstate_pear_cont &lt;- as.data.frame(as.table(USAstate_pear))\n    \n# and have a look\nhead(USAstate_pear_cont)\n\n        Var1       Var2        Freq\n1 population population  1.00000000\n2     income population  0.20822756\n3 illiteracy population  0.10762237\n4   life_exp population -0.06805195\n5     murder population  0.34364275\n6    hs_grad population -0.09848975\n\n\nIs this method obvious? No! Some creative Googling led to Stackoverflow and here we are. But, it does give us what we need.\nNow that we have the paired comparisons, we can extract the relevant data:\n\n# first we remove the same-pair correlations\nUSAstate_pear_cont &lt;- USAstate_pear_cont %&gt;% \n  filter(Freq != 1)\n\n# most positively correlated pair\nUSAstate_pear_cont %&gt;% \n  filter(Freq == max(Freq))\n\n        Var1       Var2      Freq\n1     murder illiteracy 0.7029752\n2 illiteracy     murder 0.7029752\n\n# most negatively correlated pair\nUSAstate_pear_cont %&gt;% \n  filter(Freq == min(Freq))\n\n      Var1     Var2       Freq\n1   murder life_exp -0.7808458\n2 life_exp   murder -0.7808458\n\n# least correlated pair\nUSAstate_pear_cont %&gt;% \n  filter(Freq == min(abs(Freq)))\n\n        Var1       Var2       Freq\n1       area population 0.02254384\n2 population       area 0.02254384\n\n\nNote that we use the minimum absolute value (with the abs() function) to find the least correlated pair.\n\n\nWe take the correlation matrix in the long format:\n\nUSAstate_pear_py.head()\n\n         var1        var2    cor\n0  population  population  1.000\n1      income  population  0.208\n2  illiteracy  population  0.108\n3    life_exp  population -0.068\n4      murder  population  0.344\n\n\nand use it to extract the relevant values:\n\n# filter out self-pairs\ndf_cor = USAstate_pear_py.query(\"cor != 1\")\n\n# filter for the maximum correlation value\ndf_cor[df_cor.cor == df_cor.cor.max()]\n\n          var1        var2    cor\n20      murder  illiteracy  0.703\n34  illiteracy      murder  0.703\n\n# filter for the minimum correlation value\ndf_cor[df_cor.cor == df_cor.cor.min()]\n\n        var1      var2    cor\n28    murder  life_exp -0.781\n35  life_exp    murder -0.781\n\n# filter for the least correlated value\n# create a column containing absolute values\ndf_cor[\"abs_cor\"] = df_cor[\"cor\"].abs()\ndf_cor[df_cor.abs_cor == df_cor.abs_cor.min()]\n\n          var1        var2    cor  abs_cor\n7         area  population  0.023    0.023\n56  population        area  0.023    0.023",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#answer-1",
    "href": "materials/cs3_practical_correlations.html#answer-1",
    "title": "10  Correlations",
    "section": "10.9 Answer",
    "text": "10.9 Answer\nIn order to determine which variables are most affected by the choice of Spearman vs Pearson you could just plot both matrices out side by side and try to spot what was going on, but one of the reasons we’re using programming languages is that we can be a bit more programmatic about these things. Also, our eyes aren’t that good at processing and parsing this sort of information display. A better way would be to somehow visualise the data.\n\nRPython\n\n\nFirst, calculate the Pearson and Spearman correlation matrices (technically, we’ve done the Pearson one already, but we’re doing it again for clarity here).\n\ncor_pear &lt;- USAstate %&gt;% \n    select(-state) %&gt;% \n    cor(method = \"pearson\")\n\ncor_spear &lt;- USAstate %&gt;% \n    select(-state) %&gt;% \n    cor(method = \"spearman\")\n\nWe can calculate the difference between two matrices by subtracting them.\n\ncor_diff &lt;- cor_pear - cor_spear\n\nAgain, we could now just look at a grid of 64 numbers and see if we can spot the biggest differences, but our eyes aren’t that good at processing and parsing this sort of information display. A better way would be to visualise the data.\n\nheatmap(abs(cor_diff), symm = TRUE, Rowv = NA)\n\n\n\n\n\n\n\n\nThe abs() function calculates the absolute value (i.e. just the magnitude) of the matrix values. This is just because I only care about situations where the two correlation coefficients are different from each other but I don’t care which is the larger. The symm argument tells the function that we have a symmetric matrix and in conjunction with the Rowv = NA argument stops the plot from reordering the rows and columns. The Rowv = NA argument also stops the function from adding dendrograms to the margins of the plot.\n\n\nFirst, calculate the Pearson and Spearman correlation matrices (technically, we’ve done the Pearson one already, but we’re doing it again for clarity here).\n\ncor_pear_py = USAstate_py.corr(method = \"pearson\", numeric_only = True)\ncor_spea_py = USAstate_py.corr(method = \"spearman\", numeric_only = True)\n\nWe can calculate the difference between two matrices by subtracting them.\n\ncor_dif_py = cor_pear_py - cor_spea_py\n\nAgain, we could now just look at a grid of 64 numbers and see if we can spot the biggest differences, but our eyes aren’t that good at processing and parsing this sort of information display. A better way would be to visualise the data.\n\n# get the row names in a column\ncor_dif_py = cor_dif_py.rename_axis(\"var1\").reset_index()\n\n# reformat the data into a long format\n# and round the values\ncor_dif_py = pd.melt(cor_dif_py,\n        id_vars=['var1'],\n        var_name='var2',\n        value_name='cor').round(3)\n        \n# create a column with absolute correlation difference values\ncor_dif_py[\"abs_cor\"] = cor_dif_py[\"cor\"].abs()\n\n# have a look at the final data frame\ncor_dif_py.head()\n\n         var1        var2    cor  abs_cor\n0  population  population  0.000    0.000\n1      income  population  0.084    0.084\n2  illiteracy  population -0.205    0.205\n3    life_exp  population  0.036    0.036\n4      murder  population -0.002    0.002\n\n\nNow we can plot the data:\n\np = (ggplot(cor_dif_py,\n        aes(x = \"var1\", y = \"var2\", fill = \"abs_cor\")) +\n     geom_tile() +\n     geom_text(aes(label = \"abs_cor\"),\n               colour = \"white\",\n               size = 10))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nAll in all there is not a huge difference in correlation coefficients, since the values are all quite small. Most of the changes occur along the area variable. One possible explanation could be that certain states with a large area have a relatively large effect on the Pearson’s r coefficient. For example, Alaska has an area that is over twice as big as the next state - Texas.\nIf, for example, we’d look a bit closer then we would find for area and income that Pearson gives a value of 0.36, a slight positive correlation, whereas Spearman gives a value of 0.057, basically uncorrelated.\nThis means that this is basically ignored by Spearman.\nWell done, Mr. Spearman.",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#summary",
    "href": "materials/cs3_practical_correlations.html#summary",
    "title": "10  Correlations",
    "section": "10.10 Summary",
    "text": "10.10 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nCorrelation is the degree to which two variables are linearly related\nCorrelation does not imply causation\nWe can visualise correlations by plotting variables against each other or creating heatmap-type plots of the correlation coefficients\nTwo main correlation coefficients are Pearson’s r and Spearman’s rank, with Spearman’s rank being less sensitive to outliers",
    "crumbs": [
      "CS3: Continuous predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html",
    "href": "materials/cs4_practical_two-way-anova.html",
    "title": "11  Two-way ANOVA",
    "section": "",
    "text": "11.1 Libraries and functions",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#libraries-and-functions",
    "href": "materials/cs4_practical_two-way-anova.html#libraries-and-functions",
    "title": "11  Two-way ANOVA",
    "section": "",
    "text": "NoteClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n11.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n\n\n11.1.2 Functions\n\n# Creates diagnostic plots\nggResidpanel::resid_panel()\n\n# Creates a linear model\nstats::lm()\n\n# Creates an ANOVA table for a linear model\nstats::anova()\n\n\n\n\n\n11.1.3 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n# Custom Python script for diagnostic plots, load from base working directory\nfrom dgplots import dgplots\n\n\n\n11.1.4 Functions\n\n# Summary statistics\npandas.DataFrame.describe()\n\n# Plots the first few rows of a DataFrame\npandas.DataFrame.head()\n\n# Reads in a .csv file\npandas.DataFrame.read_csv()\n\n# Creates a model from a formula and data frame\nstatsmodels.formula.api.ols()\n\n# Creates an ANOVA table for one or more fitted linear models\nstatsmodels.stats.anova.anova_lm()\n\n# Custom function to create diagnostic plots\ndgplots()\n\nNote: you can download the dgplots script here.",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#purpose-and-aim",
    "href": "materials/cs4_practical_two-way-anova.html#purpose-and-aim",
    "title": "11  Two-way ANOVA",
    "section": "11.2 Purpose and aim",
    "text": "11.2 Purpose and aim\nA two-way analysis of variance is used when we have two categorical predictor variables (or factors) and a single continuous response variable. For example, when we are looking at how body weight (continuous response variable in kilograms) is affected by sex (categorical variable, male or female) and exercise type (categorical variable, control or runner).\n\n\n\n\n\n\n\n\n\nWhen analysing these type of data there are two things we want to know:\n\nDoes either of the predictor variables have an effect on the response variable i.e. does sex affect body weight? Or does being a runner affect body weight?\nIs there any interaction between the two predictor variables? An interaction would mean that the effect that exercise has on your weight depends on whether you are male or female rather than being independent of your sex. For example if being male means that runners weigh more than non-runners, but being female means that runners weight less than non-runners then we would say that there was an interaction.\n\nWe will first consider how to visualise the data before then carrying out an appropriate statistical test.",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#data-and-hypotheses",
    "href": "materials/cs4_practical_two-way-anova.html#data-and-hypotheses",
    "title": "11  Two-way ANOVA",
    "section": "11.3 Data and hypotheses",
    "text": "11.3 Data and hypotheses\nWe will recreate the example analysis used in the lecture. The data are stored as a .csv file called data/CS4-exercise.csv.",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#summarise-and-visualise",
    "href": "materials/cs4_practical_two-way-anova.html#summarise-and-visualise",
    "title": "11  Two-way ANOVA",
    "section": "11.4 Summarise and visualise",
    "text": "11.4 Summarise and visualise\nexercise is a data frame with three variables; weight, sex and exercise. weight is the continuous response variable, whereas sex and exercise are the categorical predictor variables.\n\nRPython\n\n\nFirst, we read in the data:\n\nexercise &lt;- read_csv(\"data/CS4-exercise.csv\")\n\nYou can visualise the data with:\n\n# visualise the data, sex vs weight\nggplot(exercise,\n       aes(x = sex, y = weight)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n# visualise the data, exercise vs weight\nggplot(exercise,\n       aes(x = exercise, y = weight)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nFirst, we read in the data:\n\nexercise_py = pd.read_csv(\"data/CS4-exercise.csv\")\n\nYou can visualise the data with:\n\n# visualise the data, sex vs weight\np = (ggplot(exercise_py,\n        aes(x = \"sex\", y = \"weight\")) +\n  geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n# visualise the data, exercise vs weight\np = (ggplot(exercise_py,\n        aes(x = \"exercise\", y = \"weight\")) +\n  geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nThese produce box plots showing the response variable (weight) only in terms of one of the predictor variables. The values of the other predictor variable in each case aren’t taken into account.\nA better way would be to visualise both variables at the same time. We can do this as follows:\n\nRPython\n\n\n\nggplot(exercise,\n       aes(x = sex, y = weight, fill = exercise)) +\n  geom_boxplot() +\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nThis produces box plots for all (four) combinations of the predictor variables. We are plotting sex on the x-axis; weight on the y-axis and filling the box plot by exercise regime.\nHere I’ve also changed the default colouring scheme, by using scale_fill_brewer(palette = \"Dark2\"). This uses a colour-blind friendly colour palette (more about the Brewer colour pallete here).\n\n\n\np = (ggplot(exercise_py,\n        aes(x = \"sex\",\n            y = \"weight\", fill = \"exercise\")) +\n     geom_boxplot() +\n     scale_fill_brewer(type = \"qual\", palette = \"Dark2\"))\n\np.show()\n\n\n\n\n\n\n\n\nThis produces box plots for all (four) combinations of the predictor variables. We are plotting sex on the x-axis; weight on the y-axis and filling the box plot by exercise regime.\nHere I’ve also changed the default colouring scheme, by using scale_fill_brewer(type = \"qual\", palette = \"Dark2\"). This uses a colour-blind friendly colour palette (more about the Brewer colour pallete here).\n\n\n\nIn this example there are only four box plots and so it is relatively easy to compare them and look for any interactions between variables, but if there were more than two groups per categorical variable, it would become harder to spot what was going on.\nTo compare categorical variables more easily we can just plot the group means which aids our ability to look for interactions and the main effects of each predictor variable. This is called an interaction plot.\nCreate an interaction plot:\n\nRPython\n\n\nWe’re adding a bit of jitter to the data, to avoid too much overlap between the data points. We can do this with geom_jitter().\n\nggplot(exercise,\n       aes(x = sex, y = weight,\n           colour = exercise, group = exercise)) +\n  geom_jitter(width = 0.05) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nHere we plot weight on the y-axis, by sex on the x-axis.\n\nwe colour the data by exercise regime and group the data by exercise to work out the mean values of each group\ngeom_jitter(width = 0.05) displays the data, with a tiny bit of random noise, to separate the data points a bit for visualisation\nstat_summary(fun = mean)calculates the mean for each group\nscale_colour_brewer() lets us define the colour palette\n\nThe choice of which categorical factor is plotted on the horizontal axis and which is plotted as different lines is completely arbitrary. Looking at the data both ways shouldn’t add anything but often you’ll find that you prefer one plot to another.\nPlot the interaction plot the other way round:\n\nggplot(exercise,\n       aes(x = exercise, y = weight,\n           colour = sex, group = sex)) +\n  geom_jitter(width = 0.05) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\n\n\nWe’re adding a bit of jitter to the data, to avoid too much overlap between the data points. We can do this with geom_jitter().\n\np = (ggplot(exercise_py,\n        aes(x = \"sex\", y = \"weight\",\n            colour = \"exercise\", group = \"exercise\")) +\n     geom_jitter(width = 0.05) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\np.show()\n\n\n\n\n\n\n\n\nHere we plot weight on the y-axis, by sex on the x-axis.\n\nwe colour the data by exercise regime and group the data by exercise to work out the mean values of each group\ngeom_jitter(width = 0.05) displays the data, with a tiny bit of random noise, to separate the data points a bit for visualisation\nstat_summary(fun_data = \"mean_cl_boot\")calculates the mean for each group\nscale_colour_brewer() lets us define the colour palette\n\nThe choice of which categorical factor is plotted on the horizontal axis and which is plotted as different lines is completely arbitrary. Looking at the data both ways shouldn’t add anything but often you’ll find that you prefer one plot to another.\nPlot the interaction plot the other way round:\n\np = (ggplot(exercise_py,\n        aes(x = \"exercise\", y = \"weight\",\n            colour = \"sex\", group = \"sex\")) +\n     geom_jitter(width = 0.05) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n  scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nBy now you should have a good feeling for the data and could already provide some guesses to the following three questions:\n\nDoes there appear to be any interaction between the two categorical variables?\nIf not:\n\nDoes exercise have an effect on weight?\nDoes sex have an effect on weight?\n\n\nWe can now attempt to answer these three questions more formally using an ANOVA test. We have to test for three things: the interaction, the effect of exercise and the effect of sex.",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#assumptions",
    "href": "materials/cs4_practical_two-way-anova.html#assumptions",
    "title": "11  Two-way ANOVA",
    "section": "11.5 Assumptions",
    "text": "11.5 Assumptions\nBefore we can formally test these things we first need to define the model and check the underlying assumptions. We use the following code to define the model:\n\nRPython\n\n\n\n# define the linear model\nlm_exercise &lt;- lm(weight ~ sex + exercise + sex:exercise,\n                  data = exercise)\n\nThe sex:exercise term is how R represents the concept of an interaction between these two variables.\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"weight ~ exercise * sex\", data = exercise_py)\n# and get the fitted parameters of the model\nlm_exercise_py = model.fit()\n\nThe formula weight ~ exercise * sex can be read as “weight depends on exercise and sex and the interaction between exercise and sex.\n\n\n\nAs the two-way ANOVA is a type of linear model we need to satisfy pretty much the same assumptions as we did for a simple linear regression or a one-way ANOVA:\n\nThe data must not have any systematic pattern to it\nThe residuals must be normally distributed\nThe residuals must have homogeneity of variance\nThe fit should not depend overly much on a single point (no point should have high leverage).\n\nAgain, we will check these assumptions visually by producing four key diagnostic plots.\n\nRPython\n\n\n\nresid_panel(lm_exercise,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\nThe Residual plot shows the residuals against the predicted values. There is no systematic pattern here and this plot is pretty good.\nThe Q-Q plot allows a visual inspection of normality. Again, this looks OK (not perfect but OK).\nThe Location-Scale plot allows us to investigate whether there is homogeneity of variance. This plot is fine (not perfect but fine).\nThe Cook’s D plot shows that no individual point has a high influence on the model (all values are well below 0.5)\n\n\nThere is a shorthand way of writing:\nweight ~ sex + exercise + sex:exercise\nIf you use the following syntax:\nweight ~ sex * exercise\nThen R interprets it exactly the same way as writing all three terms. You can see this if you compare the output of the following two commands:\n\nanova(lm(weight ~ sex + exercise + sex:exercise,\n         data = exercise))\n\nAnalysis of Variance Table\n\nResponse: weight\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nsex            1 4510.1  4510.1 366.911 &lt; 2.2e-16 ***\nexercise       1 1312.0  1312.0 106.733 &lt; 2.2e-16 ***\nsex:exercise   1  404.4   404.4  32.902 4.889e-08 ***\nResiduals    156 1917.6    12.3                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm(weight ~ sex * exercise,\n         data = exercise))\n\nAnalysis of Variance Table\n\nResponse: weight\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nsex            1 4510.1  4510.1 366.911 &lt; 2.2e-16 ***\nexercise       1 1312.0  1312.0 106.733 &lt; 2.2e-16 ***\nsex:exercise   1  404.4   404.4  32.902 4.889e-08 ***\nResiduals    156 1917.6    12.3                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\ndgplots(lm_exercise_py)",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#implement-and-interpret-test",
    "href": "materials/cs4_practical_two-way-anova.html#implement-and-interpret-test",
    "title": "11  Two-way ANOVA",
    "section": "11.6 Implement and interpret test",
    "text": "11.6 Implement and interpret test\nThe assumptions appear to be met well enough, meaning we can implement the ANOVA. We do this as follows (this is probably the easiest bit!):\n\nRPython\n\n\n\n# perform the ANOVA\nanova(lm_exercise)\n\nAnalysis of Variance Table\n\nResponse: weight\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nsex            1 4510.1  4510.1 366.911 &lt; 2.2e-16 ***\nexercise       1 1312.0  1312.0 106.733 &lt; 2.2e-16 ***\nsex:exercise   1  404.4   404.4  32.902 4.889e-08 ***\nResiduals    156 1917.6    12.3                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe have a row in the table for each of the different effects that we’ve asked R to consider. The last column is the important one as this contains the p-values. We need to look at the interaction row first.\n\n\n\nsm.stats.anova_lm(lm_exercise_py, typ = 2)\n\n                   sum_sq     df           F        PR(&gt;F)\nexercise      1311.970522    1.0  106.733448  2.177106e-19\nsex           4636.450232    1.0  377.191645  1.760076e-43\nexercise:sex   404.434414    1.0   32.902172  4.889216e-08\nResidual      1917.556353  156.0         NaN           NaN\n\n\nWe have a row in the table for each of the different effects that we’ve asked Python to consider. The last column is the important one as this contains the p-values. We need to look at the interaction row first.\n\n\n\nsex:exercise has a p-value of about 4.89e-08 (which is smaller than 0.05) and so we can conclude that the interaction between sex and exercise is significant.\nThis is where we must stop.\nThe top two lines (corresponding to the effects of sex and exercise) are meaningless now. This is because the interaction means that we cannot interpret the main effects independently.\nIn this case, weight depends on and the sex and the exercise regime. This means the effect of sex on weight is dependent on exercise (and vice-versa).\nWe would report this as follows:\n\nA two-way ANOVA test showed that there was a significant interaction between the effects of sex and exercise on weight (p = 4.89e-08). Exercise was associated with a small loss of weight in males but a larger loss of weight in females.",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#exercises",
    "href": "materials/cs4_practical_two-way-anova.html#exercises",
    "title": "11  Two-way ANOVA",
    "section": "11.7 Exercises",
    "text": "11.7 Exercises\n\n11.7.1 Auxin response\n\n\n\n\n\n\nExerciseExercise 1\n\n\n\n\n\n\nLevel: \nPlant height responses to auxin in different genotypes\nThese data/CS4-auxin.csv data are from a simulated experiment that looks at the effect of the plant hormone auxin on plant height.\nThe experiment consists of two genotypes: a wild type control and a mutant (genotype). The plants are treated with auxin at different concentrations: none, low and high, which are stored in the concentration column.\nThe response variable plant height (plant_height) is then measured at the end of their life cycle, in centimeters.\nQuestions to answer:\n\nVisualise the data using boxplots and interaction plots.\nDoes there appear to be any interaction between genotype and concentration?\nCarry out a two-way ANOVA test.\nCheck the assumptions.\nWhat can you conclude? (Write a sentence to summarise).\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n11.8 Answer\n\nRPython\n\n\n\nLoad the data\n\n# read in the data\nauxin_response &lt;- read_csv(\"data/CS4-auxin.csv\")\n\n# let's have a peek at the data\nhead(auxin_response)\n\n# A tibble: 6 × 3\n  genotype concentration plant_height\n  &lt;chr&gt;    &lt;chr&gt;                &lt;dbl&gt;\n1 control  high                  33.7\n2 control  high                  27.1\n3 control  high                  22.9\n4 control  high                  28.7\n5 control  high                  30.7\n6 control  high                  26.9\n\n\n\n\nVisualise the data\n\nggplot(auxin_response,\n       aes(x = genotype, y = plant_height)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nggplot(auxin_response,\n       aes(x = concentration, y = plant_height)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLet’s look at the interaction plots. We’re only plotting the mean values here, but feel free to explore the data itself by adding another geom_.\n\n# by genotype\nggplot(auxin_response,\n       aes(x = concentration,\n          y = plant_height,\n          colour = genotype, group = genotype)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  geom_jitter(alpha = 0.3, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n# by concentration\nggplot(auxin_response,\n       aes(x = genotype,\n           y = plant_height,\n           colour = concentration, group = concentration)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  geom_jitter(alpha = 0.3, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nWe’ve constructed both box plots and two interaction plots. We only needed to do one interaction plot but I find it can be quite useful to look at the data from different angles.\nThe interaction plots show the mean values for each group, so I prefer to overlay this with the actual data. Both interaction plots suggest that there is an interaction here as the lines in the plots aren’t parallel. Looking at the interaction plot with concentration on the x-axis, it appears that there is non-difference between genotypes when the concentration is low, but that there is a difference between genotypes when the concentration is none or high.\n\n\nAssumptions\nFirst we need to define the model:\n\n# define the linear model, with interaction term\nlm_auxin &lt;- lm(plant_height ~ concentration * genotype,\n               data = auxin_response)\n\nNext, we check the assumptions:\n\nresid_panel(lm_auxin,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\nSo, these actually all look pretty good, with the data looking normally distributed (Q-Q plot), linearity OK (Residual plot), homogeneity of variance looking sharp (Location-scale plot) and no clear influential points (Cook’s D plot).\n\n\nImplement the test\nLet’s carry out a two-way ANOVA:\n\n# perform the ANOVA\nanova(lm_auxin)\n\nAnalysis of Variance Table\n\nResponse: plant_height\n                        Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nconcentration            2 4708.2 2354.12 316.333 &lt; 2.2e-16 ***\ngenotype                 1   83.8   83.84  11.266 0.0009033 ***\nconcentration:genotype   2 1034.9  517.45  69.531 &lt; 2.2e-16 ***\nResiduals              269 2001.9    7.44                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nInterpret the output and report the results\nThere is a significant interaction between concentration and genotype.\n\n\n\n\nLoad the data\n\n# read in the data\nauxin_response_py = pd.read_csv(\"data/CS4-auxin.csv\")\n\n# let's have a peek at the data\nauxin_response_py.head()\n\n  genotype concentration  plant_height\n0  control          high          33.7\n1  control          high          27.1\n2  control          high          22.9\n3  control          high          28.7\n4  control          high          30.7\n\n\n\n\nVisualise the data\n\np = (ggplot(auxin_response_py,\n       aes(x = \"genotype\", y = \"plant_height\")) +\n  geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\np = (ggplot(auxin_response_py,\n       aes(x = \"concentration\", y = \"plant_height\")) +\n  geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\nLet’s look at the interaction plots. We’re also including the data itself here with geom_jitter().\n\n# by genotype\np = (ggplot(auxin_response_py,\n        aes(x = \"concentration\",\n            y = \"plant_height\",\n            colour = \"genotype\", group = \"genotype\")) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     geom_jitter(alpha = 0.3, width = 0.1) +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\np.show()\n\n\n\n\n\n\n\n# by concentration\np = (ggplot(auxin_response_py,\n        aes(x = \"genotype\",\n            y = \"plant_height\",\n            colour = \"concentration\", group = \"concentration\")) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     geom_jitter(alpha = 0.3, width = 0.1) +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\np.show()\n\n\n\n\n\n\n\n\nWe’ve constructed both box plots and two interaction plots. We only needed to do one interaction plot but I find it can be quite useful to look at the data from different angles.\nThe interaction plots show the mean values for each group, so I prefer to overlay this with the actual data. Both interaction plots suggest that there is an interaction here as the lines in the plots aren’t parallel. Looking at the interaction plot with concentration on the x-axis, it appears that there is non-difference between genotypes when the concentration is low, but that there is a difference between genotypes when the concentration is none or high.\n\n\nAssumptions\nFirst we need to define the model:\n\n# create a linear model\nmodel = smf.ols(formula= \"plant_height ~ C(genotype) * concentration\", data = auxin_response_py)\n# and get the fitted parameters of the model\nlm_auxin_py = model.fit()\n\nNext, we check the assumptions:\n\ndgplots(lm_auxin_py)\n\n\n\n\nSo, these actually all look pretty good, with the data looking normally distributed (Q-Q plot), linearity OK (Residual plot), homogeneity of variance looking sharp (Location-scale plot) and no clear influential points (Influential points plot).\n\n\nImplement the test\nLet’s carry out a two-way ANOVA:\n\nsm.stats.anova_lm(lm_auxin_py, typ = 2)\n\n                                sum_sq     df           F        PR(&gt;F)\nC(genotype)                  83.839560    1.0   11.265874  9.033241e-04\nconcentration              4578.890410    2.0  307.642376  3.055198e-70\nC(genotype):concentration  1034.894263    2.0   69.531546  4.558874e-25\nResidual                   2001.872330  269.0         NaN           NaN\n\n\n\n\nInterpret the output and report the results\nThere is definitely a significant interaction between concentration and genotype.\n\n\n\n\nSo, we can conclude the following:\n\nA two-way ANOVA showed that there is a significant interaction between genotype and auxin concentration on plant height (p = 4.56e-25). Increasing auxin concentration appears to result in a reduction of plant height in both wild type and mutant genotypes. The response in the mutant genotype seems to be less pronounced than in wild type.\n\n\n\n\n\n\n\n\n\n\n\n11.8.1 Tulips\n\n\n\n\n\n\nExerciseExercise 2\n\n\n\n\n\n\nLevel: \nBlooms and growing conditions\nWe’re sticking with the plant theme and using the data/CS4-tulip.csv data set, which contains information on an experiment to determine the best conditions for growing tulips (well someone has to care about these sorts of things!). The average number of flower heads (blooms) were recorded for 27 different plots. Each plot experienced one of three different watering regimes and one of three different shade regimes.\n\nInvestigate how the number of blooms is affected by different growing conditions.\n\nNote: have a look at the data and make sure that they are in the correct format!\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\n\nRPython\n\n\n\nLoad the data\n\n# read in the data\ntulip &lt;- read_csv(\"data/CS4-tulip.csv\")\n\nRows: 27 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): water, shade, blooms\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# have a quick look at the data\ntulip\n\n# A tibble: 27 × 3\n   water shade blooms\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1     1    0  \n 2     1     2    0  \n 3     1     3  111. \n 4     2     1  183. \n 5     2     2   59.2\n 6     2     3   76.8\n 7     3     1  225. \n 8     3     2   83.8\n 9     3     3  135. \n10     1     1   80.1\n# ℹ 17 more rows\n\n\nIn this data set the watering regime (water) and shading regime (shade) are encoded with numerical values. However, these numbers are actually categories, representing the amount of water/shade.\nAs such, we don’t want to treat these as numbers but as factors. At the moment they are numbers, which we can tell with &lt;dbl&gt;, which stands for double.\nWe can convert the columns using the as_factor() function. Because we’d like to keep referring to these columns as factors, we will update our existing data set.\n\n# convert watering and shade regimes to factor\ntulip &lt;- tulip %&gt;% \n  mutate(water = as_factor(water),\n         shade = as_factor(shade))\n\nThis data set has three variables; blooms (which is the response variable) and water and shade (which are the two potential predictor variables).\n\n\nVisualise the data\nAs always we’ll visualise the data first:\n\n# by watering regime\nggplot(tulip,\n       aes(x = water, y = blooms)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n# by shading regime\nggplot(tulip,\n       aes(x = shade, y = blooms)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n# interaction plot by watering regime\nggplot(tulip,\n       aes(x = shade,\n           y = blooms,\n           colour = water, group = water)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  geom_jitter(alpha = 0.3, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n# interaction plot by shade regime\nggplot(tulip,\n       aes(x = water,\n           y = blooms,\n           colour = shade, group = shade)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  geom_jitter(alpha = 0.3, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nAgain, both interaction plots suggest that there might be an interaction here. Digging in a little deeper from a descriptive perspective, it looks as though that water regime 1 is behaving differently to water regimes 2 and 3 under different shade conditions.\n\n\nAssumptions\nFirst we need to define the model:\n\n# define the linear model\nlm_tulip &lt;- lm(blooms ~ water * shade,\n               data = tulip)\n\nNext, we check the assumptions:\n\nresid_panel(lm_tulip,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\nThese are actually all OK. A two-way ANOVA analysis is on the cards.\n\n\nImplement the test\nLet’s carry out the two-way ANOVA.\n\n# perform the ANOVA\nanova(lm_tulip)\n\nAnalysis of Variance Table\n\nResponse: blooms\n            Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nwater        2 103626   51813 22.0542 1.442e-05 ***\nshade        2  36376   18188  7.7417   0.00375 ** \nwater:shade  4  41058   10265  4.3691   0.01211 *  \nResiduals   18  42288    2349                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nInterpret the output and report results\nSo we do appear to have a significant interaction between water and shade as expected.\n\n\n\n\nLoad the data\n\n# read in the data\ntulip_py = pd.read_csv(\"data/CS4-tulip.csv\")\n\n# have a quick look at the data\ntulip_py.head()\n\n   water  shade  blooms\n0      1      1    0.00\n1      1      2    0.00\n2      1      3  111.04\n3      2      1  183.47\n4      2      2   59.16\n\n\nIn this data set the watering regime (water) and shading regime (shade) are encoded with numerical values. However, these numbers are actually categories, representing the amount of water/shade.\nAs such, we don’t want to treat these as numbers but as factors. We can convert the columns using astype(object). Because we’d like to keep referring to these columns as factors, we will update our existing data set.\n\n# convert watering and shade regimes to factor\ntulip_py['water'] = tulip_py['water'].astype(object)\ntulip_py['shade'] = tulip_py['shade'].astype(object)\n\nThis data set has three variables; blooms (which is the response variable) and water and shade (which are the two potential predictor variables).\n\n\nVisualise the data\nAs always we’ll visualise the data first:\n\n# by watering regime\np = (ggplot(tulip_py,\n        aes(x = \"water\", y = \"blooms\")) +\n     geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n  \n# by shading regime\np = (ggplot(tulip_py,\n        aes(x = \"shade\", y = \"blooms\")) +\n     geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n# interaction plot by watering regime\np = (ggplot(tulip_py,\n        aes(x = \"shade\", y = \"blooms\",\n            colour = \"water\", group = \"water\")) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     geom_jitter(alpha = 0.3, width = 0.1) +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\np.show()\n\n\n\n\n\n\n\n# interaction plot by shade regime\np = (ggplot(tulip_py,\n        aes(x = \"water\", y = \"blooms\",\n            colour = \"shade\", group = \"shade\")) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     geom_jitter(alpha = 0.3, width = 0.1) +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\np.show()\n\n\n\n\n\n\n\n\nAgain, both interaction plots suggest that there might be an interaction here. Digging in a little deeper from a descriptive perspective, it looks as though that water regime 1 is behaving differently to water regimes 2 and 3 under different shade conditions.\n\n\nAssumptions\nFirst we need to define the model:\n\n# create a linear model\nmodel = smf.ols(formula= \"blooms ~ water * shade\", data = tulip_py)\n# and get the fitted parameters of the model\nlm_tulip_py = model.fit()\n\nNext, we check the assumptions:\n\ndgplots(lm_tulip_py)\n\n\n\n\nThese are actually all OK. A two-way ANOVA analysis is on the cards.\n\n\nImplement the test\nLet’s carry out the two-way ANOVA.\n\nsm.stats.anova_lm(lm_tulip_py, typ = 2)\n\n                    sum_sq    df          F    PR(&gt;F)\nwater        103625.786985   2.0  22.054200  0.000014\nshade         36375.936807   2.0   7.741723  0.003750\nwater:shade   41058.139437   4.0   4.369108  0.012108\nResidual      42288.185200  18.0        NaN       NaN\n\n\n\n\nInterpret the output and report results\nSo we do appear to have a significant interaction between water and shade as expected.\n\n\n\n\n\nA two-way ANOVA showed that there is a significant interaction between watering and shading regimes on number of blooms (p = 1.21e-02).",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#answer",
    "href": "materials/cs4_practical_two-way-anova.html#answer",
    "title": "11  Two-way ANOVA",
    "section": "11.8 Answer",
    "text": "11.8 Answer\n\nRPython\n\n\n\nLoad the data\n\n# read in the data\nauxin_response &lt;- read_csv(\"data/CS4-auxin.csv\")\n\n# let's have a peek at the data\nhead(auxin_response)\n\n# A tibble: 6 × 3\n  genotype concentration plant_height\n  &lt;chr&gt;    &lt;chr&gt;                &lt;dbl&gt;\n1 control  high                  33.7\n2 control  high                  27.1\n3 control  high                  22.9\n4 control  high                  28.7\n5 control  high                  30.7\n6 control  high                  26.9\n\n\n\n\nVisualise the data\n\nggplot(auxin_response,\n       aes(x = genotype, y = plant_height)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nggplot(auxin_response,\n       aes(x = concentration, y = plant_height)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLet’s look at the interaction plots. We’re only plotting the mean values here, but feel free to explore the data itself by adding another geom_.\n\n# by genotype\nggplot(auxin_response,\n       aes(x = concentration,\n          y = plant_height,\n          colour = genotype, group = genotype)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  geom_jitter(alpha = 0.3, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n# by concentration\nggplot(auxin_response,\n       aes(x = genotype,\n           y = plant_height,\n           colour = concentration, group = concentration)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  geom_jitter(alpha = 0.3, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nWe’ve constructed both box plots and two interaction plots. We only needed to do one interaction plot but I find it can be quite useful to look at the data from different angles.\nThe interaction plots show the mean values for each group, so I prefer to overlay this with the actual data. Both interaction plots suggest that there is an interaction here as the lines in the plots aren’t parallel. Looking at the interaction plot with concentration on the x-axis, it appears that there is non-difference between genotypes when the concentration is low, but that there is a difference between genotypes when the concentration is none or high.\n\n\nAssumptions\nFirst we need to define the model:\n\n# define the linear model, with interaction term\nlm_auxin &lt;- lm(plant_height ~ concentration * genotype,\n               data = auxin_response)\n\nNext, we check the assumptions:\n\nresid_panel(lm_auxin,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\nSo, these actually all look pretty good, with the data looking normally distributed (Q-Q plot), linearity OK (Residual plot), homogeneity of variance looking sharp (Location-scale plot) and no clear influential points (Cook’s D plot).\n\n\nImplement the test\nLet’s carry out a two-way ANOVA:\n\n# perform the ANOVA\nanova(lm_auxin)\n\nAnalysis of Variance Table\n\nResponse: plant_height\n                        Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nconcentration            2 4708.2 2354.12 316.333 &lt; 2.2e-16 ***\ngenotype                 1   83.8   83.84  11.266 0.0009033 ***\nconcentration:genotype   2 1034.9  517.45  69.531 &lt; 2.2e-16 ***\nResiduals              269 2001.9    7.44                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nInterpret the output and report the results\nThere is a significant interaction between concentration and genotype.\n\n\n\n\nLoad the data\n\n# read in the data\nauxin_response_py = pd.read_csv(\"data/CS4-auxin.csv\")\n\n# let's have a peek at the data\nauxin_response_py.head()\n\n  genotype concentration  plant_height\n0  control          high          33.7\n1  control          high          27.1\n2  control          high          22.9\n3  control          high          28.7\n4  control          high          30.7\n\n\n\n\nVisualise the data\n\np = (ggplot(auxin_response_py,\n       aes(x = \"genotype\", y = \"plant_height\")) +\n  geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\np = (ggplot(auxin_response_py,\n       aes(x = \"concentration\", y = \"plant_height\")) +\n  geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\nLet’s look at the interaction plots. We’re also including the data itself here with geom_jitter().\n\n# by genotype\np = (ggplot(auxin_response_py,\n        aes(x = \"concentration\",\n            y = \"plant_height\",\n            colour = \"genotype\", group = \"genotype\")) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     geom_jitter(alpha = 0.3, width = 0.1) +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\np.show()\n\n\n\n\n\n\n\n# by concentration\np = (ggplot(auxin_response_py,\n        aes(x = \"genotype\",\n            y = \"plant_height\",\n            colour = \"concentration\", group = \"concentration\")) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     geom_jitter(alpha = 0.3, width = 0.1) +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\np.show()\n\n\n\n\n\n\n\n\nWe’ve constructed both box plots and two interaction plots. We only needed to do one interaction plot but I find it can be quite useful to look at the data from different angles.\nThe interaction plots show the mean values for each group, so I prefer to overlay this with the actual data. Both interaction plots suggest that there is an interaction here as the lines in the plots aren’t parallel. Looking at the interaction plot with concentration on the x-axis, it appears that there is non-difference between genotypes when the concentration is low, but that there is a difference between genotypes when the concentration is none or high.\n\n\nAssumptions\nFirst we need to define the model:\n\n# create a linear model\nmodel = smf.ols(formula= \"plant_height ~ C(genotype) * concentration\", data = auxin_response_py)\n# and get the fitted parameters of the model\nlm_auxin_py = model.fit()\n\nNext, we check the assumptions:\n\ndgplots(lm_auxin_py)\n\n\n\n\nSo, these actually all look pretty good, with the data looking normally distributed (Q-Q plot), linearity OK (Residual plot), homogeneity of variance looking sharp (Location-scale plot) and no clear influential points (Influential points plot).\n\n\nImplement the test\nLet’s carry out a two-way ANOVA:\n\nsm.stats.anova_lm(lm_auxin_py, typ = 2)\n\n                                sum_sq     df           F        PR(&gt;F)\nC(genotype)                  83.839560    1.0   11.265874  9.033241e-04\nconcentration              4578.890410    2.0  307.642376  3.055198e-70\nC(genotype):concentration  1034.894263    2.0   69.531546  4.558874e-25\nResidual                   2001.872330  269.0         NaN           NaN\n\n\n\n\nInterpret the output and report the results\nThere is definitely a significant interaction between concentration and genotype.\n\n\n\n\nSo, we can conclude the following:\n\nA two-way ANOVA showed that there is a significant interaction between genotype and auxin concentration on plant height (p = 4.56e-25). Increasing auxin concentration appears to result in a reduction of plant height in both wild type and mutant genotypes. The response in the mutant genotype seems to be less pronounced than in wild type.",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#summary",
    "href": "materials/cs4_practical_two-way-anova.html#summary",
    "title": "11  Two-way ANOVA",
    "section": "11.9 Summary",
    "text": "11.9 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nA two-way ANOVA is used when there are two categorical variables and a single continuous variable\nWe can visually check for interactions between the categorical variables by using interaction plots\nThe two-way ANOVA is a type of linear model and assumes the following:\n\nthe data have no systematic pattern\nthe residuals are normally distributed\nthe residuals have homogeneity of variance\nthe fit does not depend on a single point (no single point has high leverage)",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html",
    "title": "12  Linear regression with grouped data",
    "section": "",
    "text": "12.1 Libraries and functions",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#libraries-and-functions",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#libraries-and-functions",
    "title": "12  Linear regression with grouped data",
    "section": "",
    "text": "NoteClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n12.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n# Helper functions for tidying data\nlibrary(broom)\n\n\n\n12.1.2 Functions\n\n# Gets underlying data out of model object\nbroom::augment()\n\n# Creates diagnostic plots\nggResidpanel::resid_panel()\n\n# Performs an analysis of variance\nstats::anova()\n\n# Creates a linear model\nstats::lm()\n\n\n\n\n\n12.1.3 Libraries\n\n# A fundamental package for scientific computing in Python\nimport numpy as np\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n# Custom Python script for diagnostic plots, load from base working directory\nfrom dgplots import dgplots\n\n\n\n12.1.4 Functions\n\n# Computes natural logarithm of value\nnumpy.log()\n\n# Plots the first few rows of a DataFrame\npandas.DataFrame.head()\n\n# Reads in a .csv file\npandas.DataFrame.read_csv()\n\n# Creates a model from a formula and data frame\nstatsmodels.formula.api.ols()\n\n# Custom function to create diagnostic plots\ndgplots()\n\nNote: you can download the dgplots script here.",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#purpose-and-aim",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#purpose-and-aim",
    "title": "12  Linear regression with grouped data",
    "section": "12.2 Purpose and aim",
    "text": "12.2 Purpose and aim\nA linear regression analysis with grouped data is used when we have one categorical predictor variable (or factor), and one continuous predictor variable. The response variable must still be continuous however.\nFor example in an experiment that looks at light intensity in woodland, how is light intensity (continuous: lux) affected by the height at which the measurement is taken, recorded as depth measured from the top of the canopy (continuous: meters) and by the type of woodland (categorical: Conifer or Broad leaf).\n\n\n\n\n\n\n\n\n\nWhen analysing these type of data we want to know:\n\nIs there a difference between the groups?\nDoes the continuous predictor variable affect the continuous response variable (does canopy depth affect measured light intensity?)\nIs there any interaction between the two predictor variables? Here an interaction would display itself as a difference in the slopes of the regression lines for each group, so for example perhaps the conifer data set has a significantly steeper line than the broad leaf woodland data set.\n\nIn this case, no interaction means that the regression lines will have the same slope. Essentially the analysis is identical to two-way ANOVA.\n\nWe will plot the data and visually inspect it.\nWe will test for an interaction and if it doesn’t exist then:\n\nWe can test to see if either predictor variable has an effect (i.e. do the regression lines have different intercepts? and is the common gradient significantly different from zero?)\n\n\nWe will first consider how to visualise the data before then carrying out an appropriate statistical test.",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#data-and-hypotheses",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#data-and-hypotheses",
    "title": "12  Linear regression with grouped data",
    "section": "12.3 Data and hypotheses",
    "text": "12.3 Data and hypotheses\nThe data are stored in data/CS4-treelight.csv. This is a data frame with four variables; id, light, depth and species. light is the continuous response variable, depth is the continuous predictor variable and species is the categorical predictor variable.\nRead in the data and inspect them:\n\nRPython\n\n\n\n# read in the data\ntreelight &lt;- read_csv(\"data/CS4-treelight.csv\")\n\n# inspect the data\ntreelight\n\n# A tibble: 23 × 4\n      id light depth species\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  \n 1     1 4106.  1    Conifer\n 2     2 4934.  1.75 Conifer\n 3     3 4417.  2.5  Conifer\n 4     4 4529.  3.25 Conifer\n 5     5 3443.  4    Conifer\n 6     6 4640.  4.75 Conifer\n 7     7 3082.  5.5  Conifer\n 8     8 2368.  6.25 Conifer\n 9     9 2777.  7    Conifer\n10    10 2419.  7.75 Conifer\n# ℹ 13 more rows\n\n\n\n\n\n# load the data\ntreelight_py = pd.read_csv(\"data/CS4-treelight.csv\")\n\n# and have a look\ntreelight_py.head()\n\n   id        light  depth  species\n0   1  4105.646110   1.00  Conifer\n1   2  4933.925144   1.75  Conifer\n2   3  4416.527443   2.50  Conifer\n3   4  4528.618186   3.25  Conifer\n4   5  3442.610306   4.00  Conifer",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#summarise-and-visualise",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#summarise-and-visualise",
    "title": "12  Linear regression with grouped data",
    "section": "12.4 Summarise and visualise",
    "text": "12.4 Summarise and visualise\n\nRPython\n\n\n\n# plot the data\nggplot(treelight,\n       aes(x = depth, y = light, colour = species)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Depth (m)\",\n       y = \"Light intensity (lux)\")\n\n\n\n\n\n\n\n\n\n\n\n# plot the data\np = (ggplot(treelight_py,\n        aes(x = \"depth\",\n            y = \"light\",\n            colour = \"species\")) +\n     geom_point() +\n     scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n     labs(x = \"Depth (m)\",\n         y = \"Light intensity (lux)\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nIt looks like there is a slight negative correlation between depth and light intensity, with light intensity reducing as the canopy depth increases. It would be useful to plot the regression lines in this plot.\n\nRPython\n\n\n\n# plot the data\nggplot(treelight,\n       aes(x = depth, y = light, colour = species)) +\n  geom_point() +\n  # add regression lines\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Depth (m)\",\n       y = \"Light intensity (lux)\")\n\n\n\n\n\n\n\n\n\n\n\n# plot the data\np = (ggplot(treelight_py,\n        aes(x = \"depth\",\n            y = \"light\",\n            colour = \"species\")) +\n     geom_point() +\n     # add regression lines\n     geom_smooth(method = \"lm\", se = False) +\n     scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n     labs(x = \"Depth (m)\",\n          y = \"Light intensity (lux)\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nLooking at this plot, there doesn’t appear to be any significant interaction between the woodland type (Broadleaf and Conifer) and the depth at which light measurements were taken (depth) on the amount of light intensity getting through the canopy as the gradients of the two lines appear to be very similar. There does appear to be a noticeable slope to both lines and both lines look as though they have very different intercepts. All of this suggests that there isn’t any interaction but that both depth and species have a significant effect on light independently.",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#implement-and-interpret-the-test",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#implement-and-interpret-the-test",
    "title": "12  Linear regression with grouped data",
    "section": "12.5 Implement and interpret the test",
    "text": "12.5 Implement and interpret the test\nIn this case we’re going to implement the test before checking the assumptions (I know, let’s live a little!). You’ll find out why soon…\nWe can test for a possible interaction more formally:\n\nRPython\n\n\n\nanova(lm(light ~ depth * species,\n         data = treelight))\n\nAnalysis of Variance Table\n\nResponse: light\n              Df   Sum Sq  Mean Sq  F value    Pr(&gt;F)    \ndepth          1 30812910 30812910 107.8154 2.861e-09 ***\nspecies        1 51029543 51029543 178.5541 4.128e-11 ***\ndepth:species  1   218138   218138   0.7633    0.3932    \nResiduals     19  5430069   285793                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRemember that depth * species is a shorthand way of writing the full set of depth + species + depth:species terms in R i.e. both main effects and the interaction effect.\n\n\nUnfortunately there is no clear way of defining interaction models in pingouin. So we’re resorting back to statsmodels, just like we had to when we performed the Shapiro-Wilk test on the residuals.\nIf you haven’t loaded statsmodels yet, run the following:\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nNext, we create a linear model and get the .fit():\n\n# create a linear model\nmodel = smf.ols(formula = \"light ~ depth * C(species)\",\n                data = treelight_py)\n\n# and get the fitted parameters of the model\nlm_treelight_py = model.fit()\n\nTo get the relevant values, we can print the summary of the model fit. This gives us a rather huge table. Don’t be too daunted by it - there is a logic to the madness and for now we’re mainly interested in the P&gt;|t| column.\n\nprint(lm_treelight_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  light   R-squared:                       0.938\nModel:                            OLS   Adj. R-squared:                  0.928\nMethod:                 Least Squares   F-statistic:                     95.71\nDate:                Wed, 03 Dec 2025   Prob (F-statistic):           1.19e-11\nTime:                        14:05:16   Log-Likelihood:                -174.91\nNo. Observations:                  23   AIC:                             357.8\nDf Residuals:                      19   BIC:                             362.4\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n===============================================================================================\n                                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------\nIntercept                    7798.5655    298.623     26.115      0.000    7173.541    8423.590\nC(species)[T.Conifer]       -2784.5833    442.274     -6.296      0.000   -3710.274   -1858.893\ndepth                        -221.1256     61.802     -3.578      0.002    -350.478     -91.773\ndepth:C(species)[T.Conifer]   -71.0357     81.309     -0.874      0.393    -241.217      99.145\n==============================================================================\nOmnibus:                        1.435   Durbin-Watson:                   2.176\nProb(Omnibus):                  0.488   Jarque-Bera (JB):                1.269\nSkew:                           0.444   Prob(JB):                        0.530\nKurtosis:                       2.267   Cond. No.                         31.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nAs with two-way ANOVA we have a row in the table for each of the different effects. At this point we are particularly interested in the p-values. We need to look at the interaction first.\nThe interaction term between depth and species has a p-value of 0.393 (which is bigger than 0.05) and so we can conclude that the interaction between depth and species isn’t significant. As such we can now consider whether each of the predictor variables independently has an effect.\nBoth depth and species have very small p-values (2.86x10-9 and 4.13x10 -11) and so we can conclude that they do have a significant effect on light.\nThis means that the two regression lines should have the same non-zero slope, but different intercepts. We would now like to know what those values are.\n\n12.5.1 Finding intercept values\nFinding the intercept values is not entirely straightforward and there is some deciphering required to get this right.\nFor a simple straight line such as the linear regression for the conifer data by itself, the output is relatively straightforward:\n\nRPython\n\n\nI’m being a bit lazy here. Since I don’t want to save the filtered data in a separate object, I’m using the pipe to send the Conifer data to the lm() function. However, the first argument in the lm() function is not data, so we need to tell it specifically that the data is coming from the pipe. We do this with the . notation:\n\n# filter the Conifer data and fit a linear model\ntreelight %&gt;% \n  filter(species == \"Conifer\") %&gt;% \n  lm(light ~ depth, data = .)\n\n\nCall:\nlm(formula = light ~ depth, data = .)\n\nCoefficients:\n(Intercept)        depth  \n     5014.0       -292.2  \n\n\n\n\nWe have two options to obtain the intercept for conifers only. We could subset our data, keeping only the conifer values. We could then create a linear model of those data, and obtain the relevant intercept.\nHowever, since we already created a model for the entire data set (including the interaction term) and printed the summary of that, we can actually derive the intercept value with the information that we’ve got.\nIn the coef table of the summary there are several values:\nIntercept                      7798.5655\nC(species)[T.Conifer]         -2784.5833\ndepth                         -221.1256\ndepth:C(species)[T.Conifer]   -71.0357\nThis tells us that the overall intercept value for the model with the interaction term is 7798.5655. The C(species)[T.Conifer] term means that, to go from this overall intercept value to the intercept for conifer, we need to add -2784.5833.\nDoing the maths gives us an intercept of \\(7798.5655 + (-2784.5833) = 5014\\) if we round this.\nEqually, if we want to get the coefficient for depth, then we take the reference value of -221.1256 and add the value next to depth:C(species)[T.Conifer] to it. This gives us \\(-221.1256 + (-71.0357) = -292.2\\) if we round it.\n\n\n\nWe can interpret this as meaning that the intercept of the regression line is 5014 and the coefficient of the depth variable (the number in front of it in the equation) is -292.2.\nSo, the equation of the regression line for the conifer data is given by:\n\\[\\begin{equation}\nlight = 5014 + -292.2 \\times depth\n\\end{equation}\\]\nThis means that for every extra 1 m of depth of forest canopy we lose 292.2 lux of light.\nWhen we looked at the full data set, we found that interaction wasn’t important. This means that we will have a model with two distinct intercepts but only a single slope (that’s what you get for a linear regression without any interaction), so we need to calculate that specific combination. We do this is as follows:\n\nRPython\n\n\n\nlm(light ~ depth + species,\n   data = treelight)\n\n\nCall:\nlm(formula = light ~ depth + species, data = treelight)\n\nCoefficients:\n   (Intercept)           depth  speciesConifer  \n        7962.0          -262.2         -3113.0  \n\n\nNotice the + symbol in the argument, as opposed to the * symbol used earlier. This means that we are explicitly not including an interaction term in this fit, and consequently we are forcing R to calculate the equation of lines which have the same gradient.\nIdeally we would like R to give us two equations, one for each forest type, so four parameters in total. Unfortunately, R is parsimonious and doesn’t do that. Instead R gives you three coefficients, and these require a bit of interpretation.\nThe first two numbers that R returns (underneath Intercept and depth) are the exact intercept and slope coefficients for one of the lines (in this case they correspond to the data for Broadleaf woodlands).\nFor the coefficients belonging to the other line, R uses these first two coefficients as baseline values and expresses the other coefficients relative to these ones. R also doesn’t tell you explicitly which group it is using as its baseline reference group! (Did I mention that R can be very helpful at times 😉?)\nSo, how to decipher the above output?\nFirst, I need to work out which group has been used as the baseline.\n\nIt will be the group that comes first alphabetically, so it should be Broadleaf\nThe other way to check would be to look and see which group is not mentioned in the above table. Conifer is mentioned (in the SpeciesConifer heading) and so again the baseline group is Broadleaf.\n\nThis means that the intercept value and depth coefficient correspond to the Broadleaf group and as a result I know what the equation of one of my lines is:\nBroadleaf:\n\\[\\begin{equation}\nlight = 7962 + -262.2 \\times depth\n\\end{equation}\\]\nIn this example we know that the gradient is the same for both lines (because we explicitly asked to exclude an interaction), so all I need to do is find the intercept value for the Conifer group. Unfortunately, the final value given underneath SpeciesConifer does not give me the intercept for Conifer, instead it tells me the difference between the Conifer group intercept and the baseline intercept i.e. the equation for the regression line for conifer woodland is given by:\n\\[\\begin{equation}\nlight = (7962 + -3113) + -262.2 \\times depth\n\\end{equation}\\]\n\\[\\begin{equation}\nlight = 4829 + -262.2 \\times depth\n\\end{equation}\\]\n\n\nThe way we obtain the values for the model without the interaction is very similar to what we did for the conifer data. We need to update our model first, to remove the interaction:\n\n# create a linear model\nmodel = smf.ols(formula = \"light ~ depth + C(species)\",\n                data = treelight_py)\n\n# and get the fitted parameters of the model\nlm_treelight_add_py = model.fit()\n\nNotice the + symbol in the argument, as opposed to the * symbol used earlier. This means that we are explicitly not including an interaction term in this fit, and consequently we are forcing Python to calculate the equation of lines which have the same gradient.\nWe can get the relevant coefficients as follows:\n\nprint(lm_treelight_add_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  light   R-squared:                       0.935\nModel:                            OLS   Adj. R-squared:                  0.929\nMethod:                 Least Squares   F-statistic:                     144.9\nDate:                Wed, 03 Dec 2025   Prob (F-statistic):           1.26e-12\nTime:                        14:05:16   Log-Likelihood:                -175.37\nNo. Observations:                  23   AIC:                             356.7\nDf Residuals:                      20   BIC:                             360.1\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=========================================================================================\n                            coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------\nIntercept              7962.0316    231.356     34.415      0.000    7479.431    8444.633\nC(species)[T.Conifer] -3113.0265    231.586    -13.442      0.000   -3596.106   -2629.947\ndepth                  -262.1656     39.922     -6.567      0.000    -345.441    -178.891\n==============================================================================\nOmnibus:                        2.068   Durbin-Watson:                   2.272\nProb(Omnibus):                  0.356   Jarque-Bera (JB):                1.677\nSkew:                           0.633   Prob(JB):                        0.432\nKurtosis:                       2.618   Cond. No.                         13.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAgain, I need to work out which group has been used as the baseline.\n\nIt will be the group that comes first alphabetically, so it should be Broadleaf\nThe other way to check would be to look and see which group is not mentioned in the above table. Conifer is mentioned (in the C(species)[T.Conifer] heading) and so again the baseline group is Broadleaf.\n\nThis means that the intercept value and depth coefficient correspond to the Broadleaf group and as a result I know what the equation of one of my lines is:\nBroadleaf:\n\\[\\begin{equation}\nlight = 7962 + -262.2 \\times depth\n\\end{equation}\\]\nIn this example we know that the gradient is the same for both lines (because we explicitly asked to exclude an interaction), so all I need to do is find the intercept value for the Conifer group. Unfortunately, the final value given in C(species)[T.Conifer] does not give me the intercept for Conifer, instead it tells me the difference between the Conifer group intercept and the baseline intercept i.e. the equation for the regression line for conifer woodland is given by:\n\\[\\begin{equation}\nlight = (7962 + -3113) + -262.2 \\times depth\n\\end{equation}\\]\n\\[\\begin{equation}\nlight = 4829 + -262.2 \\times depth\n\\end{equation}\\]\n\n\n\n\n\n12.5.2 Adding custom regression lines\nIn the example above we determined that the interaction term species:depth was not significant. It would be good to visualise the model without the interaction term.\n\nRPython\n\n\nThis is relatively straightforward if we understand the output of the model a bit better.\nFirst of all, we load the broom library. This is part of tidyverse, so you don’t have to install it. It is not loaded by default, hence us loading it. What broom does it changes the format of many common base R outputs into a more tidy format, so we can work with the output in our analyses more easily.\nThe function we use here is called augment(). What this does is take a model object and a dataset and adds information about each observation in the dataset.\n\n# define the model without interaction term\nlm_additive &lt;- lm(light ~ species + depth,\n                  data = treelight)\n\n# load the broom package\nlibrary(broom)\n\n# augment the model\nlm_additive %&gt;% augment()\n\n# A tibble: 23 × 9\n   light species depth .fitted .resid   .hat .sigma .cooksd .std.resid\n   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 4106. Conifer  1      4587.  -481. 0.191    531. 0.0799      -1.01 \n 2 4934. Conifer  1.75   4390.   544. 0.156    528. 0.0766       1.11 \n 3 4417. Conifer  2.5    4194.   223. 0.128    542. 0.00985      0.449\n 4 4529. Conifer  3.25   3997.   532. 0.105    530. 0.0440       1.06 \n 5 3443. Conifer  4      3800.  -358. 0.0896   538. 0.0163      -0.706\n 6 4640. Conifer  4.75   3604.  1037. 0.0801   486. 0.120        2.03 \n 7 3082. Conifer  5.5    3407.  -325. 0.0769   540. 0.0113      -0.637\n 8 2368. Conifer  6.25   3210.  -842. 0.0801   507. 0.0793      -1.65 \n 9 2777. Conifer  7      3014.  -237. 0.0896   542. 0.00719     -0.468\n10 2419. Conifer  7.75   2817.  -398. 0.105    537. 0.0247      -0.792\n# ℹ 13 more rows\n\n\nThe output shows us lots of data. Our original light values are in the light column and it’s the same for species and depth. What has been added is information about the fitted (or predicted) values based on the light ~ depth + species model we defined.\nThe fitted or predicted values are in the .fitted column, with corresponding residuals in the .resid column. Remember, your data = predicted values + error, so if you would add .fitted + resid then you would end up with your original data again.\nUsing this information we can now plot the regression lines by species:\n\n# plot the regression lines by species\nlm_additive %&gt;%\n  augment() %&gt;% \n  ggplot(aes(x = depth, y = .fitted, colour = species)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nLastly, if we would want to plot the data and regression lines together, we could change the code as follows:\n\n# plot the regression lines\nlm_additive %&gt;%\n  augment() %&gt;% \n  ggplot(aes(x = depth, y = .fitted, colour = species)) +\n  # add the original data points\n  geom_point(data = treelight,\n             aes(x = depth, y = light, colour = species)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Depth (m)\",\n       y = \"Light intensity (lux)\")\n\n\n\n\n\n\n\n\n\n\nTo do this, we need to do the following:\n\ncreate a linear model without the interaction term (we did this previously)\nextract the predicted values of the model\nplot these against the original data\n\n\n# get predicted values\nlm_treelight_add_py.predict()\n\narray([4586.83958789, 4390.21542165, 4193.59125541, 3996.96708918,\n       3800.34292294, 3603.7187567 , 3407.09459046, 3210.47042422,\n       3013.84625799, 2817.22209175, 2620.59792551, 2423.97375927,\n       2227.34959303, 7322.87199901, 7047.59816627, 5781.86286681,\n       7543.35323075, 6319.56442008, 7267.03073579, 7773.27242247,\n       5822.76069339, 7766.98044915, 6532.70501628])\n\n\nWe can’t easily use the predicted values in this kind of format, so we’re adding them to the existing data, in a column called .fitted:\n\n# add predicted values to data set\ntreelight_py['.fitted'] = lm_treelight_add_py.predict()\n\n# have a peek at the data\ntreelight_py.head()\n\n   id        light  depth  species      .fitted\n0   1  4105.646110   1.00  Conifer  4586.839588\n1   2  4933.925144   1.75  Conifer  4390.215422\n2   3  4416.527443   2.50  Conifer  4193.591255\n3   4  4528.618186   3.25  Conifer  3996.967089\n4   5  3442.610306   4.00  Conifer  3800.342923\n\n\nNow we can simply plot the data:\n\n# plot the data\np = (ggplot(treelight_py,\n        aes(x = \"depth\",\n            y = \"light\",\n            colour = \"species\")) +\n     geom_point() +\n     # add regression lines\n     geom_line(aes(x = \"depth\",\n                   y = \".fitted\",\n                   colour = \"species\")) +\n     scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n     labs(x = \"Depth (m)\",\n          y = \"Light intensity (lux)\"))\n\np.show()",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#assumptions",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#assumptions",
    "title": "12  Linear regression with grouped data",
    "section": "12.6 Assumptions",
    "text": "12.6 Assumptions\nIn this case we first wanted to check if the interaction was significant, prior to checking the assumptions. If we would have checked the assumptions first, then we would have done that one the full model (with the interaction), then done the ANOVA if everything was OK. We would have then found out that the interaction was not significant, meaning we’d have to re-check the assumptions with the new model. In what order you do it is a bit less important here. The main thing is that you check the assumptions and report on it!\nAnyway, hopefully you’ve got the gist of checking assumptions for linear models by now: diagnostic plots!\n\nRPython\n\n\n\nresid_panel(lm_additive,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\nThe Residuals plot looks OK, no systematic pattern.\nThe Q-Q plot isn’t perfect, but I’m happy with the normality assumption.\nThe Location-Scale plot is OK, some very slight suggestion of heterogeneity of variance, but nothing to be too worried about.\nThe Cook’s D plot shows that all of the points are OK\n\n\n\n\ndgplots(lm_treelight_add_py)\n\n\n\n\n\nThe Residuals plot looks OK, no systematic pattern.\nThe Q-Q plot isn’t perfect, but I’m happy with the normality assumption.\nThe Location-Scale plot is OK, some very slight suggestion of heterogeneity of variance, but nothing to be too worried about.\nThe Influential points plot shows that all of the points are OK\n\n\n\n\nWoohoo!",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#dealing-with-interaction",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#dealing-with-interaction",
    "title": "12  Linear regression with grouped data",
    "section": "12.7 Dealing with interaction",
    "text": "12.7 Dealing with interaction\nIf there had been a significant interaction between the two predictor variables (for example, if light intensity had dropped off significantly faster in conifer woods than in broad leaf woods, in addition to being lower overall, then we would again be looking for two equations for the linear regression, but this time the gradients vary as well. In this case interaction is important and so we need the output from a linear regression that explicitly includes the interaction term:\n\nRPython\n\n\n\nlm(light ~ depth + species + depth:species,\n   data = treelight)\n\nor written using the short-hand:\n\nlm(light ~ depth * species,\n   data = treelight)\n\nThere really is absolutely no difference in the end result. Either way this gives us the following output:\n\n\n\nCall:\nlm(formula = light ~ depth * species, data = treelight)\n\nCoefficients:\n         (Intercept)                 depth        speciesConifer  \n             7798.57               -221.13              -2784.58  \ndepth:speciesConifer  \n              -71.04  \n\n\nAs before the Broadleaf line is used as the baseline regression and we can read off the values for its intercept and slope directly:\nBroadleaf: \\[\\begin{equation}\nlight = 7798.57 + -221.13 \\times depth\n\\end{equation}\\]\nNote that this is different from the previous section, by allowing for an interaction all fitted values will change.\nFor the conifer line we will have a different intercept value and a different gradient value. As before the value underneath speciesConifer gives us the difference between the intercept of the conifer line and the broad leaf line. The new, additional term depth:speciesConifer tells us how the coefficient of depth varies for the conifer line i.e. how the gradient is different. Putting these two together gives us the following equation for the regression line conifer woodland:\nConifer: \\[\\begin{equation}\nlight = (7798.57 + -2784.58) + (-221.13 + -71.04) \\times depth\n\\end{equation}\\]\n\\[\\begin{equation}\nlight = 5014 + -292.2 \\times depth\n\\end{equation}\\]\n\n\nWe’ve actually created this model before, but for clarity we’ll define it here again.\n\n# create a linear model\nmodel = smf.ols(formula = \"light ~ depth * C(species)\",\n                data = treelight_py)\n\n# and get the fitted parameters of the model\nlm_treelight_py = model.fit()\n\nWe get the model parameters as follows:\n\nprint(lm_treelight_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  light   R-squared:                       0.938\nModel:                            OLS   Adj. R-squared:                  0.928\nMethod:                 Least Squares   F-statistic:                     95.71\nDate:                Wed, 03 Dec 2025   Prob (F-statistic):           1.19e-11\nTime:                        14:05:18   Log-Likelihood:                -174.91\nNo. Observations:                  23   AIC:                             357.8\nDf Residuals:                      19   BIC:                             362.4\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n===============================================================================================\n                                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------\nIntercept                    7798.5655    298.623     26.115      0.000    7173.541    8423.590\nC(species)[T.Conifer]       -2784.5833    442.274     -6.296      0.000   -3710.274   -1858.893\ndepth                        -221.1256     61.802     -3.578      0.002    -350.478     -91.773\ndepth:C(species)[T.Conifer]   -71.0357     81.309     -0.874      0.393    -241.217      99.145\n==============================================================================\nOmnibus:                        1.435   Durbin-Watson:                   2.176\nProb(Omnibus):                  0.488   Jarque-Bera (JB):                1.269\nSkew:                           0.444   Prob(JB):                        0.530\nKurtosis:                       2.267   Cond. No.                         31.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAs before the Broadleaf line is used as the baseline regression and we can read off the values for its intercept and slope directly:\nBroadleaf: \\[\\begin{equation}\nlight = 7798.57 + -221.13 \\times depth\n\\end{equation}\\]\nNote that this is different from the previous section, by allowing for an interaction all fitted values will change.\nFor the conifer line we will have a different intercept value and a different gradient value. As before the value next to C(species)[T.Conifer] gives us the difference between the intercept of the conifer line and the broad leaf line. The interaction term depth:C(species)[T.Conifer] tells us how the coefficient of depth varies for the conifer line i.e. how the gradient is different. Putting these two together gives us the following equation for the regression line conifer woodland:\nConifer: \\[\\begin{equation}\nlight = (7798.57 + -2784.58) + (-221.13 + -71.04) \\times depth\n\\end{equation}\\]\n\\[\\begin{equation}\nlight = 5014 + -292.2 \\times depth\n\\end{equation}\\]\n\n\n\nThese also happen to be exactly the regression lines that you would get by calculating a linear regression on each group’s data separately.",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#exercises",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#exercises",
    "title": "12  Linear regression with grouped data",
    "section": "12.8 Exercises",
    "text": "12.8 Exercises\n\n12.8.1 Clover and yarrow\n\n\n\n\n\n\nExerciseExercise 1\n\n\n\n\n\n\nLevel: \nClover and yarrow field trials\nThe data/CS4-clover.csv data set contains information on field trials at three different farms (A, B and C). Each farm recorded the yield of clover in each of ten fields along with the density of yarrow stalks in each field.\n\nInvestigate how clover yield is affected by yarrow stalk density. Is there evidence of competition between the two species?\nIs there a difference between farms?\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\n\nRPython\n\n\n\nclover &lt;- read_csv(\"data/CS4-clover.csv\")\n\nThis data set has three variables; yield (which is the response variable), yarrow (which is a continuous predictor variable) and farm (which is the categorical predictor variables). As always we’ll visualise the data first:\n\n# plot the data\nggplot(clover,\n       aes(x = yarrow, y = yield,\n           colour = farm)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nLooking at this plot as it stands, it’s pretty clear that yarrow density has a significant effect on yield, but it’s pretty hard to see from the plot whether there is any effect of farm, or whether there is any interaction. In order to work that out we’ll want to add the regression lines for each farm separately.\n\n# plot the data\nggplot(clover,\n       aes(x = yarrow, y = yield,\n           colour = farm, group = farm)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nThe regression lines are very close together. Although the line for Farm C crosses the other two, it doesn’t look like we’re going to see a very strong interaction - if there is one. There doesn’t seem to be an effect of farm in general, since the lines are not separate from each other.\nLet’s carry out the analysis:\n\nlm_clover &lt;- lm(yield ~ yarrow * farm,\n                data = clover)\n\nanova(lm_clover)\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nyarrow       1 8538.3  8538.3 28.3143 1.847e-05 ***\nfarm         2    3.8     1.9  0.0063    0.9937    \nyarrow:farm  2  374.7   187.4  0.6213    0.5457    \nResiduals   24 7237.3   301.6                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis confirms our suspicions from looking at the plot. There isn’t any interaction between yarrow and farm. yarrow density has a statistically significant effect on yield but there isn’t any difference between the different farms on the yields of clover.\nLet’s check the assumptions:\n\nresid_panel(lm_clover,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\nThis is a borderline case.\n\nNormality is fine (Q-Q plot)\nThere aren’t any highly influential points (Cook’s D plot)\nThere is a strong suggestion of heterogeneity of variance (Location-Scale plot). Most of the points are relatively close to the regression lines, but the there is a much great spread of points low yarrow density (which corresponds to high yield values, which is what the predicted values correspond to).\nFinally, there is a slight suggestion that the data might not be linear, that it might curve slightly (Residual plot).\n\n\n\n\nclover_py = pd.read_csv(\"data/CS4-clover.csv\")\n\nThis data set has three variables; yield (which is the response variable), yarrow (which is a continuous predictor variable) and farm (which is the categorical predictor variables). As always we’ll visualise the data first:\n\n# plot the data\np = (ggplot(clover_py,\n        aes(x = \"yarrow\",\n            y = \"yield\", colour = \"farm\")) +\n     geom_point() +\n     scale_color_brewer(type = \"qual\", palette = \"Dark2\"))\n\np.show()\n\n\n\n\n\n\n\n\nLooking at this plot as it stands, it’s pretty clear that yarrow density has a significant effect on yield, but it’s pretty hard to see from the plot whether there is any effect of farm, or whether there is any interaction. In order to work that out we’ll want to add the regression lines for each farm separately.\n\n# plot the data\np = (ggplot(clover_py,\n        aes(x = \"yarrow\",\n            y = \"yield\", colour = \"farm\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\", se = False) +\n     scale_color_brewer(type = \"qual\", palette = \"Dark2\"))\n\np.show()\n\n\n\n\n\n\n\n\nThe regression lines are very close together. Although the line for Farm C crosses the other two, it doesn’t look like we’re going to see a very strong interaction - if there is one. There doesn’t seem to be an effect of farm in general, since the lines are not separate from each other.\nLet’s carry out the analysis. First, we need to change the name of the yield column, because yield is a keyword in Python, and we can’t use it inside the model formula. How annoying.\n\n# rename yield column\nclover_py = clover_py.rename(columns = {\"yield\": \"clover_yield\"})\n\n# create a linear model\nmodel = smf.ols(formula = \"clover_yield ~ yarrow * C(farm)\",\n                data = clover_py)\n\n# and get the fitted parameters of the model\nlm_clover_py = model.fit()\n\nPerform an ANOVA on the model:\n\nsm.stats.anova_lm(lm_clover_py, typ = 2)\n\n                     sum_sq    df          F    PR(&gt;F)\nC(farm)            3.808918   2.0   0.006315  0.993706\nyarrow          8540.833898   1.0  28.322674  0.000018\nyarrow:C(farm)   374.718749   2.0   0.621312  0.545659\nResidual        7237.311354  24.0        NaN       NaN\n\n\nThis confirms our suspicions from looking at the plot. There isn’t any interaction between yarrow and farm. yarrow density has a statistically significant effect on yield but there isn’t any difference between the different farms on the yields of clover.\nLet’s check the assumptions:\n\ndgplots(lm_clover_py)\n\n\n\n\nThis is a borderline case.\n\nNormality is fine (Q-Q plot)\nThere aren’t any highly influential points (Influential points plot)\nThere is a strong suggestion of heterogeneity of variance (Location-Scale plot). Most of the points are relatively close to the regression lines, but the there is a much great spread of points low yarrow density (which corresponds to high yield values, which is what the predicted values correspond to).\nFinally, there is a slight suggestion that the data might not be linear, that it might curve slightly (Residuals plot).\n\n\n\n\nWe have two options; both of which are arguably OK to do in real life.\n\nWe can claim that these assumptions are well enough met and just report the analysis that we’ve just done.\nWe can decide that the analysis is not appropriate and look for other options.\n\nWe can try to transform the data by taking logs of yield. This might fix both of our problems: taking logs of the response variable has the effect of improving heterogeneity of variance when the Residuals plot is more spread out on the right vs. the left (like ours). It also is appropriate if we think the true relationship between the response and predictor variables is exponential rather than linear (which we might have). We do have the capabilities to try this option.\nWe could try a permutation based approach (beyond the remit of this course, and actually a bit tricky in this situation). This wouldn’t address the non-linearity but it would deal with the variance assumption.\nWe could come up with a specific functional, mechanistic relationship between yarrow density and clover yield based upon other aspects of their biology. For example there might be a threshold effect such that for yarrow densities below a particular value, clover yields are unaffected, but as soon as yarrow values get above that threshold the clover yield decreases (maybe even linearly). This would require a much better understanding of clover-yarrow dynamics (of which I personally know very little).\n\n\nLet’s do a quick little transformation of the data, and repeat our analysis see if our assumptions are better met this time (just for the hell of it):\n\nRPython\n\n\n\n# plot log-transformed data\nggplot(clover,\n       aes(x = yarrow, y = log(yield), colour = farm)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\n\n\nTo log-transform our data, we require numpy.\n\n# import numpy\nimport numpy as np\n\nFirst, we create a new column containing the log-transformed data:\n\nclover_py[\"log_yield\"] = np.log(clover_py[\"clover_yield\"])\n\nThen we can plot them:\n\n# plot log-transformed data\np = (ggplot(clover_py,\n         aes(x = \"yarrow\",\n             y = \"log_yield\", colour = \"farm\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\", se = False) +\n     scale_color_brewer(type = \"qual\", palette = \"Dark2\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nAgain, this looks plausible. There’s a noticeable outlier from Farm B (data point at the bottom of the plot) but otherwise we see that: there probably isn’t an interaction; there is likely to be an effect of yarrow on log_yield; and there probably isn’t any difference between the farms.\nLet’s do the analysis:\n\nRPython\n\n\n\n# define linear model\nlm_log_clover &lt;- lm(log(yield) ~ yarrow * farm,\n                    data = clover)\n\nanova(lm_log_clover)\n\nAnalysis of Variance Table\n\nResponse: log(yield)\n            Df  Sum Sq Mean Sq F value   Pr(&gt;F)    \nyarrow       1 10.6815 10.6815 27.3233 2.34e-05 ***\nfarm         2  0.0862  0.0431  0.1103   0.8960    \nyarrow:farm  2  0.8397  0.4199  1.0740   0.3575    \nResiduals   24  9.3823  0.3909                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWoop. All good so far. We have the same conclusions as before in terms of what is significant and what isn’t. Now we just need to check the assumptions:\n\nresid_panel(lm_log_clover,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"log_yield ~ yarrow * C(farm)\",\n                data = clover_py)\n\n# and get the fitted parameters of the model\nlm_log_clover_py = model.fit()\n\nPerform an ANOVA on the model:\n\nsm.stats.anova_lm(lm_log_clover_py, typ = 2)\n\n                   sum_sq    df          F    PR(&gt;F)\nC(farm)          0.086222   2.0   0.110278  0.896037\nyarrow          10.638411   1.0  27.213127  0.000024\nyarrow:C(farm)   0.839715   2.0   1.073998  0.357498\nResidual         9.382305  24.0        NaN       NaN\n\n\nWoop. All good so far. We have the same conclusions as before in terms of what is significant and what isn’t. Now we just need to check the assumptions:\n\ndgplots(lm_clover_py)\n\n\n\n\n\n\n\nWell, this is actually a better set of diagnostic plots. Whilst one data point (for example in the Q-Q plot) is a clear outlier, if we ignore that point then all of the other plots do look better.\nSo now we know that yarrow is a significant predictor of yield and we’re happy that the assumptions have been met.",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#summary",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#summary",
    "title": "12  Linear regression with grouped data",
    "section": "12.9 Summary",
    "text": "12.9 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nA linear regression analysis with grouped data is used when we have one categorical and one continuous predictor variable, together with one continuous response variable\nWe can visualise the data by plotting a regression line together with the original data\nWhen performing an ANOVA, we need to check for interaction terms\nWe check the underlying assumptions using diagnostic plots\nWe can create an equation for the regression line for each group in the data using the parameter from the linear model output",
    "crumbs": [
      "CS4: Two predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html",
    "href": "materials/cs5_practical_multiple-linear-regression.html",
    "title": "13  Multiple linear regression",
    "section": "",
    "text": "13.1 Libraries and functions",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#libraries-and-functions",
    "href": "materials/cs5_practical_multiple-linear-regression.html#libraries-and-functions",
    "title": "13  Multiple linear regression",
    "section": "",
    "text": "NoteClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n13.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n# Helper functions for tidying data\nlibrary(broom)\n\n\n\n13.1.2 Functions\n\n# Gets underlying data out of model object\nbroom::augment()\n\n# Creates diagnostic plots\nggResidpanel::resid_panel()\n\n# Performs an analysis of variance\nstats::anova()\n\n# Creates a linear model\nstats::lm()\n\n\n\n\n\n13.1.3 Libraries\n\n# A fundamental package for scientific computing in Python\nimport numpy as np\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n# Custom Python script for diagnostic plots, load from base working directory\nfrom dgplots import dgplots\n\n\n\n13.1.4 Functions\n\n# Reads in a .csv file\npandas.DataFrame.read_csv()\n\n# Creates a model from a formula and data frame\nstatsmodels.formula.api.ols()\n\n# Custom function to create diagnostic plots\ndgplots()\n\nNote: you can download the dgplots script here.",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#purpose-and-aim",
    "href": "materials/cs5_practical_multiple-linear-regression.html#purpose-and-aim",
    "title": "13  Multiple linear regression",
    "section": "13.2 Purpose and aim",
    "text": "13.2 Purpose and aim\nRevisiting the linear model framework and expanding to systems with three predictor variables.",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#data-and-hypotheses",
    "href": "materials/cs5_practical_multiple-linear-regression.html#data-and-hypotheses",
    "title": "13  Multiple linear regression",
    "section": "13.3 Data and hypotheses",
    "text": "13.3 Data and hypotheses\nThe data set we’ll be using is located in data/CS5-pm2_5.csv. It contains data on air pollution levels measured in London, in 2019. It also contains several meteorological measurements. Each variable was recorded on a daily basis.\nNote: some of the variables are based on simulations.\nIt contains the following variables:\n\n\n\nvariable\nexplanation\n\n\n\n\navg_temp\naverage daily temperature (\\(^\\circ C\\))\n\n\ndate\ndate of record\n\n\nlocation\nlocation in London (inner or outer)\n\n\npm2_5\nconcentration of PM2.5 (\\(\\mu g / m^3\\))\n\n\nrain_mm\ndaily rainfall in mm (same across both locations)\n\n\nwind_m_s\nwind speed in \\(m/s\\)",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#summarise-and-visualise",
    "href": "materials/cs5_practical_multiple-linear-regression.html#summarise-and-visualise",
    "title": "13  Multiple linear regression",
    "section": "13.4 Summarise and visualise",
    "text": "13.4 Summarise and visualise\n\nRPython\n\n\nLet’s first load the data:\n\npm2_5 &lt;- read_csv(\"data/CS5-pm2_5.csv\")\n\nRows: 730 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): date, location\ndbl (4): avg_temp, pm2_5, rain_mm, wind_m_s\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(pm2_5)\n\n# A tibble: 6 × 6\n  avg_temp date       location pm2_5 rain_mm wind_m_s\n     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1      4.5 01/01/2019 inner     17.1     2.3     3.87\n2      4.9 01/01/2019 outer     10.8     2.3     5.84\n3      4.3 02/01/2019 inner     14.9     2.3     3.76\n4      4.8 02/01/2019 outer     11.4     2.3     6   \n5      4   03/01/2019 inner     18.5     1.4     2.13\n6      4.5 03/01/2019 outer     15.0     1.4     2.57\n\n\nIt’s the pm2_5 response variable we’re interested in here. Let’s start by checking if there might be a difference between PM2.5 level in inner and outer London:\n\nggplot(pm2_5,\n       aes(x = location, y = pm2_5)) +\n    geom_boxplot() +\n    geom_jitter(width = 0.1, alpha = 0.3)\n\n\n\n\n\n\n\n\nI’ve added the (jittered) data to the plot, with some transparency (alpha = 0.3). It’s always good to look at the actual data and not just summary statistics (which is what the box plot is).\nThere seems to be quite a difference between the PM2.5 levels in the two London areas, with the levels in inner London being markedly higher. I’m not surprised by this! So when we do our statistical testing, I would expect to find a clear difference between the locations.\nApart from the location, there are quite a few numerical descriptor variables. We could plot them one-by-one, but that’s a bit tedious. So instead we use the pairs() function again. This only works on numerical data, so we select all the columns that are numeric with select_if(is.numeric):\n\npm2_5 %&gt;% \n    select_if(is.numeric) %&gt;% \n    pairs(lower.panel = NULL)\n\n\n\n\n\n\n\n\nWe can see that there is not much of a correlation between pm2_5 and avg_temp or rain_mm, whereas there might be something going on in relation to wind_m_s.\nOther notable things include that rainfall seems completely independent of wind speed (rain fall seems pretty constant). Nor does the average temperature seem in any way related to wind speed (it looks like a random collection of data points!).\nWe can visualise the relationship between pm2_5 and wind_m_s in a bit more detail, by plotting the data against each other and colouring by location:\n\nggplot(pm2_5,\n       aes(x = wind_m_s, y = pm2_5,\n           colour = location)) +\n    geom_point()\n\n\n\n\n\n\n\n\nThis seems to show that there might be some linear relationship between PM2.5 levels and wind speed.\nAnother way of looking at this would be to create a correlation matrix, like we did before in the correlations chapter:\n\npm2_5 %&gt;% \n    select_if(is.numeric) %&gt;% \n    cor()\n\n            avg_temp       pm2_5     rain_mm    wind_m_s\navg_temp  1.00000000  0.03349457  0.03149221 -0.01107855\npm2_5     0.03349457  1.00000000 -0.02184951 -0.41733945\nrain_mm   0.03149221 -0.02184951  1.00000000  0.04882097\nwind_m_s -0.01107855 -0.41733945  0.04882097  1.00000000\n\n\nThis confirms what we saw in the plots, there aren’t any very strong correlations between the different (numerical) variables, apart from a negative correlation between pm2_5 and wind_m_s, which has a Pearson’s r of \\(r\\) = -0.42.\n\n\nLet’s first load the data:\n\npm2_5_py = pd.read_csv(\"data/CS5-pm2_5.csv\")\n\npm2_5_py.head()\n\n   avg_temp        date location   pm2_5  rain_mm  wind_m_s\n0       4.5  01/01/2019    inner  17.126      2.3      3.87\n1       4.9  01/01/2019    outer  10.821      2.3      5.84\n2       4.3  02/01/2019    inner  14.884      2.3      3.76\n3       4.8  02/01/2019    outer  11.416      2.3      6.00\n4       4.0  03/01/2019    inner  18.471      1.4      2.13\n\n\nIt’s the pm2_5 response variable we’re interested in here. Let’s start by checking if there might be a difference between PM2.5 level in inner and outer London:\n\np = (ggplot(pm2_5_py, aes(x = \"location\", y = \"pm2_5\")) +\n    geom_boxplot() +\n    geom_jitter(width = 0.1, alpha = 0.7))\n\np.show()\n\n\n\n\n\n\n\n\nI’ve added the (jittered) data to the plot, with some transparency (alpha = 0.7). It’s always good to look at the actual data and not just summary statistics (which is what the box plot is).\nThere seems to be quite a difference between the PM2.5 levels in the two London areas, with the levels in inner London being markedly higher. I’m not surprised by this! So when we do our statistical testing, I would expect to find a clear difference between the locations.\nApart from the location, there are quite a few numerical descriptor variables. At this point I should probably bite the bullet and install seaborn, so I can use the pairplot() function.\nBut I’m not going to ;-)\nI’ll just tell you that there is not much of a correlation between pm2_5 and avg_temp or rain_mm, whereas there might be something going on in relation to wind_m_s. So I plot that instead and colour it by location:\n\np = (ggplot(pm2_5_py,\n        aes(x = \"wind_m_s\",\n            y = \"pm2_5\",\n            colour = \"location\")) +\n     geom_point())\n\np.show()\n\n\n\n\n\n\n\n\nThis seems to show that there might be some linear relationship between PM2.5 levels and wind speed.\nIf I would plot all the other variables against each other, then I would spot that rainfall seems completely independent of wind speed (rain fall seems pretty constant). Nor does the average temperature seem in any way related to wind speed (it looks like a random collection of data points!). You can check this yourself!\nAnother way of looking at this would be to create a correlation matrix, like we did before in the correlations chapter:\n\npm2_5_py.corr(numeric_only = True)\n\n          avg_temp     pm2_5   rain_mm  wind_m_s\navg_temp  1.000000  0.033495  0.031492 -0.011079\npm2_5     0.033495  1.000000 -0.021850 -0.417339\nrain_mm   0.031492 -0.021850  1.000000  0.048821\nwind_m_s -0.011079 -0.417339  0.048821  1.000000\n\n\nThis confirms what we saw in the plots, there aren’t any very strong correlations between the different (numerical) variables, apart from a negative correlation between pm2_5 and wind_m_s, which has a Pearson’s r of \\(r\\) = -0.42.",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#implement-and-interpret-the-test",
    "href": "materials/cs5_practical_multiple-linear-regression.html#implement-and-interpret-the-test",
    "title": "13  Multiple linear regression",
    "section": "13.5 Implement and interpret the test",
    "text": "13.5 Implement and interpret the test\nFrom our initial observations we derived that there might be some relationship between PM2.5 levels and wind speed. We also noticed that this is likely to be different between inner and outer London.\nIf we would want to test for every variable and interaction, then we would end up with a rather huge model, which would even include 3-way and a 4-way interaction! To illustrate the point that the process of model testing applies to as many variables as you like, we’re adding the avg_temp and rain_mm variables to our model.\nSo in this case we create a model that takes into account all of the main effects (avg_temp, location, rain_mm, wind_m_s). We also include a potential two-way interaction (location:wind_m_s). The two-way interaction may be of interest since the PM2.5 levels in response to wind speed seem to differ between the two locations.\nOur model is then as follows:\npm2_5 ~ avg_temp + location + rain_mm + wind_m_s + wind_m_s:location\nSo let’s define and explore it!\n\nRPython\n\n\nWe write the model as follows:\n\nlm_pm2_5_full &lt;- lm(pm2_5 ~ avg_temp + location +\n                            rain_mm + wind_m_s +\n                            wind_m_s:location,\n                    data = pm2_5)\n\nLet’s look at the coefficients:\n\nlm_pm2_5_full\n\n\nCall:\nlm(formula = pm2_5 ~ avg_temp + location + rain_mm + wind_m_s + \n    wind_m_s:location, data = pm2_5)\n\nCoefficients:\n           (Intercept)                avg_temp           locationouter  \n              18.18286                 0.01045                -2.07084  \n               rain_mm                wind_m_s  locationouter:wind_m_s  \n              -0.02788                -0.28545                -0.42945  \n\n\n\n\n\n\n\n\nTipExtracting coefficients with tidy()\n\n\n\n\n\nThis will give us quite a few coefficients, so instead of just calling the lm object, I’m restructuring the output using the tidy() function from the broom package. It’s installed with tidyverse but you have to load it separately using library(broom).\n\nlm_pm2_5_full %&gt;%\n    tidy() %&gt;% \n    select(term, estimate)\n\n# A tibble: 6 × 2\n  term                   estimate\n  &lt;chr&gt;                     &lt;dbl&gt;\n1 (Intercept)             18.2   \n2 avg_temp                 0.0105\n3 locationouter           -2.07  \n4 rain_mm                 -0.0279\n5 wind_m_s                -0.285 \n6 locationouter:wind_m_s  -0.429 \n\n\n\n\n\nThe question is, are all of these terms statistically significant? To find out we perform an ANOVA:\n\nanova(lm_pm2_5_full)\n\nAnalysis of Variance Table\n\nResponse: pm2_5\n                   Df  Sum Sq Mean Sq   F value  Pr(&gt;F)    \navg_temp            1    5.29    5.29    5.0422 0.02504 *  \nlocation            1 3085.37 3085.37 2940.5300 &lt; 2e-16 ***\nrain_mm             1    2.48    2.48    2.3644 0.12457    \nwind_m_s            1  728.13  728.13  693.9481 &lt; 2e-16 ***\nlocation:wind_m_s   1  134.82  134.82  128.4912 &lt; 2e-16 ***\nResiduals         724  759.66    1.05                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom this we can see that the interaction between location and wind_m_s is statistically significant. Which means that we can’t just talk about the effect of location or wind_m_s on PM2.5 levels, without taking the other variable into account!\nThe p-value for the avg_temp is significant, whereas the rain_mm main effect is not. This means that rain fall is not contributing much to model’s ability to explain our data. This matches what we already saw when we visualised the data.\nWhat to do? We’ll explore this in more detail in the chapter on model comparisons, but for now the most sensible option would be to redefine the model, but exclude the rain_mm variable. Here I have rewritten the model and named it lm_pm2_5_red to indicate it is a reduced model (with fewer variables than our original full model):\n\nlm_pm2_5_red &lt;- lm(pm2_5 ~ avg_temp + location + wind_m_s + location:wind_m_s, data = pm2_5)\n\nLet’s look at the new model coefficients:\n\nlm_pm2_5_red\n\n\nCall:\nlm(formula = pm2_5 ~ avg_temp + location + wind_m_s + location:wind_m_s, \n    data = pm2_5)\n\nCoefficients:\n           (Intercept)                avg_temp           locationouter  \n              18.13930                 0.01031                -2.07380  \n              wind_m_s  locationouter:wind_m_s  \n              -0.28631                -0.42880  \n\n\nAs we did in the linear regression on grouped data, we end up with two linear equations, one for inner London and one for outer London.\nOur reference group is inner (remember, it takes a reference group in alphabetical order and we can see outer in the output).\nSo we end up with:\n\\(PM2.5_{inner} = 18.14 + 0.01 \\times avg\\_temp - 0.29 \\times wind\\_m\\_s\\)\n\\(PM2.5_{outer} = (18.14 - 2.07) + 0.01 \\times avg\\_temp + (-0.29 - 0.43) \\times wind\\_m\\_s\\)\nwhich gives\n\\(PM2.5_{outer} = 16.07 + 0.01 \\times avg\\_temp - 0.72 \\times wind\\_m\\_s\\)\nWe still need to check the assumptions of the model:\n\nresid_panel(lm_pm2_5_red,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\nThey all look pretty good, with the only weird thing being a small empty zone of predicted values just under 16. Nothing that is getting me worried though.\nIt’d be useful to visualise the model. We can take the model and use the augment() function to extract the fitted values (.fitted). These are the values for pm2_5 that the model is predicting. We can then plot these against the wind_m_s measurements, colouring by location:\n\nlm_pm2_5_red %&gt;% \n  augment() %&gt;% \n  ggplot(aes(x = wind_m_s,\n             y = pm2_5, colour = location)) +\n  geom_point() +\n  geom_smooth(aes(y = .fitted))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nWe write the model as follows:\n\n# create a linear model\nmodel = smf.ols(formula = \"pm2_5 ~ avg_temp + C(location) + rain_mm + wind_m_s + wind_m_s:location\", data = pm2_5_py)\n# and get the fitted parameters of the model\nlm_pm2_5_full_py = model.fit()\n\nThis will give us quite a few coefficients, so instead of just printing the entire summary table, we’re extracting the parameters with .params:\n\nlm_pm2_5_full_py.params\n\nIntercept                     18.182858\nC(location)[T.outer]          -2.070843\navg_temp                       0.010451\nrain_mm                       -0.027880\nwind_m_s                      -0.285450\nwind_m_s:location[T.outer]    -0.429455\ndtype: float64\n\n\nThe question is, are all of these terms statistically significant? To find out we perform an ANOVA:\n\nsm.stats.anova_lm(lm_pm2_5_full_py)\n\n                      df       sum_sq      mean_sq            F         PR(&gt;F)\nC(location)          1.0  3085.163730  3085.163730  2940.332225  3.791413e-257\navg_temp             1.0     5.498107     5.498107     5.240001   2.236033e-02\nrain_mm              1.0     2.480813     2.480813     2.364352   1.245723e-01\nwind_m_s             1.0   728.129799   728.129799   693.948101  8.928636e-108\nwind_m_s:location    1.0   134.820234   134.820234   128.491164   1.567268e-27\nResidual           724.0   759.661960     1.049257          NaN            NaN\n\n\nFrom this we can see that the interaction between location and wind_m_s is statistically significant. Which means that we can’t just talk about the effect of location or wind_m_s on PM2.5 levels, without taking the other variable into account!\nThe p-value for the avg_temp is significant, whereas the rain_mm main effect is not. This means that rain fall is not contributing much to model’s ability to explain our data. This matches what we already saw when we visualised the data.\nWhat to do? We’ll explore this in more detail in the chapter on model comparisons, but for now the most sensible option would be to redefine the model, but exclude the rain_mm variable. Here I have rewritten the model and named it lm_pm2_5_red to indicate it is a reduced model (with fewer variables than our original full model):\n\n# create a linear model\nmodel = smf.ols(formula = \"pm2_5 ~ avg_temp + C(location) * wind_m_s\", data = pm2_5_py)\n# and get the fitted parameters of the model\nlm_pm2_5_red_py = model.fit()\n\nLet’s look at the new model coefficients:\n\nlm_pm2_5_red_py.params\n\nIntercept                        18.139300\nC(location)[T.outer]             -2.073802\navg_temp                          0.010312\nwind_m_s                         -0.286313\nC(location)[T.outer]:wind_m_s    -0.428800\ndtype: float64\n\n\nAs we did in the linear regression on grouped data, we end up with two linear equations, one for inner London and one for outer London.\nOur reference group is inner (remember, it takes a reference group in alphabetical order and we can see outer in the output).\nSo we end up with:\n\\(PM2.5_{inner} = 18.14 + 0.01 \\times avg\\_temp - 0.29 \\times wind\\_m\\_s\\)\n\\(PM2.5_{outer} = (18.14 - 2.07) + 0.01 \\times avg\\_temp + (-0.29 - 0.43) \\times wind\\_m\\_s\\)\nwhich gives\n\\(PM2.5_{outer} = 16.07 + 0.01 \\times avg\\_temp - 0.72 \\times wind\\_m\\_s\\)\nWe still need to check the assumptions of the model:\n\ndgplots(lm_pm2_5_red_py)\n\n\n\n\nThey all look pretty good, with the only weird thing being a small empty zone of predicted values just under 16. Nothing that is getting me worried though.\nIt’d be useful to visualise the model. We can take the model and extract the fitted values (.fittedvalues). These are the pm2_5 that the model is predicting. We can then plot these against the wind_m_s measurements, colouring by location. We’re also adding the original values to the plot with geom_point():\n\np = (ggplot(pm2_5_py, aes(x = \"wind_m_s\",\n                     y = \"pm2_5\",\n                     colour = \"location\")) +\n    geom_point() +\n    geom_smooth(aes(y = lm_pm2_5_red_py.fittedvalues)))\n\np.show()",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#exploring-models",
    "href": "materials/cs5_practical_multiple-linear-regression.html#exploring-models",
    "title": "13  Multiple linear regression",
    "section": "13.6 Exploring models",
    "text": "13.6 Exploring models\nRather than stop here however, we will use the concept of the linear model to its full potential and show that we can construct and analyse any possible combination of predictor variables for this data set. Namely we will consider the following four extra models, where reduce the complexity to the model, step-by-step:\n\n\n\nModel\nDescription\n\n\n\n\n1. pm2_5 ~ wind_m_s + location\nAn additive model\n\n\n2. pm2_5 ~ wind_m_s\nEquivalent to a simple linear regression\n\n\n3. pm2_5 ~ location\nEquivalent to a one-way ANOVA\n\n\n4. pm2_5 ~ 1\nThe null model, where we have no predictors\n\n\n\n\n13.6.1 Additive model\nTo create the additive model, we drop the interaction term (keep in mind, this is to demonstrate the process - we would normally not do this because the interaction term is significant!).\n\nRPython\n\n\nFirst, we define the model:\n\nlm_pm2_5_add &lt;- lm(pm2_5 ~ avg_temp + location + wind_m_s,\n                   data = pm2_5)\n\nWe can visualise this as follows:\n\nlm_pm2_5_add %&gt;% \n    augment() %&gt;% \n    ggplot(aes(x = wind_m_s, y = pm2_5,\n               colour = location)) +\n    geom_point() +\n    geom_smooth(aes(y = .fitted))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNext, we extract the coefficient estimates:\n\nlm_pm2_5_add\n\n\nCall:\nlm(formula = pm2_5 ~ avg_temp + location + wind_m_s, data = pm2_5)\n\nCoefficients:\n  (Intercept)       avg_temp  locationouter       wind_m_s  \n     19.04867        0.01587       -4.05339       -0.49868  \n\n\n\n\nFirst, we define the model\n\n# create a linear model\nmodel = smf.ols(formula = \"pm2_5 ~ avg_temp + C(location) + wind_m_s\",\n                data = pm2_5_py)\n                \n# and get the fitted parameters of the model\nlm_pm2_5_add_py = model.fit()\n\nWe can visualise this as follows:\n\np = (ggplot(pm2_5_py, aes(x = \"wind_m_s\",\n                      y = \"pm2_5\",\n                      colour = \"location\")) +\n    geom_point() +\n    geom_smooth(aes(y = lm_pm2_5_add_py.fittedvalues)))\n\np.show()\n\n\n\n\n\n\n\n\nNext, we extract the coefficient estimates:\n\nlm_pm2_5_add_py.params\n\nIntercept               19.048672\nC(location)[T.outer]    -4.053394\navg_temp                 0.015872\nwind_m_s                -0.498683\ndtype: float64\n\n\n\n\n\nSo our two equations would be as follows:\n\\(PM2.5_{inner} = 19.04 + 0.016 \\times avg\\_temp - 0.50 \\times wind\\_m\\_s\\)\n\\(PM2.5_{outer} = (19.04 - 4.05) + 0.016 \\times avg\\_temp - 0.50 \\times wind\\_m\\_s\\)\ngives\n\\(PM2.5_{outer} = 14.99 + 0.016 \\times avg\\_temp - 0.50 \\times wind\\_m\\_s\\)\n\n\n13.6.2 Revisiting linear regression\n\nRPython\n\n\nFirst, we define the model:\n\nlm_pm2_5_wind &lt;- lm(pm2_5 ~ wind_m_s,\n                   data = pm2_5)\n\nWe can visualise this as follows:\n\nlm_pm2_5_wind %&gt;% \n    augment() %&gt;% \n    ggplot(aes(x = wind_m_s, y = pm2_5)) +\n    geom_point() +\n    geom_smooth(aes(y = .fitted))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipAlternative using geom_smooth()\n\n\n\n\n\n\nggplot(pm2_5, aes(x = wind_m_s, y = pm2_5)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we extract the coefficient estimates:\n\nlm_pm2_5_wind\n\n\nCall:\nlm(formula = pm2_5 ~ wind_m_s, data = pm2_5)\n\nCoefficients:\n(Intercept)     wind_m_s  \n    17.3267      -0.5285  \n\n\n\n\nFirst, we define the model\n\n# create a linear model\nmodel = smf.ols(formula = \"pm2_5 ~ wind_m_s\",\n                data = pm2_5_py)\n                \n# and get the fitted parameters of the model\nlm_pm2_5_wind_py = model.fit()\n\nWe can visualise this as follows:\n\np = (ggplot(pm2_5_py, aes(x = \"wind_m_s\",\n                      y = \"pm2_5\")) +\n    geom_point() +\n    geom_smooth(aes(y = lm_pm2_5_wind_py.fittedvalues), colour = \"blue\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipAlternative using geom_smooth()\n\n\n\n\n\n\np = (ggplot(pm2_5_py, aes(x = \"wind_m_s\", y = \"pm2_5\")) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = False, colour = \"blue\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nNext, we extract the coefficient estimates:\n\nlm_pm2_5_wind_py.params\n\nIntercept    17.326708\nwind_m_s     -0.528510\ndtype: float64\n\n\n\n\n\nThis gives us the following equation:\n\\(PM2.5 = 17.33 - 0.53 \\times wind\\_m\\_s\\)\n\n\n13.6.3 Revisiting ANOVA\nIf we’re just looking at the effect of location, then we’re essentially doing a one-way ANOVA.\n\nRPython\n\n\nFirst, we define the model:\n\nlm_pm2_5_loc &lt;- lm(pm2_5 ~ location,\n                   data = pm2_5)\n\nWe can visualise this as follows:\n\nlm_pm2_5_loc %&gt;% \n    augment() %&gt;% \n    ggplot(aes(x = location, y = pm2_5)) +\n    geom_jitter(alpha = 0.3, width = 0.1) +\n    geom_point(aes(y = .fitted), colour = \"blue\", size = 3)\n\n\n\n\n\n\n\n\nOK, what’s going on here? I’ve plotted the .fitted values (the values predicted by the model) in blue and overlaid the original (with a little bit of jitter to avoid overplotting). However, there are only two predicted values!\nWe can check this and see that each unique fitted value occurs 365 times:\n\nlm_pm2_5_loc %&gt;% \n    augment() %&gt;% \n    count(location, .fitted)\n\n# A tibble: 2 × 3\n  location .fitted     n\n  &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;\n1 inner       16.9   365\n2 outer       12.8   365\n\n\nThis makes sense if we think back to our original ANOVA exploration. There we established that an ANOVA is just a special case of a linear model, where the fitted values are equal to the mean of each group.\nWe could even check this:\n\npm2_5 %&gt;% \n    group_by(location) %&gt;% \n    summarise(mean_pm2_5 = mean(pm2_5))\n\n# A tibble: 2 × 2\n  location mean_pm2_5\n  &lt;chr&gt;         &lt;dbl&gt;\n1 inner          16.9\n2 outer          12.8\n\n\nSo, that matches. We move on and extract the coefficient estimates:\n\nlm_pm2_5_loc\n\n\nCall:\nlm(formula = pm2_5 ~ location, data = pm2_5)\n\nCoefficients:\n  (Intercept)  locationouter  \n       16.943         -4.112  \n\n\nThese values match up exactly with the predicted values for each individual location.\n\n\nFirst, we define the model:\n\n# create a linear model\nmodel = smf.ols(formula = \"pm2_5 ~ C(location)\", data = pm2_5_py)\n# and get the fitted parameters of the model\nlm_pm2_5_loc_py = model.fit()\n\nWe can visualise this as follows:\n\np = (ggplot(pm2_5_py, aes(x = \"location\",\n                     y = \"pm2_5\")) +\n    geom_jitter(alpha = 0.3, width = 0.1) +\n    geom_point(aes(y = lm_pm2_5_loc_py.fittedvalues), colour = \"blue\", size = 3))\n\np.show()\n\n\n\n\n\n\n\n\nOK, what’s going on here? I’ve plotted the fittedvalues (the values predicted by the model) in blue and overlaid the original (with a little bit of jitter to avoid overplotting). However, there are only two predicted values!\nWe can check this and see that each unique fitted value occurs 365 times, using the value_counts() function on the fitted values:\n\nlm_pm2_5_loc_py.fittedvalues.value_counts()\n\n16.942926    365\n12.831356    365\nName: count, dtype: int64\n\n\nThis makes sense if we think back to our original ANOVA exploration. There we established that an ANOVA is just a special case of a linear model, where the fitted values are equal to the mean of each group.\nWe could even check this:\n\npm2_5_py.groupby(\"location\")[\"pm2_5\"].mean()\n\nlocation\ninner    16.942926\nouter    12.831356\nName: pm2_5, dtype: float64\n\n\nSo, that matches. We move on and extract the coefficient estimates:\n\nlm_pm2_5_loc_py.params\n\nIntercept               16.942926\nC(location)[T.outer]    -4.111570\ndtype: float64\n\n\nThese values match up exactly with the predicted values for each individual location.\n\n\n\nThis gives us the following equation:\n\\(\\bar{PM2.5_{inner}} = 16.94\\)\n\\(\\bar{PM2.5_{outer}} = 16.94 - 4.11 = 12.83\\)\n\n\n13.6.4 The null model\nThe null model by itself is rarely analysed for its own sake but is instead used a reference point for more sophisticated model selection techniques. It represents your data as an overal average value.\n\nRPython\n\n\nWe define the null model as follows:\n\nlm_pm2_5_null &lt;- lm(pm2_5 ~ 1, data = pm2_5)\n\nWe can just view the model:\n\nlm_pm2_5_null\n\n\nCall:\nlm(formula = pm2_5 ~ 1, data = pm2_5)\n\nCoefficients:\n(Intercept)  \n      14.89  \n\n\n\n\nWe define the null model as follows:\n\n# create a linear model\nmodel = smf.ols(formula = \"pm2_5 ~ 1\", data = pm2_5_py)\n# and get the fitted parameters of the model\nlm_pm2_5_null_py = model.fit()\n\nWe can just view the model parameters:\n\nlm_pm2_5_null_py.params\n\nIntercept    14.887141\ndtype: float64\n\n\n\n\n\nThis shows us that there is just one value: 14.89. This is the average across all the PM2.5 values in the data set.\nHere we’d predict the PM2.5 values as follows:\n\\(PM2.5 = 14.89\\)",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#exercises",
    "href": "materials/cs5_practical_multiple-linear-regression.html#exercises",
    "title": "13  Multiple linear regression",
    "section": "13.7 Exercises",
    "text": "13.7 Exercises\n\n13.7.1 Trees\n\n\n\n\n\n\nExerciseExercise 1\n\n\n\n\n\n\nLevel: \nTrees: an example with only continuous variables\nUse the data/CS5-trees.csv data set. This is a data frame with 31 observations of 3 continuous variables. The variables are the height height, diameter girth and timber volume volume of 31 felled black cherry trees.\nInvestigate the relationship between volume (as a dependent variable) and height and girth (as predictor variables).\n\nHere all variables are continuous and so there isn’t a way of producing a 2D plot of all three variables for visualisation purposes using R’s standard plotting functions.\nconstruct four linear models\n\nAssume volume depends on height, girth and an interaction between girth and height\nAssume volume depends on height and girth but that there isn’t any interaction between them.\nAssume volume only depends on girth (plot the result, with the regression line).\nAssume volume only depends on height (plot the result, with the regression line).\n\nFor each linear model write down the algebraic equation that the linear model produces that relates volume to the two continuous predictor variables.\nCheck the assumptions of each model. Do you have any concerns?\n\nNB: For two continuous predictors, the interaction term is simply the two values multiplied together (so girth:height means girth x height)\n\nUse the equations to calculate the predicted volume of a tree that has a diameter of 20 inches and a height of 67 feet in each case.\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nLet’s construct the four linear models in turn.\n\nFull model\n\nRPython\n\n\nFirst, we read in the data:\n\ntrees &lt;- read_csv(\"data/CS5-trees.csv\")\n\n\n# define the model\nlm_trees_full &lt;- lm(volume ~ height * girth,\n                   data = trees)\n\n# view the model\nlm_trees_full\n\n\nCall:\nlm(formula = volume ~ height * girth, data = trees)\n\nCoefficients:\n (Intercept)        height         girth  height:girth  \n     69.3963       -1.2971       -5.8558        0.1347  \n\n\n\n\nFirst, we read in the data:\n\ntrees_py = pd.read_csv(\"data/CS5-trees.csv\")\n\n\n# create a linear model\nmodel = smf.ols(formula = \"volume ~ height * girth\",\n                data = trees_py)\n# and get the fitted parameters of the model\nlm_trees_full_py = model.fit()\n\nExtract the parameters:\n\nlm_trees_full_py.params\n\nIntercept       69.396316\nheight          -1.297083\ngirth           -5.855848\nheight:girth     0.134654\ndtype: float64\n\n\n\n\n\nWe can use this output to get the following equation:\nvolume = 69.40 + -1.30 \\(\\times\\) height + -5.86 \\(\\times\\) girth + 0.13 \\(\\times\\) height \\(\\times\\) girth\nIf we stick the numbers in (girth = 20 and height = 67) we get the following equation:\nvolume = 69.40 + -1.30 \\(\\times\\) 67 + -5.86 \\(\\times\\) 20 + 0.13 \\(\\times\\) 67 \\(\\times\\) 20\nvolume = 45.81\nHere we note that the interaction term just requires us to multiple the three numbers together (we haven’t looked at continuous predictors before in the examples and this exercise was included as a check to see if this whole process was making sense).\nIf we look at the diagnostic plots for the model using the following commands we get:\n\nRPython\n\n\n\nresid_panel(lm_trees_full,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\n\n\ndgplots(lm_trees_full_py)\n\n\n\n\n\n\n\nAll assumptions are OK.\n\nThere is some suggestion of heterogeneity of variance (with the variance being lower for small and large fitted (i.e. predicted volume) values), but that can be attributed to there only being a small number of data points at the edges, so I’m not overly concerned.\nSimilarly, there is a suggestion of snaking in the Q-Q plot (suggesting some lack of normality) but this is mainly due to the inclusion of one data point and overall the plot looks acceptable.\nThere are no highly influential points\n\n\n\nAdditive model\n\nRPython\n\n\n\n# define the model\nlm_trees_add &lt;- lm(volume ~ height + girth,\n                   data = trees)\n\n# view the model\nlm_trees_add\n\n\nCall:\nlm(formula = volume ~ height + girth, data = trees)\n\nCoefficients:\n(Intercept)       height        girth  \n   -57.9877       0.3393       4.7082  \n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"volume ~ height + girth\",\n                data = trees_py)\n# and get the fitted parameters of the model\nlm_trees_add_py = model.fit()\n\nExtract the parameters:\n\nlm_trees_add_py.params\n\nIntercept   -57.987659\nheight        0.339251\ngirth         4.708161\ndtype: float64\n\n\n\n\n\nWe can use this output to get the following equation:\nvolume = -57.99 + 0.34 \\(\\times\\) height + 4.71 \\(\\times\\) girth\nIf we stick the numbers in (girth = 20 and height = 67) we get the following equation:\nvolume = -57.99 + 0.34 \\(\\times\\) 67 + 4.71 \\(\\times\\) 20\nvolume = 58.91\nIf we look at the diagnostic plots for the model using the following commands we get the following:\n\nRPython\n\n\n\nresid_panel(lm_trees_add,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\n\n\ndgplots(lm_trees_add_py)\n\n\n\n\n\n\n\nThis model isn’t great.\n\nThere is a worrying lack of linearity exhibited in the Residuals plot suggesting that this linear model isn’t appropriate.\nAssumptions of Normality seem OK\nEquality of variance is harder to interpret. Given the lack of linearity in the data it isn’t really sensible to interpret the Location-Scale plot as it stands (since the plot is generated assuming that we’ve fitted a straight line through the data), but for the sake of practicing interpretation we’ll have a go. There is definitely suggestions of heterogeneity of variance here with a cluster of points with fitted values of around 20 having noticeably lower variance than the rest of the dataset.\nOne point is influential and if there weren’t issues with the linearity of the model I would remove this point and repeat the analysis. As it stands there isn’t much point.\n\n\n\nHeight-only model\n\nRPython\n\n\n\n# define the model\nlm_height &lt;- lm(volume ~ height,\n              data = trees)\n\n# view the model\nlm_height\n\n\nCall:\nlm(formula = volume ~ height, data = trees)\n\nCoefficients:\n(Intercept)       height  \n    -87.124        1.543  \n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"volume ~ height\",\n                data = trees_py)\n# and get the fitted parameters of the model\nlm_height_py = model.fit()\n\nExtract the parameters:\n\nlm_height_py.params\n\nIntercept   -87.123614\nheight        1.543350\ndtype: float64\n\n\n\n\n\nWe can use this output to get the following equation:\nvolume = -87.12 + 1.54 \\(\\times\\) height\nIf we stick the numbers in (girth = 20 and height = 67) we get the following equation:\nvolume = -87.12 + 1.54 \\(\\times\\) 67\nvolume = 16.28\nIf we look at the diagnostic plots for the model using the following commands we get the following:\n\nRPython\n\n\n\nresid_panel(lm_height,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\n\n\ndgplots(lm_height_py)\n\n\n\n\n\n\n\nThis model also isn’t great.\n\nThe main issue here is the clear heterogeneity of variance. For trees with bigger volumes the data are much more spread out than for trees with smaller volumes (as can be seen clearly from the Location-Scale plot).\nApart from that, the assumption of Normality seems OK\nAnd there aren’t any hugely influential points in this model\n\n\n\nGirth-only model\n\nRPython\n\n\n\n# define the model\nlm_girth &lt;- lm(volume ~ girth,\n               data = trees)\n\n# view the model\nlm_girth\n\n\nCall:\nlm(formula = volume ~ girth, data = trees)\n\nCoefficients:\n(Intercept)        girth  \n    -36.943        5.066  \n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"volume ~ girth\",\n                data = trees_py)\n# and get the fitted parameters of the model\nlm_girth_py = model.fit()\n\nExtract the parameters:\n\nlm_girth_py.params\n\nIntercept   -36.943459\ngirth         5.065856\ndtype: float64\n\n\n\n\n\nWe can use this output to get the following equation:\nvolume = -36.94 + 5.07 \\(\\times\\) girth\nIf we stick the numbers in (girth = 20 and height = 67) we get the following equation:\nvolume = -36.94 + 5.07 \\(\\times\\) 20\nvolume = 64.37\nIf we look at the diagnostic plots for the model using the following commands we get the following:\n\nRPython\n\n\n\nresid_panel(lm_girth,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\n\n\ndgplots(lm_girth_py)\n\n\n\n\n\n\n\nThe diagnostic plots here look rather similar to the ones we generated for the additive model and we have the same issue with a lack of linearity, heterogeneity of variance and one of the data points being influential.",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#summary",
    "href": "materials/cs5_practical_multiple-linear-regression.html#summary",
    "title": "13  Multiple linear regression",
    "section": "13.8 Summary",
    "text": "13.8 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nWe can define a linear model with any combination of categorical and continuous predictor variables\nUsing the coefficients of the model we can construct the linear model equation\nThe underlying assumptions of a linear model with three (or more) predictor variables are the same as those of a two-way ANOVA",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html",
    "href": "materials/cs5_practical_model-comparisons.html",
    "title": "14  Model comparisons",
    "section": "",
    "text": "14.1 Libraries and functions",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#libraries-and-functions",
    "href": "materials/cs5_practical_model-comparisons.html#libraries-and-functions",
    "title": "14  Model comparisons",
    "section": "",
    "text": "NoteClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n14.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n# Helper functions for tidying data\nlibrary(broom)\n\n\n\n14.1.2 Functions\n\n# Calculates the Akaike Information Criterion\nstats::AIC()\n\n# Performs a backwards step-wise elimination process\nstats::step()\n\n\n\n\n\n14.1.3 Libraries\n\n# A fundamental package for scientific computing in Python\nimport numpy as np\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n\n\n14.1.4 Functions\n\n# Reads in a .csv file\npandas.DataFrame.read_csv()\n\n# Creates a model from a formula and data frame\nstatsmodels.formula.api.ols()",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#purpose-and-aim",
    "href": "materials/cs5_practical_model-comparisons.html#purpose-and-aim",
    "title": "14  Model comparisons",
    "section": "14.2 Purpose and aim",
    "text": "14.2 Purpose and aim\nIn the previous example we used a single data set and fitted five linear models to it depending on which predictor variables we used. Whilst this was fun (seriously, what else would you be doing right now?) it seems that there should be a “better way”. Well, thankfully there is! In fact there a several methods that can be used to compare different models in order to help identify “the best” model. More specifically, we can determine if a full model (which uses all available predictor variables and interactions) is necessary to appropriately describe the dependent variable, or whether we can throw away some of the terms (e.g. an interaction term) because they don’t offer any useful predictive power.\nHere we will use the Akaike Information Criterion in order to compare different models.",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#data-and-hypotheses",
    "href": "materials/cs5_practical_model-comparisons.html#data-and-hypotheses",
    "title": "14  Model comparisons",
    "section": "14.3 Data and hypotheses",
    "text": "14.3 Data and hypotheses\nThis section uses the data/CS5-ladybird.csv data set. This data set comprises of 20 observations of three variables (one dependent and two predictor). This records the clutch size (eggs) in a species of ladybird, alongside two potential predictor variables; the mass of the female (weight), and the colour of the male (male) which is a categorical variable.",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#backwards-stepwise-elimination",
    "href": "materials/cs5_practical_model-comparisons.html#backwards-stepwise-elimination",
    "title": "14  Model comparisons",
    "section": "14.4 Backwards Stepwise Elimination",
    "text": "14.4 Backwards Stepwise Elimination\nFirst, we load the data and visualise it:\n\nRPython\n\n\n\nladybird &lt;- read_csv(\"data/CS5-ladybird.csv\")\n\nhead(ladybird)\n\n# A tibble: 6 × 4\n     id male  weight  eggs\n  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     1 Wild    10.6    27\n2     2 Wild    10.6    26\n3     3 Wild    12.5    28\n4     4 Wild    11.4    24\n5     5 Wild    14.9    30\n6     6 Wild    10.4    21\n\n\nWe can visualise the data by male, so we can see if the eggs clutch size differs a lot between the two groups:\n\nggplot(ladybird,\n       aes(x = male, y = eggs)) +\n    geom_boxplot() +\n    geom_jitter(width = 0.05)\n\n\n\n\n\n\n\n\nWe can also plot the egg clutch size against the weight, again for each male colour group:\n\n# visualise the data\nggplot(ladybird,\n       aes(x = weight, y = eggs,\n           colour = male)) +\n    geom_point() +\n    scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\n\n\n\nladybird_py = pd.read_csv(\"data/CS5-ladybird.csv\")\n\nWe can visualise the data by male, so we can see if the eggs clutch size differs a lot between the two groups:\n\np = (ggplot(ladybird_py,\n        aes(x = \"male\", y = \"eggs\")) +\n        geom_boxplot() +\n        geom_jitter(width = 0.05))\n\np.show()\n\n\n\n\n\n\n\n\nWe can also plot the egg clutch size against the weight, again for each male colour group:\n\np = (ggplot(ladybird_py,\n        aes(x = \"weight\", y = \"eggs\",\n            colour = \"male\")) +\n        geom_point() +\n        scale_color_brewer(type = \"qual\",\n                           palette = \"Dark2\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nWe can see a few things already:\n\nThere aren’t a huge number of data points in each group, so we need to be a bit cautious with drawing any firm conclusions.\nThere is quite some spread in the egg clutch sizes, with two observations in the Melanic group being rather low.\nFrom the box plot, there does not seem to be much difference in the egg clutch size between the two male colour groups.\nThe scatter plot suggests that egg clutch size seems to increase somewhat linearly as the weight of the female goes up. There does not seem to be much difference between the two male colour groups in this respect.\n\n\n14.4.1 Comparing models with AIC (step 1)\nWe start with the complete or full model, that takes into account any possible interaction between weight and male.\nNext, we define the reduced model. This is the next, most simple model. In this case we’re removing the interaction and constructing an additive model.\n\nRPython\n\n\n\n# define the full model\nlm_full &lt;- lm(eggs ~ weight * male,\n              data = ladybird)\n\n\n# define the additive model\nlm_add &lt;- lm(eggs ~ weight + male,\n             data = ladybird)\n\nWe then extract the AIC values for each of the models:\n\nAIC(lm_full)\n\n[1] 100.0421\n\nAIC(lm_add)\n\n[1] 99.19573\n\n\n\n\n\n# create the model\nmodel = smf.ols(formula= \"eggs ~ weight * C(male)\", data = ladybird_py)\n# and get the fitted parameters of the model\nlm_full_py = model.fit()\n\n\n# create the additive linear model\nmodel = smf.ols(formula= \"eggs ~ weight + C(male)\", data = ladybird_py)\n# and get the fitted parameters of the model\nlm_add_py = model.fit()\n\nWe then extract the AIC values for each of the models:\n\nlm_full_py.aic\n\nnp.float64(98.04206256815984)\n\nlm_add_py.aic\n\nnp.float64(97.19572958073036)\n\n\n\n\n\nEach line tells you the AIC score for that model. The full model has 4 parameters (the intercept, the coefficient for the continuous variable weight, the coefficient for the categorical variable male and a coefficient for the interaction term weight:male). The additive model has a lower AIC score with only 3 parameters (since we’ve dropped the interaction term). There are different ways of interpreting AIC scores but the most widely used interpretation says that:\n\nif the difference between two AIC scores is greater than 2, then the model with the smallest AIC score is more supported than the model with the higher AIC score\nif the difference between the two models’ AIC scores is less than 2 then both models are equally well supported\n\nThis choice of language (supported vs significant) is deliberate and there are areas of statistics where AIC scores are used differently from the way we are going to use them here (ask if you want a bit of philosophical ramble from me). However, in this situation we will use the AIC scores to decide whether our reduced model is at least as good as the full model. Here since the difference in AIC scores is less than 2, we can say that dropping the interaction term has left us with a model that is both simpler (fewer terms) and as least as good (AIC score) as the full model. As such our additive model eggs ~ weight + male is designated our current working minimal model.\n\n\n14.4.2 Comparing models with AIC (step 2)\nNext, we see which of the remaining terms can be dropped. We will look at the models where we have dropped both male and weight (i.e. eggs ~ weight and eggs ~ male) and compare their AIC values with the AIC of our current minimal model (eggs ~ weight + male). If the AIC values of at least one of our new reduced models is lower (or at least no more than 2 greater) than the AIC of our current minimal model, then we can drop the relevant term and get ourselves a new minimal model. If we find ourselves in a situation where we can drop more than one term we will drop the term that gives us the model with the lowest AIC.\n\nRPython\n\n\nDrop the variable weight and examine the AIC:\n\n# define the model\nlm_male &lt;- lm(eggs ~ male,\n              data = ladybird)\n\n# extract the AIC\nAIC(lm_male)\n\n[1] 118.7093\n\n\nDrop the variable male and examine the AIC:\n\n# define the model\nlm_weight &lt;- lm(eggs ~ weight,\n                data = ladybird)\n\n# extract the AIC\nAIC(lm_weight)\n\n[1] 97.52601\n\n\n\n\nDrop the variable weight and examine the AIC:\n\n# create the model\nmodel = smf.ols(formula= \"eggs ~ C(male)\", data = ladybird_py)\n# and get the fitted parameters of the model\nlm_male_py = model.fit()\n\n# extract the AIC\nlm_male_py.aic\n\nnp.float64(116.7092646564482)\n\n\nDrop the variable male and examine the AIC:\n\n# create the model\nmodel = smf.ols(formula= \"eggs ~ weight\", data = ladybird_py)\n# and get the fitted parameters of the model\nlm_weight_py = model.fit()\n\n# extract the AIC\nlm_weight_py.aic\n\nnp.float64(95.52601286248304)\n\n\n\n\n\nConsidering both outputs together and comparing with the AIC of our current minimal model, we can see that dropping male has decreased the AIC further, whereas dropping weight has actually increased the AIC and thus worsened the model quality.\nHence we can drop male and our new minimal model is eggs ~ weight.\n\n\n14.4.3 Comparing models with AIC (step 3)\nOur final comparison is to drop the variable weight and compare this simple model with a null model (eggs ~ 1), which assumes that the clutch size is constant across all parameters.\nDrop the variable weight and see if that has an effect:\n\nRPython\n\n\n\n# define the model\nlm_null &lt;- lm(eggs ~ 1,\n              data = ladybird)\n\n# extract the AIC\nAIC(lm_null)\n\n[1] 117.2178\n\n\n\n\n\n# create the model\nmodel = smf.ols(formula= \"eggs ~ 1\", data = ladybird_py)\n# and get the fitted parameters of the model\nlm_null_py = model.fit()\n\n# extract the AIC\nlm_null_py.aic\n\nnp.float64(115.21783038624153)\n\n\n\n\n\nThe AIC of our null model is quite a bit larger than that of our current minimal model eggs ~ weight and so we conclude that weight is important. As such our minimal model is eggs ~ weight.\nSo, in summary, we could conclude that:\n\nFemale size is a useful predictor of clutch size, but male type is not so important.\n\nAt this point we can then continue analysing this minimal model, by checking the diagnostic plots and checking the assumptions. If they all pan out, then we can continue with an ANOVA.",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#notes-on-backwards-stepwise-elimination",
    "href": "materials/cs5_practical_model-comparisons.html#notes-on-backwards-stepwise-elimination",
    "title": "14  Model comparisons",
    "section": "14.5 Notes on Backwards Stepwise Elimination",
    "text": "14.5 Notes on Backwards Stepwise Elimination\nThis method of finding a minimal model by starting with a full model and removing variables is called backward stepwise elimination. Although regularly practised in data analysis, there is increasing criticism of this approach, with calls for it to be avoided entirely.\nWhy have we made you work through this procedure then? Given their prevalence in academic papers, it is very useful to be aware of these procedures and to know that there are issues with them. In other situations, using AIC for model comparisons are justified and you will come across them regularly. Additionally, there may be situations where you feel there are good reasons to drop a parameter from your model – using this technique you can justify that this doesn’t affect the model fit. Taken together, using backwards stepwise elimination for model comparison is still a useful technique.\n\n\n\n\n\n\nNoteAutomatic BSE in R (but not Python)\n\n\n\nPerforming backwards stepwise elimination manually can be quite tedious. Thankfully R acknowledges this and there is a single inbuilt function called step() that can perform all of the necessary steps for you using AIC.\n\n# define the full model\nlm_full &lt;- lm(eggs ~ weight * male,\n              data = ladybird)\n\n# perform backwards stepwise elimination\nstep(lm_full)\n\nThis will perform a full backwards stepwise elimination process and will find the minimal model for you.\nYes, I could have told you this earlier, but where’s the fun in that? (it is also useful for you to understand the steps behind the technique I suppose…)\nWhen doing this in Python, you are a bit stuck. There does not seem to be an equivalent function. If you want to cobble something together yourself, then use this link as a starting point.",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#exercises",
    "href": "materials/cs5_practical_model-comparisons.html#exercises",
    "title": "14  Model comparisons",
    "section": "14.6 Exercises",
    "text": "14.6 Exercises\nWe are going to practice the backwards stepwise elimination technique on some of the data sets we analysed previously.\nFor each of the following data sets I would like you to:\n\nDefine the response variable\nDefine the relevant predictor variables\nDefine relevant interactions\nPerform a backwards stepwise elimination and discover the minimal model using AIC\n\nNB: if an interaction term is significant then any main factor that is part of the interaction term cannot be dropped from the model.\nPerform a BSE on the following data sets:\n\ndata/CS5-trees.csv\ndata/CS5-pm2_5.csv\n\n\n14.6.1 BSE: Trees\n\n\n\n\n\n\nExerciseExercise 1\n\n\n\n\n\n\nLevel: \nBSE on trees:\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n14.7 Answer\nLet’s start by reading in the data and checking which variables are in the data set.\n\nRPython\n\n\n\ntrees &lt;- read_csv(\"data/CS5-trees.csv\")\n\nhead(trees)\n\n# A tibble: 6 × 3\n  girth height volume\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n\n\n\nFirst, we read in the data (if needed) and have a look at the variables:\n\ntrees_py = pd.read_csv(\"data/CS5-trees.csv\")\n\ntrees_py.head()\n\n   girth  height  volume\n0    8.3      70    10.3\n1    8.6      65    10.3\n2    8.8      63    10.2\n3   10.5      72    16.4\n4   10.7      81    18.8\n\n\n\n\n\n\nThe response variable is volume\nThe predictor variables are girth and height\nThe only possible interaction term is girth:height\nWe perform a BSE on the model using the step() function\n\nThe full model is volume ~ girth * height.\nWe perform the BSE as follows:\n\nRPython\n\n\nDefine the model and use the step() function:\n\n# define the full model\nlm_trees &lt;- lm(volume ~ girth * height,\n               data = trees)\n\n# perform BSE\nstep(lm_trees)\n\nStart:  AIC=65.49\nvolume ~ girth * height\n\n               Df Sum of Sq    RSS    AIC\n&lt;none&gt;                      198.08 65.495\n- girth:height  1    223.84 421.92 86.936\n\n\n\nCall:\nlm(formula = volume ~ girth * height, data = trees)\n\nCoefficients:\n (Intercept)         girth        height  girth:height  \n     69.3963       -5.8558       -1.2971        0.1347  \n\n\n\n\nWe first define the full model, then the model without the interaction (model_1).\nWe extract the AICs for both models. Because we do not have an automated way of performing the BSE, we’re stringing together a few operations to make the code a bit more concise (getting the .fit() of the model and immediately extracting the aic value):\n\n# define the models\nmodel_full = smf.ols(formula= \"volume ~ girth * height\",\n                     data = trees_py)\n\nmodel_1 = smf.ols(formula= \"volume ~ girth + height\",\n                  data = trees_py)\n\n# get the AIC of the model\nmodel_full.fit().aic\n\nnp.float64(153.46916240903528)\n\nmodel_1.fit().aic\n\nnp.float64(174.9099729872703)\n\n\n\n\n\nThis BSE approach only gets as far as the first step (trying to drop the interaction term). We see immediately that dropping the interaction term makes the model worse. This means that the best model is still the full model.\n\n\n\n\n\n\n\n\n\n\n14.7.1 BSE: Air pollution\n\n\n\n\n\n\nExerciseExercise 2\n\n\n\n\n\n\nLevel: \nPerform a BSE on pm2_5. Let’s start by reading in the data and checking which variables are in the data set.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n14.8 Answer\n\nRPython\n\n\n\npm2_5 &lt;- read_csv(\"data/CS5-pm2_5.csv\")\n\nhead(pm2_5)\n\n# A tibble: 6 × 6\n  avg_temp date       location pm2_5 rain_mm wind_m_s\n     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1      4.5 01/01/2019 inner     17.1     2.3     3.87\n2      4.9 01/01/2019 outer     10.8     2.3     5.84\n3      4.3 02/01/2019 inner     14.9     2.3     3.76\n4      4.8 02/01/2019 outer     11.4     2.3     6   \n5      4   03/01/2019 inner     18.5     1.4     2.13\n6      4.5 03/01/2019 outer     15.0     1.4     2.57\n\n\n\n\n\npm2_5_py = pd.read_csv(\"data/CS5-pm2_5.csv\")\n\npm2_5_py.head()\n\n   avg_temp        date location   pm2_5  rain_mm  wind_m_s\n0       4.5  01/01/2019    inner  17.126      2.3      3.87\n1       4.9  01/01/2019    outer  10.821      2.3      5.84\n2       4.3  02/01/2019    inner  14.884      2.3      3.76\n3       4.8  02/01/2019    outer  11.416      2.3      6.00\n4       4.0  03/01/2019    inner  18.471      1.4      2.13\n\n\n\n\n\n\nThe response variable is pm2_5\nThe predictor variables are all the variables, apart from date and pm2_5. It would be strange to try and create a model that relies on each individual measurement!\nWe can add the wind_m_s:location interaction, since it appeared that there is a difference between inner and outer London pollution levels, in relation to wind speed\nWe can start the backwards stepwise elimination with the following model:\n\npm2_5 ~ avg_temp + rain_mm + wind_m_s * location\n\nRPython\n\n\n\n# define the model\nlm_pm2_5 &lt;- lm(pm2_5 ~ avg_temp + rain_mm + wind_m_s * location,\n               data = pm2_5)\n\n# perform BSE\nstep(lm_pm2_5)\n\nStart:  AIC=41.08\npm2_5 ~ avg_temp + rain_mm + wind_m_s * location\n\n                    Df Sum of Sq    RSS     AIC\n- rain_mm            1     0.351 760.01  39.412\n- avg_temp           1     1.804 761.47  40.806\n&lt;none&gt;                           759.66  41.075\n- wind_m_s:location  1   134.820 894.48 158.336\n\nStep:  AIC=39.41\npm2_5 ~ avg_temp + wind_m_s + location + wind_m_s:location\n\n                    Df Sum of Sq    RSS     AIC\n- avg_temp           1     1.758 761.77  39.098\n&lt;none&gt;                           760.01  39.412\n- wind_m_s:location  1   134.530 894.54 156.385\n\nStep:  AIC=39.1\npm2_5 ~ wind_m_s + location + wind_m_s:location\n\n                    Df Sum of Sq    RSS     AIC\n&lt;none&gt;                           761.77  39.098\n- wind_m_s:location  1    136.95 898.72 157.788\n\n\n\nCall:\nlm(formula = pm2_5 ~ wind_m_s + location + wind_m_s:location, \n    data = pm2_5)\n\nCoefficients:\n           (Intercept)                wind_m_s           locationouter  \n               18.2422                 -0.2851                 -2.0597  \nwind_m_s:locationouter  \n               -0.4318  \n\n\n\n\nWe first define the full model, again stringing together a few operations to be more concise.\n\n# define the model\nmodel_full = smf.ols(formula= \"pm2_5 ~ avg_temp + rain_mm + wind_m_s * C(location)\", data = pm2_5_py)\n\n# get the AIC of the model\nmodel_full.fit().aic\n\nnp.float64(2112.725435984824)\n\n\nCan we drop the interaction term or any of the remaining main effects?\n\n# define the model\nmodel_1 = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + rain_mm + wind_m_s + C(location)\",\n    data = pm2_5_py)\n    \nmodel_2 = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + wind_m_s * C(location)\",\n    data = pm2_5_py)\n    \nmodel_3 = smf.ols(\n    formula= \"pm2_5 ~ rain_mm + wind_m_s * C(location)\",\n    data = pm2_5_py)\n\n# get the AIC of the models\nmodel_1.fit().aic\n\nnp.float64(2229.9865962235162)\n\nmodel_2.fit().aic\n\nnp.float64(2111.06230638372)\n\nmodel_3.fit().aic\n\nnp.float64(2112.456639492829)\n\n# compare to the full model\nmodel_full.fit().aic\n\nnp.float64(2112.725435984824)\n\n\nThe AIC goes up quite a bit if we drop the interaction term. This means that we cannot drop the interaction term, nor any of the main effects that are included in the interaction. These are wind_m_s and location.\nThe model with the lowest AIC is the one without the rain_mm term, so our working model is:\n\nworking_model = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + wind_m_s * C(location)\",\n    data = pm2_5_py)\n\nNow we can again check if dropping the avg_temp term or the wind_m_s:location interaction has any effect on the model performance:\n\nmodel_1 = smf.ols(\n    formula= \"pm2_5 ~ wind_m_s * C(location)\",\n    data = pm2_5_py)\n    \nmodel_2 = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + wind_m_s + C(location)\",\n    data = pm2_5_py)\n\n# get the AIC of the models\nmodel_1.fit().aic\n\nnp.float64(2110.748543452394)\n\nmodel_2.fit().aic\n\nnp.float64(2228.0355952158666)\n\nworking_model.fit().aic\n\nnp.float64(2111.06230638372)\n\n\nThis shows that dropping the avg_temp term lowers the AIC, whereas dropping the interaction term makes the model markedly worse.\nSo, our new working model is:\n\nworking_model = smf.ols(\n    formula= \"pm2_5 ~ wind_m_s * C(location)\",\n    data = pm2_5_py)\n\nLastly, now we’ve dropped the avg_temp term we can do one final check on the interaction term:\n\nmodel_1 = smf.ols(\n    formula= \"pm2_5 ~ wind_m_s + C(location) + wind_m_s + C(location)\",\n    data = pm2_5_py)\n\nmodel_1.fit().aic\n\nnp.float64(2229.438631394105)\n\nworking_model.fit().aic\n\nnp.float64(2110.748543452394)\n\n\nDropping the interaction still makes the model worse.\n\n\n\nOur minimal model is thus:\npm2_5 ~ wind_m_s + location + wind_m_s:location",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#answer",
    "href": "materials/cs5_practical_model-comparisons.html#answer",
    "title": "14  Model comparisons",
    "section": "14.7 Answer",
    "text": "14.7 Answer\nLet’s start by reading in the data and checking which variables are in the data set.\n\nRPython\n\n\n\ntrees &lt;- read_csv(\"data/CS5-trees.csv\")\n\nhead(trees)\n\n# A tibble: 6 × 3\n  girth height volume\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n\n\n\nFirst, we read in the data (if needed) and have a look at the variables:\n\ntrees_py = pd.read_csv(\"data/CS5-trees.csv\")\n\ntrees_py.head()\n\n   girth  height  volume\n0    8.3      70    10.3\n1    8.6      65    10.3\n2    8.8      63    10.2\n3   10.5      72    16.4\n4   10.7      81    18.8\n\n\n\n\n\n\nThe response variable is volume\nThe predictor variables are girth and height\nThe only possible interaction term is girth:height\nWe perform a BSE on the model using the step() function\n\nThe full model is volume ~ girth * height.\nWe perform the BSE as follows:\n\nRPython\n\n\nDefine the model and use the step() function:\n\n# define the full model\nlm_trees &lt;- lm(volume ~ girth * height,\n               data = trees)\n\n# perform BSE\nstep(lm_trees)\n\nStart:  AIC=65.49\nvolume ~ girth * height\n\n               Df Sum of Sq    RSS    AIC\n&lt;none&gt;                      198.08 65.495\n- girth:height  1    223.84 421.92 86.936\n\n\n\nCall:\nlm(formula = volume ~ girth * height, data = trees)\n\nCoefficients:\n (Intercept)         girth        height  girth:height  \n     69.3963       -5.8558       -1.2971        0.1347  \n\n\n\n\nWe first define the full model, then the model without the interaction (model_1).\nWe extract the AICs for both models. Because we do not have an automated way of performing the BSE, we’re stringing together a few operations to make the code a bit more concise (getting the .fit() of the model and immediately extracting the aic value):\n\n# define the models\nmodel_full = smf.ols(formula= \"volume ~ girth * height\",\n                     data = trees_py)\n\nmodel_1 = smf.ols(formula= \"volume ~ girth + height\",\n                  data = trees_py)\n\n# get the AIC of the model\nmodel_full.fit().aic\n\nnp.float64(153.46916240903528)\n\nmodel_1.fit().aic\n\nnp.float64(174.9099729872703)\n\n\n\n\n\nThis BSE approach only gets as far as the first step (trying to drop the interaction term). We see immediately that dropping the interaction term makes the model worse. This means that the best model is still the full model.",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#answer-1",
    "href": "materials/cs5_practical_model-comparisons.html#answer-1",
    "title": "14  Model comparisons",
    "section": "14.8 Answer",
    "text": "14.8 Answer\n\nRPython\n\n\n\npm2_5 &lt;- read_csv(\"data/CS5-pm2_5.csv\")\n\nhead(pm2_5)\n\n# A tibble: 6 × 6\n  avg_temp date       location pm2_5 rain_mm wind_m_s\n     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1      4.5 01/01/2019 inner     17.1     2.3     3.87\n2      4.9 01/01/2019 outer     10.8     2.3     5.84\n3      4.3 02/01/2019 inner     14.9     2.3     3.76\n4      4.8 02/01/2019 outer     11.4     2.3     6   \n5      4   03/01/2019 inner     18.5     1.4     2.13\n6      4.5 03/01/2019 outer     15.0     1.4     2.57\n\n\n\n\n\npm2_5_py = pd.read_csv(\"data/CS5-pm2_5.csv\")\n\npm2_5_py.head()\n\n   avg_temp        date location   pm2_5  rain_mm  wind_m_s\n0       4.5  01/01/2019    inner  17.126      2.3      3.87\n1       4.9  01/01/2019    outer  10.821      2.3      5.84\n2       4.3  02/01/2019    inner  14.884      2.3      3.76\n3       4.8  02/01/2019    outer  11.416      2.3      6.00\n4       4.0  03/01/2019    inner  18.471      1.4      2.13\n\n\n\n\n\n\nThe response variable is pm2_5\nThe predictor variables are all the variables, apart from date and pm2_5. It would be strange to try and create a model that relies on each individual measurement!\nWe can add the wind_m_s:location interaction, since it appeared that there is a difference between inner and outer London pollution levels, in relation to wind speed\nWe can start the backwards stepwise elimination with the following model:\n\npm2_5 ~ avg_temp + rain_mm + wind_m_s * location\n\nRPython\n\n\n\n# define the model\nlm_pm2_5 &lt;- lm(pm2_5 ~ avg_temp + rain_mm + wind_m_s * location,\n               data = pm2_5)\n\n# perform BSE\nstep(lm_pm2_5)\n\nStart:  AIC=41.08\npm2_5 ~ avg_temp + rain_mm + wind_m_s * location\n\n                    Df Sum of Sq    RSS     AIC\n- rain_mm            1     0.351 760.01  39.412\n- avg_temp           1     1.804 761.47  40.806\n&lt;none&gt;                           759.66  41.075\n- wind_m_s:location  1   134.820 894.48 158.336\n\nStep:  AIC=39.41\npm2_5 ~ avg_temp + wind_m_s + location + wind_m_s:location\n\n                    Df Sum of Sq    RSS     AIC\n- avg_temp           1     1.758 761.77  39.098\n&lt;none&gt;                           760.01  39.412\n- wind_m_s:location  1   134.530 894.54 156.385\n\nStep:  AIC=39.1\npm2_5 ~ wind_m_s + location + wind_m_s:location\n\n                    Df Sum of Sq    RSS     AIC\n&lt;none&gt;                           761.77  39.098\n- wind_m_s:location  1    136.95 898.72 157.788\n\n\n\nCall:\nlm(formula = pm2_5 ~ wind_m_s + location + wind_m_s:location, \n    data = pm2_5)\n\nCoefficients:\n           (Intercept)                wind_m_s           locationouter  \n               18.2422                 -0.2851                 -2.0597  \nwind_m_s:locationouter  \n               -0.4318  \n\n\n\n\nWe first define the full model, again stringing together a few operations to be more concise.\n\n# define the model\nmodel_full = smf.ols(formula= \"pm2_5 ~ avg_temp + rain_mm + wind_m_s * C(location)\", data = pm2_5_py)\n\n# get the AIC of the model\nmodel_full.fit().aic\n\nnp.float64(2112.725435984824)\n\n\nCan we drop the interaction term or any of the remaining main effects?\n\n# define the model\nmodel_1 = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + rain_mm + wind_m_s + C(location)\",\n    data = pm2_5_py)\n    \nmodel_2 = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + wind_m_s * C(location)\",\n    data = pm2_5_py)\n    \nmodel_3 = smf.ols(\n    formula= \"pm2_5 ~ rain_mm + wind_m_s * C(location)\",\n    data = pm2_5_py)\n\n# get the AIC of the models\nmodel_1.fit().aic\n\nnp.float64(2229.9865962235162)\n\nmodel_2.fit().aic\n\nnp.float64(2111.06230638372)\n\nmodel_3.fit().aic\n\nnp.float64(2112.456639492829)\n\n# compare to the full model\nmodel_full.fit().aic\n\nnp.float64(2112.725435984824)\n\n\nThe AIC goes up quite a bit if we drop the interaction term. This means that we cannot drop the interaction term, nor any of the main effects that are included in the interaction. These are wind_m_s and location.\nThe model with the lowest AIC is the one without the rain_mm term, so our working model is:\n\nworking_model = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + wind_m_s * C(location)\",\n    data = pm2_5_py)\n\nNow we can again check if dropping the avg_temp term or the wind_m_s:location interaction has any effect on the model performance:\n\nmodel_1 = smf.ols(\n    formula= \"pm2_5 ~ wind_m_s * C(location)\",\n    data = pm2_5_py)\n    \nmodel_2 = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + wind_m_s + C(location)\",\n    data = pm2_5_py)\n\n# get the AIC of the models\nmodel_1.fit().aic\n\nnp.float64(2110.748543452394)\n\nmodel_2.fit().aic\n\nnp.float64(2228.0355952158666)\n\nworking_model.fit().aic\n\nnp.float64(2111.06230638372)\n\n\nThis shows that dropping the avg_temp term lowers the AIC, whereas dropping the interaction term makes the model markedly worse.\nSo, our new working model is:\n\nworking_model = smf.ols(\n    formula= \"pm2_5 ~ wind_m_s * C(location)\",\n    data = pm2_5_py)\n\nLastly, now we’ve dropped the avg_temp term we can do one final check on the interaction term:\n\nmodel_1 = smf.ols(\n    formula= \"pm2_5 ~ wind_m_s + C(location) + wind_m_s + C(location)\",\n    data = pm2_5_py)\n\nmodel_1.fit().aic\n\nnp.float64(2229.438631394105)\n\nworking_model.fit().aic\n\nnp.float64(2110.748543452394)\n\n\nDropping the interaction still makes the model worse.\n\n\n\nOur minimal model is thus:\npm2_5 ~ wind_m_s + location + wind_m_s:location",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#summary",
    "href": "materials/cs5_practical_model-comparisons.html#summary",
    "title": "14  Model comparisons",
    "section": "14.9 Summary",
    "text": "14.9 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nWe can use Backwards Stepwise Elimination (BSE) on a full model to see if certain terms add to the predictive power of the model or not\nThe AIC allows us to compare different models - if there is a difference in AIC of more than 2 between two models, then the smallest AIC score is more supported",
    "crumbs": [
      "CS5: Multiple predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html",
    "href": "materials/cs6_practical_power-analysis.html",
    "title": "15  Power analysis",
    "section": "",
    "text": "15.1 Libraries and functions",
    "crumbs": [
      "CS6: Statistical power",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#libraries-and-functions",
    "href": "materials/cs6_practical_power-analysis.html#libraries-and-functions",
    "title": "15  Power analysis",
    "section": "",
    "text": "NoteClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n15.1.1 Libraries\n\n# A library for power analysis\nlibrary(pwr)\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n\n\n15.1.2 Functions\n\n\n\n\n15.1.3 Libraries\n\n# A fundamental package for scientific computing in Python\nimport numpy as np\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n# Custom Python script for diagnostic plots, load from base working directory\nfrom dgplots import dgplots\n\n# Custom Python script for power calculations, load from base working directory\nfrom pwr_f2_test import *\n\n\n\n15.1.4 Functions\n\n# Custom function to create diagnostic plots\ndgplots()\n\n# Custom function to perform power calculations\npwr_f2_test()\n\nNote: you can download the pwr_f2_test script here and the dgplots script here.",
    "crumbs": [
      "CS6: Statistical power",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#background",
    "href": "materials/cs6_practical_power-analysis.html#background",
    "title": "15  Power analysis",
    "section": "15.2 Background",
    "text": "15.2 Background\nAll hypothesis tests can be wrong in two ways:\n\nwe can appear to have found a significant result when there really isn’t anything there: a false positive (or Type I error), or\nwe can fail to spot a significant result when there really is something interesting going on: a false negative (or Type II error).\n\nThe probability of getting a false positive in our analysis is precisely the significance level we use in our analysis. So, in order to reduce the likelihood of getting a false positive we simply reduce the significance level of our test (from 0.05 down to 0.01 say). Easy as that.\nUnfortunately, this has unintended consequences (doesn’t everything?). It turns out that reducing the significance level means that we increase the chance of getting false negatives. This should make sense; if we’re increasing the barrier to entry in terms of acceptance then we’ll also accidentally miss out on some of the good stuff.\nPower is the capacity of a test to detect significant different results. It is affected by three things:\n\nthe effect size: i.e. how big of a difference do you want to be able to detect, or alternatively what do you consider a meaningful effect/difference to be?\nsample size\nthe significance level\n\nIn an ideal world we would want to be carrying out highly powerful tests using low significance levels, to both reduce our chance of getting a false positive and maximise our chances of finding a true effect.\nPower analysis allows us to design experiments to do just that. Given:\n\na desired power (0.8 or 80% is considered pretty good)\na significance level (0.05 or 5% is our trusty yet arbitrary steed once again)\nan effect size that we would like to detect\n\nWe can calculate the amount of data that we need to collect in our experiments. (Woohoo! it looks as if statistics will actually give us an answer at last rather than these perpetual shades-of-grey “maybes”).\nThe reality is that most of the easily usable power analysis functions all operate under the assumption that the data that you will collect will meet all of the assumptions of your chosen statistical test perfectly. So, for example, if you want to design an experiment investigating the effectiveness of a single drug compared to a placebo (so a simple t-test) and you want to know how many patients to have in each group in order for the test to work, then the standard power analysis techniques will still assume that all of the data that you end up collecting will meet the assumptions of the t-test that you have to carry out (sorry to have raised your hopes ever so slightly 😉).\n\n15.2.1 Effect size\nAs we shall see the commands for carrying out power analyses are very simple to implement apart from the concept of effect size. This is a tricky issue for most people to get to grips with for two reasons:\n\nEffect size is related to biological significance rather than statistical significance\nThe way in which we specify effect sizes\n\n\n\n\n\n\n\nNote\n\n\n\nWith respect to the first point a common conversation goes a bit like this:\nme: “So you’ve been told to carry out a power analysis, eh? Lucky you. What sort of effect size are you looking for?”\nyou: “I have no idea what you’re talking about. I want to know if my drug is any better than a placebo. How many patients do I need?”\nme: “It depends on how big a difference you think your drug will have compared to the placebo.”\nyou: “I haven’t carried out my experiment yet, so I have absolutely no idea how big the effect will be!”\nme: \n(To be honest this would be a relatively well-informed conversation: this is much closer to how things actually go)\n\n\nThe key point about effect sizes and power analyses is that you need to specify an effect size that you would be interested in observing, or one that would be biologically relevant to see. There may well actually be a 0.1% difference in effectiveness of your drug over a placebo but designing an experiment to detect that would require markedly more individuals than an experiment that was trying to detect a 50% difference in effectiveness. In reality there are three places we can get a sense of effect sizes from:\n\nA pilot study\nPrevious literature or theory\nJacob Cohen\n\nJacob Cohen was an American statistician who developed a large set of measures for effect sizes (which we will use today). He came up with a rough set of numerical measures for “small”, “medium” and “large” effect sizes that are still in use today. These do come with some caveats though; Jacob was a psychologist and so his assessment of what was a large effect may be somewhat different from yours. They do form a useful starting point however.\nThere a lot of different ways of specifying effects sizes, but we can split them up into three distinct families of estimates:\n\nCorrelation estimates: these use \\(R^2\\) as a measure of variance explained by a model (for linear models, anova etc. A large \\(R^2\\) value would indicate that a lot of variance has been explained by our model and we would expect to see a lot of difference between groups, or a tight cluster of points around a line of best fit. The argument goes that we would need fewer data points to observe such a relationship with confidence. Trying to find a relationship with a low \\(R^2\\) value would be trickier and would therefore require more data points for an equivalent power.\nDifference between means: these look at how far apart the means of two groups are, measured in units of standard deviations (for t-tests). An effect size of 2 in this case would be interpreted as the two groups having means that were two standard deviations away from each other (quite a big difference), whereas an effect size of 0.2 would be harder to detect and would require more data to pick it up.\nDifference between count data: these I freely admit I have no idea how to intuitively explain them (shock, horror). Mathematically they are based on the chi-squared statistic but that’s as good as I can tell you I’m afraid. They are, however, pretty easy to calculate.\n\nFor reference here are some of Cohen’s suggested values for effect sizes for different tests. You’ll probably be surprised by how small some of these are.\n\n\n\nTest\nSmall\nMedium\nLarge\n\n\n\n\nt-tests\n0.2\n0.5\n0.8\n\n\nanova\n0.1\n0.25\n0.4\n\n\nlinear models\n0.02\n0.15\n0.35\n\n\nchi-squared\n0.1\n0.3\n0.5\n\n\n\nWe will look at how to carry out power analyses and estimate effect sizes in this section.",
    "crumbs": [
      "CS6: Statistical power",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#power-analysis-t-test",
    "href": "materials/cs6_practical_power-analysis.html#power-analysis-t-test",
    "title": "15  Power analysis",
    "section": "15.3 Power analysis t-test",
    "text": "15.3 Power analysis t-test\nThe first example we’ll look at is how to perform a power analysis on two groups of data.\nLet’s assume that we want to design an experiment to determine whether there is a difference in the mean price of what male and female students pay at a cafe. How many male and female students would we need to observe in order to detect a “medium” effect size with 80% power and a significance level of 0.05?\nWe first need to think about which test we would use to analyse the data. Here we would have two groups of continuous response. Clearly a t-test.\n\n15.3.1 Determine effect size\nThe first thing we need to do is figure out what a “medium” effect size is. In absence of any further information we refer back to Cohen’s effect sizes.\n\nRPython\n\n\nWe’re using the pwr library, so make sure that you have installed and loaded it with the following commands:\n\n# install pwr package if needed\ninstall.packages(\"pwr\")\n\n# load the pwr package\nlibrary(pwr)\n\nWe can get Cohen’s effect size using the cohen.ES() function (ES stands for Effect Size):\n\ncohen.ES(test = \"t\", size = \"medium\")\n\n\n     Conventional effect size from Cohen (1982) \n\n           test = t\n           size = medium\n    effect.size = 0.5\n\n\nThis function just returns the default conventional values for effect sizes as determined by Jacob Cohen back in the day. It just saves us scrolling back up the page to look at the table I provided. It only takes two arguments:\n\ntest which is one of\n\n“t”, for t-tests,\n“anova” for anova,\n“f2” for linear models\n“chisq” for chi-squared test\n\nsize, which is just one of “small”, “medium” or “large”.\n\nThe bit we want is on the bottom line; we apparently want an effect size of 0.5.\n\n\nUnlike in R, Cohen’s effect sizes are not available through a package (that I am aware of). So in this case we’re referring back to the effect size table we saw earlier and define “medium” as 0.5.\n\n\n\nFor this sort of study effect size is measured in terms of Cohen’s d statistic. This is simply a measure of how different the means of the two groups are expressed in terms of the number of standard deviations they are apart from each other. So, in this case we’re looking to detect two means that are 0.5 standard deviations away from each other. In a minute we’ll look at what this means for real data.\n\n\n15.3.2 Calculating sample sizes\n\nRPython\n\n\nWe do this as follows:\n\npwr.t.test(d = 0.5, sig.level = 0.05, power = 0.8,\n           type = \"two.sample\", alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 63.76561\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThe first line is what we’re looking for n = 63.76 tells that we need 64 (rounding up) students in each group (so 128 in total) in order to carry out this study with sufficient power. The other lines should be self-explanatory (well they should be by this stage; if you need me to tell you that the function is just returning the values that you’ve just typed in then you have bigger problems to worry about).\nThe pwr.t.test() function has six arguments. Two of them specify what sort of t-test you’ll be carrying out * type; which describes the type of t-test you will eventually be carrying out (one of two.sample, one.sample or paired), and * alternative; which describes the type of alternative hypothesis you want to test (one of two.sided, less or greater)\nThe other four arguments are what is used in the power analysis:\n\nd; this is the effect size, a single number calculated using Cohen’s d statistic.\nsig.level; this is the significance level\npower; is the power\nn; this is the number of observations per sample.\n\n\n\nWe do this with the TTestIndPower() function from statsmodels.stats.power:\n\nfrom statsmodels.stats.power import TTestIndPower\n\npwr = TTestIndPower()\n\npwr.solve_power(effect_size = 0.5,\n                            alpha = 0.05,\n                            power = 0.8)\n\n63.76561058785405\n\n\nThe output n = 63.76 tells that we need 64 (rounding up) students in each group (so 128 in total) in order to carry out this study with sufficient power.\nThe TTestIndPower() function uses the four arguments that are are used in the power analysis:\n\neffect_size; this is the effect size, a single number calculated using Cohen’s d statistic.\nalpha; this is the significance level (default is 0.05)\npower; is the power\nn; this is the number of observations per sample.\n\n\n\n\nThe function works by allowing you to specify any three of these four arguments and the function works out the fourth. In the example above we have used the test in the standard fashion by specifying power, significance and desired effect size and getting the function to tell us the necessary sample size.\n\n\n15.3.3 Calculating effect sizes\nWe can use the function to answer a different question:\n\nIf I know in advance that I can only observe 30 students per group, what is the effect size that I should be able to observe with 80% power at a 5% significance level?\n\nLet’s see how we do this:\n\nRPython\n\n\n\npwr.t.test(n = 30, sig.level = 0.05, power = 0.8,\n           type = \"two.sample\", alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 30\n              d = 0.7356292\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\n\npwr = TTestIndPower()\n\npwr.solve_power(\n    effect_size = None, # solving for d\n    nobs1 = 30,         # sample size per group\n    alpha = 0.05,\n    power = 0.80,\n    alternative = \"two-sided\"\n)\n\n0.7356198334313553\n\n\n\n\n\nWe get the effect size and we can see that an experiment with this many people would only be expected to detect a difference in means of d = 0.74 standard deviations. Is this good or bad? Well, it depends on the natural variation of your data; if your data is really noisy then it will have a large variation and a large standard deviation which will mean that 0.74 standard deviations might actually be quite a big difference between your groups. If on the other hand your data doesn’t vary very much, then 0.74 standard deviations might actually be a really small number and this test could pick up even quite small differences in mean.",
    "crumbs": [
      "CS6: Statistical power",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#power-analysis-on-data",
    "href": "materials/cs6_practical_power-analysis.html#power-analysis-on-data",
    "title": "15  Power analysis",
    "section": "15.4 Power analysis on data",
    "text": "15.4 Power analysis on data\nIn both of the previous two examples we were a little bit context-free in terms of effect size. Let’s look at how we can use a pilot study with real data to calculate effect sizes and perform a power analysis to inform a future study.\nLet’s look again at the fishlength data we saw in the first practical relating to the lengths of fish from two separate rivers. This is saved as data/CS1-twosample.csv.\n\nRPython\n\n\n\n# read in the data\nfishlength &lt;- read_csv(\"data/CS1-twosample.csv\")\n\n# visualise the data\nfishlength %&gt;% \n  ggplot(aes(x = river, y = length)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nFrom the plot we can see that the groups appear to have different means. This difference is significant, as per a two-sample t-test:\n\n# perform t-test\nt.test(length ~ river,\n       data = fishlength,\n       var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  length by river\nt = 3.8433, df = 66, p-value = 0.0002754\nalternative hypothesis: true difference in means between group Aripo and group Guanapo is not equal to 0\n95 percent confidence interval:\n 0.9774482 3.0909868\nsample estimates:\n  mean in group Aripo mean in group Guanapo \n             20.33077              18.29655 \n\n\n\n\n\n# read in the data\nfishlength_py = pd.read_csv(\"data/CS1-twosample.csv\")\n\n# visualise the data\np = (ggplot(fishlength_py, aes(x = \"river\", y = \"length\")) +\n     geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\nFrom the plot we can see that the groups appear to have different means. This difference is significant, as per a two-sample t-test.\nThe ttest() function in pingouin needs two vectors as input, so we split the data as follows:\n\naripo = fishlength_py.query('river == \"Aripo\"')[\"length\"]\nguanapo = fishlength_py.query('river == \"Guanapo\"')[\"length\"]\n\nNext, we perform the t-test. We specify that the variance are equal by setting correction = False. We also transpose() the data, so we can actually see the entire output.\n\npg.ttest(aripo, guanapo,\n         correction = False).transpose()\n\n                   T-test\nT                3.843267\ndof                    66\nalternative     two-sided\np-val            0.000275\nCI95%        [0.98, 3.09]\ncohen-d          0.942375\nBF10               92.191\npower            0.966135\n\n\n\n\n\nCan we use this information to design a more efficient experiment? One that we would be confident was powerful enough to pick up a difference in means as big as was observed in this study but with fewer observations?\nLet’s first work out exactly what the effect size of this previous study really was by estimating Cohen’s d using this data.\n\nRPython\n\n\nTo this, we use the cohens_d function from the rstatix package:\n\ncohens_d(length ~ river,\n         var.equal = TRUE,\n         data = fishlength)\n\n# A tibble: 1 × 7\n  .y.    group1 group2  effsize    n1    n2 magnitude\n* &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 length Aripo  Guanapo   0.942    39    29 large    \n\n\nThe cohens_d() function calculates the effect size using the formula of the test. The effsize column contains the information that we want, in this case 0.94 .\nWe can now actually answer our question and see how many fish we really need to catch in the future:\n\npwr.t.test(d = 0.94, power = 0.8, sig.level = 0.05,\n           type = \"two.sample\", alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 18.77618\n              d = 0.94\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nTo do this, we use the compute_effsize() function from pingouin. This takes two vectors as input, so we use the aripo and guanapo objects we created earlier:\n\npg.compute_effsize(aripo, guanapo,\n                   paired = False,\n                   eftype = \"cohen\")\n\nnp.float64(0.9423748389254938)\n\n\nNote: the compute_effsize() function is able to compute various effect sizes, but we’re specifying Cohen’s d here.\nSo, the Cohen’s d value for these data are d = 0.94 .\nWe can now actually answer our question and see how many fish we really need to catch in the future:\n\npwr = TTestIndPower()\n\npwr.solve_power(\n    effect_size = 0.94,\n    alpha = 0.05,\n    power = 0.80,\n    alternative = \"two-sided\"\n)\n\n18.776177324855\n\n\n\n\n\nFrom this we can see that any future experiments would really only need to use 19 fish for each group (we always round this number up, so no fish will be harmed during the experiment…) if we wanted to be confident of detecting the difference we observed in the previous study.\nThis approach can also be used when the pilot study showed a smaller effect size that wasn’t observed to be significant (indeed arguably, a pilot study shouldn’t really concern itself with significance but should only really be used as a way of assessing potential effect sizes which can then be used in a follow-up study).",
    "crumbs": [
      "CS6: Statistical power",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#linear-model-power-calculations",
    "href": "materials/cs6_practical_power-analysis.html#linear-model-power-calculations",
    "title": "15  Power analysis",
    "section": "15.5 Linear model power calculations",
    "text": "15.5 Linear model power calculations\nThankfully the ideas we’ve covered in the t-test section should see us in good stead going forward and I can stop writing everything out in such excruciating detail (I do have other things to do you know…).\nLet’s read in data/CS2-lobsters.csv. This data set was used in an earlier practical and describes the effect of three different food sources on lobster weight .\nAs a quick reminder we’ll also plot the data and perform an ANOVA:\n\nRPython\n\n\n\n# read in the data\nlobsters &lt;- read_csv(\"data/CS2-lobsters.csv\")\n\n\n# visualise the data\nggplot(lobsters,\n       aes(x = diet, y = weight)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n# define the linear model\nlm_lobster &lt;- lm(weight ~ diet,\n                 data = lobsters)\n\n# perform ANOVA on model\nanova(lm_lobster)\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\ndiet       2 1567.2  783.61  1.6432 0.2263\nResiduals 15 7153.1  476.87               \n\n\n\n\n\n# read in the data\nlobsters_py = pd.read_csv(\"data/CS2-lobsters.csv\")\n\n\n# visualise the data\np = (ggplot(lobsters_py, aes(x = \"diet\",\n                         y = \"weight\")) +\n     geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"weight ~ C(diet)\", data = lobsters_py)\n# and get the fitted parameters of the model\nlm_lobsters_py = model.fit()\n\n\n# perform the anova on the fitted model\nsm.stats.anova_lm(lm_lobsters_py)\n\n            df       sum_sq     mean_sq        F    PR(&gt;F)\nC(diet)    2.0  1567.229381  783.614690  1.64324  0.226313\nResidual  15.0  7153.075619  476.871708      NaN       NaN\n\n\n\n\n\n\nthe box plot shows us that there might well be some differences between groups\nthe ANOVA analysis though shows that there isn’t sufficient evidence to support that claim given the insignificant p-value we observe.\n\nSo the question we can ask is:\n\nIf there really is a difference between the different food sources as big as appears here, how big a sample would we need in order to be able to detect it statistically?\n\nFirst let’s calculate the observed effect size from this study.\n\nRPython\n\n\nFor linear models the effect size is called Cohen’s \\(f^2\\). We can calculate it easily by using the \\(R^2\\) value from the model fit and shoving it in the following formula:\n\\[\\begin{equation}\nf^2 = \\frac{R^2}{1-R^2}\n\\end{equation}\\]\nWe find \\(R^2\\) from the lm_lobster summary. We can either just look at the results (spoiler alert, the \\(R^2\\) is 0.1797) and add it manually or extract the value with the broom::glance() function.\nEither way, we can calculate Cohen’s \\(f^2\\):\n\n# get the effect size for ANOVA\nR2 &lt;- summary(lm_lobster) %&gt;%\n    glance() %&gt;% \n    pull(r.squared)\n\n# calculate Cohen's f2\nf2 &lt;- R2 / (1 - R2)\n\nf2\n\n[1] 0.2190987\n\n\nSo now we’ve got Cohen’s \\(f^2\\).\n\n\nFor linear models the effect size metric we use is called \\(\\eta^2\\), or eta-squared.\nThe eta-squared value measures the contribution of the individual model terms. This is closely linked to the \\(R^2\\) value, which measures the total amount of variation that is explained by the entire model.\nSince we only have one model term here (diet), the \\(R^2\\) and \\(\\eta^2\\) values are the same.\nWe can get the \\(R^2\\) (0.1797) value from the model as follows:\n\n# get the R2 value\nR2 = lm_lobsters_py.rsquared\n\n\n\n\n\nRPython\n\n\nThere’s one more thing that we need for the power calculation for a linear model; the degrees of freedom.\nWe have two different degrees of freedom: the numerator degrees of freedom and the denominator degrees of freedom. Here the numerator degrees of freedom is 2. This is the number that we want. It is simply the number of parameters in the model minus 1. In this model there are three parameters for the three groups, so 3 - 1 = 2 (see the math isn’t too bad). The other number is called the denominator degrees of freedom, which in this case is 15. This is actually the number we want the power analysis to calculate as it’s a proxy for the number of observations used in the model, and we’ll see how in a minute.\nThe degrees of freedom are mentioned at the bottom of the model summary:\n\nsummary(lm_lobster)\n\n\nCall:\nlm(formula = weight ~ diet, data = lobsters)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.129 -16.155  -4.279  15.195  46.720 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  114.433      8.915  12.836 1.71e-09 ***\ndietMussels   21.895     12.149   1.802   0.0916 .  \ndietPellets   14.047     13.223   1.062   0.3049    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.84 on 15 degrees of freedom\nMultiple R-squared:  0.1797,    Adjusted R-squared:  0.07035 \nF-statistic: 1.643 on 2 and 15 DF,  p-value: 0.2263\n\n\nSo, we now want to run a power analysis for this linear model, using the following information:\n\npower = 0.8\nsignificance = 0.05\neffect size = 0.219\nnumerator DF = 2\n\nWe can feed this into the pwr.f2.test() function, where we use\n\nu to represent the numerator DF value\nf2 to represent Cohen’s \\(f^2\\) effect size value\n\n\npwr.f2.test(u = 2, f2 = 0.219,\n            sig.level = 0.05, power = 0.8)\n\n\n     Multiple regression power calculation \n\n              u = 2\n              v = 44.12292\n             f2 = 0.219\n      sig.level = 0.05\n          power = 0.8\n\n\nAs before most of these numbers are just what you’ve put into the function yourself. The new number is v. This is the denominator degrees of freedom required for the analysis to have sufficient power. Thankfully this number is related to the number of observations that we should use in a straightforward manner:\n\\(number\\:of\\:observations = u + v + 1\\)\nSo in our case we would ideally have 48 observations (45 + 2 + 1, remembering to round up) in our experiment.\nThe most challenging part for using power analyses for linear models is working out what the numerator degrees of freedom should be. The easiest way of thinking about it is to say that it’s the number of parameters in your model, excluding the intercept. If you look back at how we wrote out the linear model equations, then you should be able to see how many non-zero parameters would be expected. For some of the simple cases the table below will help you, but for complex linear models you will need to write out the linear model equation and count parameters (sorry!).\n\n\n\n\n\n\n\nTest\nu\n\n\n\n\none-way ANOVA\nno. of groups - 1\n\n\nsimple linear regression\n1\n\n\ntwo-way ANOVA with interaction\nno. of groups (v1) x no. of groups (v2) - 1\n\n\ntwo-way ANOVA without interaction\nno. of groups (v1) + no. of groups (v2) - 2\n\n\nANCOVA with interaction\n2 x no. of groups – 1\n\n\nANCOVA without interaction\nno. of groups\n\n\n\n\n\nWe can use the eta-squared value in the power_anova() function from pingouin.\nIf we’re trying to figure out the sample size, we need to give it the following information:\n\neta_squared, the effect size we’re after (we saved this as R2)\nk, the number of groups (three, in our case)\npower, the statistical power we’re after, in this case 80%\nalpha, the significance threshold at which we want to detect it\n\n\npg.power_anova(eta_squared = R2, k = 3, power = 0.80, alpha = 0.05)\n\n15.701046156535305\n\n\nWhen we fill in all that information, then we find that we need 15.701 samples per group - rounding up this gives 16. This means that we need a total of \\(16 \\times 3 = 48\\) samples altogether.\n\n\n\nThere are two questions you might now ask (if you’re still following all of this that is – you’re quite possibly definitely in need of a coffee by now):\n\nhow many observations should go into each group?\n\nideally they should be equally distributed (so in this case 16 per group).\n\nwhy is this so complicated, why isn’t there just a single function that just does this, and just tells me how many observations I need?\n\nVery good question – I have no answer to that sorry – sometimes life is just hard.",
    "crumbs": [
      "CS6: Statistical power",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#r-squared-and-eta-squared",
    "href": "materials/cs6_practical_power-analysis.html#r-squared-and-eta-squared",
    "title": "15  Power analysis",
    "section": "15.6 R-squared and eta-squared",
    "text": "15.6 R-squared and eta-squared\nLike we’ve seen before in the previous sessions, the \\(R^2\\) value can give us an indication of how much of the variance is explained by our model.\nSometimes you also come across \\(\\eta^2\\). What that does is that it partitions \\(R^2\\) across the predictors. This means that \\(\\eta^2\\) represents how much variance is explained by each of the predictors. If you have multiple predictors, then you would get multiple values.\nIn the case where there is one predictor, \\(R^2 = \\eta^2\\).",
    "crumbs": [
      "CS6: Statistical power",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#exercises",
    "href": "materials/cs6_practical_power-analysis.html#exercises",
    "title": "15  Power analysis",
    "section": "15.7 Exercises",
    "text": "15.7 Exercises\n\n15.7.1 Power: one-sample\n\n\n\n\n\n\nExerciseExercise 1\n\n\n\n\n\n\nLevel: \nPerforming a power analysis on a one-sample data set\nLoad in data/CS1-onesample.csv (this is the same data we looked at in the earlier practical containing information on fish lengths from the Guanapo river).\n\nAssume this was a pilot study and analyse the data using a one-sample t-test to see if there is any evidence that the mean length of fish differs from 20 mm.\nUse the results of this analysis to estimate the effect size.\nWork out how big a sample size would be required to detect an effect this big with power 0.8 and significance 0.05.\nHow would the sample size change if we wanted 0.9 power and significance 0.01?\n\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\nIn Python statsmodels uses TTestPower for one-sample and paired tests.\n\n\n\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n15.8 Answer\n\nRPython\n\n\nFirst, read in the data:\n\nfish_data &lt;- read_csv(\"data/CS1-onesample.csv\")\n\nLet’s run the one-sample t-test as we did before:\n\nt.test(length ~ 1,\n       mu = 20,\n       alternative = \"two.sided\",\n       data = fish_data)\n\n\n    One Sample t-test\n\ndata:  length\nt = -3.5492, df = 28, p-value = 0.001387\nalternative hypothesis: true mean is not equal to 20\n95 percent confidence interval:\n 17.31341 19.27969\nsample estimates:\nmean of x \n 18.29655 \n\n\n\n\nFirst, read in the data:\n\nfish_data_py = pd.read_csv(\"data/CS1-onesample.csv\")\n\nLet’s run the one-sample t-test as we did before:\n\npg.ttest(x = fish_data_py.length,\n         y = 20,\n         alternative = \"two-sided\").transpose()\n\n                     T-test\nT                 -3.549184\ndof                      28\nalternative       two-sided\np-val              0.001387\nCI95%        [17.31, 19.28]\ncohen-d            0.659067\nBF10                 25.071\npower               0.92855\n\n\n\n\n\nThere does appear to be a statistically significant result here; the mean length of the fish appears to be different from 20 mm.\nLet’s calculate the effect size using these data. This gives us the following output for the effect size in terms of the Cohen’s d metric.\n\nRPython\n\n\n\ncohens_d(length ~ 1,\n         mu = 20,\n         data = fish_data)\n\n# A tibble: 1 × 6\n  .y.    group1 group2     effsize     n magnitude\n* &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt; &lt;ord&gt;    \n1 length 1      null model  -0.659    29 moderate \n\n\n\n\n\npg.compute_effsize(x = fish_data_py.length,\n                   y = 20,\n                   paired = False,\n                   eftype = \"cohen\")\n\nnp.float64(-0.6590669150482831)\n\n\n\n\n\nOur effect size is -0.66 which is a moderate effect size. This is pretty good and it means we might have been able to detect this effect with fewer samples.\n\n\n\n\n\n\nImportant\n\n\n\nAlthough the effect size here is negative, it does not matter in terms of the power calculations whether it’s negative or positive.\n\n\nSo, let’s do the power analysis to actually calculate the minimum sample size required:\n\nRPython\n\n\n\npwr.t.test(d = -0.6590669, sig.level = 0.05, power = 0.8,\n           type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 20.07483\n              d = 0.6590669\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n\nIn Python statsmodels uses TTestPower for one-sample and paired tests.\n\nfrom statsmodels.stats.power import TTestPower\n\npwr = TTestPower()\n\npwr.solve_power(\n    effect_size = abs(-0.6590669),  # use magnitude of d\n    alpha = 0.05,\n    power = 0.80,\n    alternative = \"two-sided\"\n)\n\n20.074833843960263\n\n\n\n\n\nWe would need 21 (you round up the n value) observations in our experimental protocol in order to be able to detect an effect size this big (small?) at a 5% significance level and 80% power. Let’s see what would happen if we wanted to be even more stringent and calculate this at a significance level of 1%:\n\nRPython\n\n\n\npwr.t.test(d = -0.6590669, sig.level = 0.01, power = 0.9,\n           type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 37.62974\n              d = 0.6590669\n      sig.level = 0.01\n          power = 0.9\n    alternative = two.sided\n\n\n\n\n\npwr = TTestPower()\n\npwr.solve_power(\n    effect_size = abs(-0.6590669),  # magnitude of d; sign is irrelevant\n    alpha = 0.01,\n    power = 0.80,\n    alternative = \"two-sided\"\n)\n\n30.254023023402883\n\n\n\n\n\nThen we’d need quite a few more observations! We would need to do a bit more work if we wanted to work to this level of significance and power. Are such small differences in fish length biologically meaningful?\n\n\n\n\n\n\n\n\n\n\n15.8.1 Power: two-sample paired\n\n\n\n\n\n\nExerciseExercise 2\n\n\n\n\n\n\nLevel: \nPower analysis on a paired two-sample data set\nLoad in data/CS1-twopaired.csv (again this is the same data that we used in an earlier practical and relates to cortisol levels measured on 20 participants in the morning and evening).\n\nfirst carry out a power analysis to work out how big of an effect size this experiment should be able to detect at a power of 0.8 and significance level of 0.05. Don’t look at the data just yet!\nNow calculate the actual observed effect size from the study.\nIf you were to repeat the study in the future, how many observations would be necessary to detect the observed effect with 80% power and significance level 0.01?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n15.9 Answer\n\nRPython\n\n\nFirst, read in the data:\n\ncortisol &lt;- read_csv(\"data/CS1-twopaired.csv\")\n\n\n\n\ncortisol_py = pd.read_csv(\"data/CS1-twopaired.csv\")\n\n\n\n\nWe have a paired data set with 20 pairs of observations, what sort of effect size could we detect at a significance level of 0.05 and power of 0.8?\n\nRPython\n\n\n\npwr.t.test(n = 20, sig.level = 0.05, power = 0.8,\n           type = \"paired\")\n\n\n     Paired t test power calculation \n\n              n = 20\n              d = 0.6604413\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\n\n\n\nfrom statsmodels.stats.power import TTestPower\n\npwr = TTestPower()\n\npwr.solve_power(\n    effect_size = None,     # solve for d\n    nobs = 20,              # paired → one-sample → nobs = number of pairs\n    alpha = 0.05,\n    power = 0.80,\n    alternative = \"two-sided\"\n)\n\n0.6604416544380205\n\n\n\n\n\nRemember that we get effect size measured in Cohen’s d metric. So here this experimental design would be able to detect a d value of 0.66, which is a medium to large effect size.\nNow let’s look at the actual data and work out what the effect size actually is.\n\nRPython\n\n\n\ncohens_d(cortisol ~ time,\n         paired = TRUE,\n         data = cortisol)\n\n# A tibble: 1 × 7\n  .y.      group1  group2  effsize    n1    n2 magnitude\n* &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 cortisol evening morning   -1.16    20    20 large    \n\n\n\n\nTo do this, we need reformat our data a bit:\n\ncortisol_wide_py = pd.pivot(cortisol_py, index = \"patient_id\", columns = \"time\", values = \"cortisol\")\n\ncortisol_wide_py.head()\n\ntime        evening  morning\npatient_id                  \n1             273.2    310.6\n2              65.7    146.1\n3             256.6    297.0\n4             321.0    270.9\n5              80.3    267.5\n\n\n\npg.compute_effsize(x = cortisol_wide_py.evening,\n                   y = cortisol_wide_py.morning,\n                   paired = True,\n                   eftype = \"cohen\")\n\nnp.float64(-1.434358623934538)\n\n\nAnnoyingly, this gives us a different effect size than we would have gotten in R. This is because pingouin.compute_effsize() does not use the standard deviation of the difference scores (between evening and morning), but instead uses the standard deviation of the pooled measurements - even when the option paired = True is set.\nHowever, we’re not interested in the overall difference, we’re interested in the differences on a patient-by-patient basis.\nSo, we need to calculate these differences and then calculate the effect size by hand:\n\ndiff = cortisol_wide_py[\"evening\"] - cortisol_wide_py[\"morning\"]\n\ndz = diff.mean() / diff.std(ddof = 1)\n\ndz\n\nnp.float64(-1.159018898796262)\n\n\nHere we have the following:\n\ndiff is the vector of within-person differences (e.g., evening − morning for each participant).\ndiff.mean() gives the average change across individuals.\ndiff.std(ddof = 1) gives the standard deviation of those changes, i.e., how much individuals vary in their change.\n\n\n\n\nThis value is a massive effect size. It’s quite likely that we actually have more participants in this study than we actually need given such a large effect. Let calculate how many individuals we would actually need:\n\nRPython\n\n\n\npwr.t.test(d = -1.159019, sig.level = 0.01, power = 0.8,\n           type = \"paired\")\n\n\n     Paired t test power calculation \n\n              n = 12.10628\n              d = 1.159019\n      sig.level = 0.01\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\n\n\n\nfrom statsmodels.stats.power import TTestPower\n\npwr = TTestPower()\n\npwr.solve_power(\n    effect_size = abs(-1.159019),   # magnitude of dz\n    alpha = 0.01,\n    power = 0.80,\n    alternative=\"two-sided\"\n)\n\n12.106256219386971\n\n\n\n\n\nSo we would have only needed 13 pairs of participants in this study given the size of effect we were trying to detect.\n\n\n\n\n\n\n\n\n\n\n15.9.1 Mussel muscles\n\n\n\n\n\n\nExerciseExercise 3\n\n\n\n\n\n\nLevel: \nIn this exercise we’re going to determine a required sample size, dependent on a calculated effect size. The file data/CS6-shelllength.csv contains information from a pilot study looking at whether the standardised length of the anterior adductor muscle scar in the mussel Mytilus trossulus differs across five locations around the world (well it might be of interest to someone…).\nFind the effect size from this study and perform a power calculation (at 0.8 and 0.05 significance level) to determine how many mussel muscles need to be recorded in order to be confident that an effect really exists.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n15.10 Answer\nLet’s first load in the data and have a look at them.\n\nRPython\n\n\n\nmussels &lt;- read_csv(\"data/CS6-shelllength.csv\")\n\n\nggplot(mussels,\n       aes(x = location,\n           y = length)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nmussels_py = pd.read_csv(\"data/CS6-shelllength.csv\")\n\n\np = (ggplot(mussels_py,\n        aes(x = \"location\",\n            y = \"length\")) +\n    geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nSo we are effectively looking at a one-way ANOVA with five groups. This will be useful to know later.\nNow we fit a linear model, and perform our calculations:\n\nRPython\n\n\n\n# define the model\nlm_mussels &lt;- lm(length ~ location,\n                data = mussels)\n\n# summarise the model\nsummary(lm_mussels)\n\n\nCall:\nlm(formula = length ~ location, data = mussels)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.025400 -0.007956  0.000100  0.007000  0.031757 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         0.078012   0.004454  17.517  &lt; 2e-16 ***\nlocationNewport    -0.003213   0.006298  -0.510  0.61331    \nlocationPetersburg  0.025430   0.006519   3.901  0.00043 ***\nlocationTillamook   0.002187   0.005975   0.366  0.71656    \nlocationTvarminne   0.017687   0.006803   2.600  0.01370 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0126 on 34 degrees of freedom\nMultiple R-squared:  0.4559,    Adjusted R-squared:  0.3918 \nF-statistic: 7.121 on 4 and 34 DF,  p-value: 0.0002812\n\n\nFrom this we can see that the R-squared value is 0.4559. We can extract that as follows and then use it to calculate Cohen’s \\(f^2\\):\n\n# get the effect size for ANOVA\nR2 &lt;- summary(lm_mussels) %&gt;%\n    glance() %&gt;% \n    pull(r.squared)\n\n# calculate Cohen's f2\nf2 &lt;- R2 / (1 - R2)\n\n\nf2\n\n[1] 0.837767\n\n\nNow, our model has 5 parameters (because we have 5 groups) and so the numerator degrees of freedom will be 4 \\((5 - 1 = 4)\\). This means that we can now carry our our power analysis:\n\npwr.f2.test(u = 4, f2 = 0.837767,\n            sig.level = 0.05 , power = 0.8)\n\n\n     Multiple regression power calculation \n\n              u = 4\n              v = 14.62396\n             f2 = 0.837767\n      sig.level = 0.05\n          power = 0.8\n\n\nThis tells us that the denominator degrees of freedom should be 15 (14.62 rounded up), and this means that we would only need 20 observations in total across all five groups to detect this effect size (Remember: number of observations = numerator d.f. + denominator d.f. + 1)\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"length ~ C(location)\", data = mussels_py)\n# and get the fitted parameters of the model\nlm_mussels_py = model.fit()\n\n\n# perform the anova on the fitted model\nsm.stats.anova_lm(lm_mussels_py)\n\n               df    sum_sq   mean_sq         F    PR(&gt;F)\nC(location)   4.0  0.004520  0.001130  7.121019  0.000281\nResidual     34.0  0.005395  0.000159       NaN       NaN\n\n\nSince we only have one model term here (location), the \\(R^2\\) and \\(\\eta^2\\) values are the same.\nWe can get the \\(R^2\\) (0.4559) value from the model as follows:\n\n# get the R2 value\nR2 = lm_mussels_py.rsquared\n\nWe can now calculate the number of required samples.\nWe use the following values:\n\neta_squared = R2\nk = 5 (we have five groups)\npower = 0.80\nalpha = 0.05 (our significance threshold)\n\n\npg.power_anova(eta_squared = R2,\n               k = 5, power = 0.80, alpha = 0.05)\n\n3.924794633956814\n\n\nThis means we need 4 samples per group, so 20 in total (\\(4 \\times 5\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.10.1 Power and effect\n\n\n\n\n\n\nExerciseExercise 4\n\n\n\n\n\n\nLevel: \nThe file /data/CS6-epilepsy1.csv contains information on the ages and rates of seizures of 236 patients undertaking a clinical trial.\n\nAnalyse the data using a linear model and calculate the effect size.\nIf there would be a relationship that large between age and seizure rate how big a study would be needed to observe the effect with a 90% power?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n15.11 Answer\nLet’s first load in the data and have a look at them.\n\nRPython\n\n\nLet’s load in the data:\n\nepilepsy &lt;- read_csv(\"data/CS6-epilepsy1.csv\")\n\nRows: 236 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): age, seizure\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s just have a quick look at the data to see what we’re dealing with:\n\nggplot(data = epilepsy,\n       mapping = aes(x = age,\n                     y = seizure)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nLet’s load in the data:\n\nepilepsy_py = pd.read_csv(\"data/CS6-epilepsy1.csv\")\n\nLet’s just have a quick look at the data to see what we’re dealing with:\n\np = (ggplot(epilepsy_py, aes(x = \"age\",\n                         y = \"seizure\")) +\n  geom_point())\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nSo we are effectively looking at a simple linear regression here.\nNow we fit a linear model and determine the number of samples for the observed effect size at 90% power:\n\nRPython\n\n\n\n# define the model\nlm_epilepsy &lt;- lm(seizure ~ age,\n                  data = epilepsy)\n\n# summarise the model\nsummary(lm_epilepsy)\n\n\nCall:\nlm(formula = seizure ~ age, data = epilepsy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.77513 -0.19585 -0.04333  0.22288  1.24168 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.814935   0.124857   6.527 4.12e-10 ***\nage         -0.001990   0.004303  -0.463    0.644    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.413 on 234 degrees of freedom\nMultiple R-squared:  0.0009134, Adjusted R-squared:  -0.003356 \nF-statistic: 0.2139 on 1 and 234 DF,  p-value: 0.6441\n\n\nFrom this we get that the \\(R^2\\) value is 9.134^{-4} (which is tiny!) and we can use this to calculate Cohen’s \\(f^2\\) value using the formula in the notes:\n\nR2 &lt;- lm_epilepsy %&gt;% \n    glance() %&gt;% \n    pull(r.squared)\n\nf2 &lt;- R2 / (1 - R2)\nf2\n\n[1] 0.0009142378\n\n\nThis effect size is absolutely tiny. If we really wanted to design an experiment to pick up an effect size this small then we would expect that we’ll need 1000s of participants.\nNow, our model has 2 parameters (an intercept and a slope) and so the numerator degrees of freedom (u) will be 1 (2 - 1 = 1!). This means that we can now carry our our power analysis:\n\npwr.f2.test(u = 1, f2 = f2,\n            sig.level = 0.05 , power = 0.9)\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 11493.01\n             f2 = 0.0009142378\n      sig.level = 0.05\n          power = 0.9\n\n\nThis tells us that the denominator degrees of freedom (v) should be 1.1494^{4} (rounded up to the nearest number), and this means that we would need 11496 participants to detect this effect size (Remember: number of observations = numerator d.f. (u) + denominator d.f. (v) + 1).\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"seizure ~ age\", data = epilepsy_py)\n# and get the fitted parameters of the model\nlm_epilepsy_py = model.fit()\n\nSince we only have one model term here (age), the \\(R^2\\) and \\(\\eta^2\\) values are the same.\nWe can get the \\(R^2\\) value from the model as follows:\n\n# get the R2 value\nR2 = lm_epilepsy_py.rsquared\n\nR2\n\nnp.float64(0.0009134027561266134)\n\n\nThis is a tiny effect size! In order to detect this, we would need a hell of a lot of samples.\nThere is no straightforward way (that I currently know of) to calculate sample sizes in Python for a linear model with a continuous predictor. We can use an external script that is based on the pwr.f2.test() function in R - this uses a couple of different metrics to calculate the sample size (or effect size, power or significance threshold - depending on which values are given).\nThis takes the following arguments:\n\nf2, Cohen’s \\(f^2\\) which is calculated as \\(f^2 = \\frac{R^2}{1-R^2}\\)\nu, numerator degrees of freedom. This is the number of parameters in the model minus 1\nv, denominator degrees of freedom. This is the number of observations - number of parameters\nsig_level, significance level\npower, desired power of the test\n\nFirst, we load the pwr_f2_test() function:\n\nfrom pwr_f2_test import *\n\nNext, we calculated f2:\n\nf2 = R2 / (1 - R2)\n\n\npwr_f2_test(u = 1, f2 = f2, sig_level = 0.05 , power = 0.9)\n\nPower analysis results:\n u is: 1\n v is: 11493.013062478689\n f2 is: 0.00091423782347433\n sig_level is: 0.05\n power is: 0.9\n num_obs is: 11496\n\n\nSo we see that the number of observations we need is 11496!\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.11.1 Study size with multiple regression\n\n\n\n\n\n\nExerciseExercise 5\n\n\n\n\n\n\nLevel: \nWe wish to test the effectiveness of a new drug against a placebo. It is thought that the sex and age of the patients may have an effect on their response.\n\nWrite down a linear model equation that might describe the relationship between these variables including all possible two-way interactions.\nHow big a study would we need to detect a medium effect size (according to Cohen, this is 0.15) at a power of 90%, with significance level 0.05?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n15.12 Answer\nHere we have a system with a single response variable, and three predictor variables. One of them is gender, a categorical predictor with two possible levels (M, F). One of them is treatment, again a categorical predictor with two possible levels (Drug, placebo) and one of them is continuous (age). The last one could even be viewed as a categorical predictor, where each year is a category. If we would like to model age as such, then we’d have to define it as a factor. We’re not doing this here.\nA linear model with all possible two-way interactions would look something like this:\nresponse ~ treatment + gender + age + treatment:gender + treatment:age + age:gender\nIn order to do a power calculation for this set up, we’ll need four things:\n\nthe effect size. Here we’re told it’s a medium effect size according to Cohen so we can use his default values. Here the value is 0.15, see table further above. Alternatively, we could have looked this up online (which may give us different values, or values that are relevant to a specific discipline).\nThe desired power. Here we’re told it’s 90%\nThe significance level to work to. Again we’re told this is going to be 0.05.\nThe numerator degrees of freedom. This is the tricky bit. We can do this by adding up the degrees of freedom for each term separately.\n\nThe numerator degrees of freedom is best calculated by working out the degrees of freedom of each of the six terms separately and then adding these up.\nThere are three simple ideas here that you need:\n\nThe degrees of freedom for a categorical variable is just the number of groups - 1\nThe degrees of freedom for a continuous variable is always 1\nthe degrees of freedom for any interaction is simple the product of the degrees of the main effects involved in the interaction.\n\nSo this means:\n\nThe df for gender is 1 (2 groups - 1)\nThe df for treatment is 1 (2 groups -1)\nThe df for age is 1 (continuous predictor)\nThe df for gender:treatment is 1 (1 x 1)\nThe df for gender:age is 1 (1 x 1)\nThe df for age:treatment is 1 (1 x 1)\n\nRather boring that all of them were 1 to be honest. Anyway, given that the denominator degrees of freedom is just the sum of all of these, we can see that \\(u = 6\\).\nWe now have all of the information to carry out the power analysis.\n\nRPython\n\n\n\npwr.f2.test(u = 6, f2 = 0.15,\n            sig.level = 0.05, power = 0.9)\n\n\n     Multiple regression power calculation \n\n              u = 6\n              v = 115.5826\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.9\n\n\n\n\n\npwr_f2_test(u = 6, f2 = 0.15,\n            sig_level = 0.05, power = 0.9)\n\nPower analysis results:\n u is: 6\n v is: 115.58256817005983\n f2 is: 0.15\n sig_level is: 0.05\n power is: 0.9\n num_obs is: 123\n\n\n\n\n\nWe get a denominator df of 116, which means that we would need at least 123 participants in our study (Remember: number of observations = numerator d.f. (u) + denominator d.f. (v) + 1). Given that we have four unique combinations of gender and treatment, it would be practically sensible to round this up to 124 participants so that we could have an equal number (31) in each combination of sex and treatment. It would also be sensible to aim for a similar distribution of age ranges in each group as well.",
    "crumbs": [
      "CS6: Statistical power",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#answer",
    "href": "materials/cs6_practical_power-analysis.html#answer",
    "title": "15  Power analysis",
    "section": "15.8 Answer",
    "text": "15.8 Answer\n\nRPython\n\n\nFirst, read in the data:\n\nfish_data &lt;- read_csv(\"data/CS1-onesample.csv\")\n\nLet’s run the one-sample t-test as we did before:\n\nt.test(length ~ 1,\n       mu = 20,\n       alternative = \"two.sided\",\n       data = fish_data)\n\n\n    One Sample t-test\n\ndata:  length\nt = -3.5492, df = 28, p-value = 0.001387\nalternative hypothesis: true mean is not equal to 20\n95 percent confidence interval:\n 17.31341 19.27969\nsample estimates:\nmean of x \n 18.29655 \n\n\n\n\nFirst, read in the data:\n\nfish_data_py = pd.read_csv(\"data/CS1-onesample.csv\")\n\nLet’s run the one-sample t-test as we did before:\n\npg.ttest(x = fish_data_py.length,\n         y = 20,\n         alternative = \"two-sided\").transpose()\n\n                     T-test\nT                 -3.549184\ndof                      28\nalternative       two-sided\np-val              0.001387\nCI95%        [17.31, 19.28]\ncohen-d            0.659067\nBF10                 25.071\npower               0.92855\n\n\n\n\n\nThere does appear to be a statistically significant result here; the mean length of the fish appears to be different from 20 mm.\nLet’s calculate the effect size using these data. This gives us the following output for the effect size in terms of the Cohen’s d metric.\n\nRPython\n\n\n\ncohens_d(length ~ 1,\n         mu = 20,\n         data = fish_data)\n\n# A tibble: 1 × 6\n  .y.    group1 group2     effsize     n magnitude\n* &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt; &lt;ord&gt;    \n1 length 1      null model  -0.659    29 moderate \n\n\n\n\n\npg.compute_effsize(x = fish_data_py.length,\n                   y = 20,\n                   paired = False,\n                   eftype = \"cohen\")\n\nnp.float64(-0.6590669150482831)\n\n\n\n\n\nOur effect size is -0.66 which is a moderate effect size. This is pretty good and it means we might have been able to detect this effect with fewer samples.\n\n\n\n\n\n\nImportant\n\n\n\nAlthough the effect size here is negative, it does not matter in terms of the power calculations whether it’s negative or positive.\n\n\nSo, let’s do the power analysis to actually calculate the minimum sample size required:\n\nRPython\n\n\n\npwr.t.test(d = -0.6590669, sig.level = 0.05, power = 0.8,\n           type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 20.07483\n              d = 0.6590669\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n\nIn Python statsmodels uses TTestPower for one-sample and paired tests.\n\nfrom statsmodels.stats.power import TTestPower\n\npwr = TTestPower()\n\npwr.solve_power(\n    effect_size = abs(-0.6590669),  # use magnitude of d\n    alpha = 0.05,\n    power = 0.80,\n    alternative = \"two-sided\"\n)\n\n20.074833843960263\n\n\n\n\n\nWe would need 21 (you round up the n value) observations in our experimental protocol in order to be able to detect an effect size this big (small?) at a 5% significance level and 80% power. Let’s see what would happen if we wanted to be even more stringent and calculate this at a significance level of 1%:\n\nRPython\n\n\n\npwr.t.test(d = -0.6590669, sig.level = 0.01, power = 0.9,\n           type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 37.62974\n              d = 0.6590669\n      sig.level = 0.01\n          power = 0.9\n    alternative = two.sided\n\n\n\n\n\npwr = TTestPower()\n\npwr.solve_power(\n    effect_size = abs(-0.6590669),  # magnitude of d; sign is irrelevant\n    alpha = 0.01,\n    power = 0.80,\n    alternative = \"two-sided\"\n)\n\n30.254023023402883\n\n\n\n\n\nThen we’d need quite a few more observations! We would need to do a bit more work if we wanted to work to this level of significance and power. Are such small differences in fish length biologically meaningful?",
    "crumbs": [
      "CS6: Statistical power",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#answer-1",
    "href": "materials/cs6_practical_power-analysis.html#answer-1",
    "title": "15  Power analysis",
    "section": "15.9 Answer",
    "text": "15.9 Answer\n\nRPython\n\n\nFirst, read in the data:\n\ncortisol &lt;- read_csv(\"data/CS1-twopaired.csv\")\n\n\n\n\ncortisol_py = pd.read_csv(\"data/CS1-twopaired.csv\")\n\n\n\n\nWe have a paired data set with 20 pairs of observations, what sort of effect size could we detect at a significance level of 0.05 and power of 0.8?\n\nRPython\n\n\n\npwr.t.test(n = 20, sig.level = 0.05, power = 0.8,\n           type = \"paired\")\n\n\n     Paired t test power calculation \n\n              n = 20\n              d = 0.6604413\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\n\n\n\nfrom statsmodels.stats.power import TTestPower\n\npwr = TTestPower()\n\npwr.solve_power(\n    effect_size = None,     # solve for d\n    nobs = 20,              # paired → one-sample → nobs = number of pairs\n    alpha = 0.05,\n    power = 0.80,\n    alternative = \"two-sided\"\n)\n\n0.6604416544380205\n\n\n\n\n\nRemember that we get effect size measured in Cohen’s d metric. So here this experimental design would be able to detect a d value of 0.66, which is a medium to large effect size.\nNow let’s look at the actual data and work out what the effect size actually is.\n\nRPython\n\n\n\ncohens_d(cortisol ~ time,\n         paired = TRUE,\n         data = cortisol)\n\n# A tibble: 1 × 7\n  .y.      group1  group2  effsize    n1    n2 magnitude\n* &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 cortisol evening morning   -1.16    20    20 large    \n\n\n\n\nTo do this, we need reformat our data a bit:\n\ncortisol_wide_py = pd.pivot(cortisol_py, index = \"patient_id\", columns = \"time\", values = \"cortisol\")\n\ncortisol_wide_py.head()\n\ntime        evening  morning\npatient_id                  \n1             273.2    310.6\n2              65.7    146.1\n3             256.6    297.0\n4             321.0    270.9\n5              80.3    267.5\n\n\n\npg.compute_effsize(x = cortisol_wide_py.evening,\n                   y = cortisol_wide_py.morning,\n                   paired = True,\n                   eftype = \"cohen\")\n\nnp.float64(-1.434358623934538)\n\n\nAnnoyingly, this gives us a different effect size than we would have gotten in R. This is because pingouin.compute_effsize() does not use the standard deviation of the difference scores (between evening and morning), but instead uses the standard deviation of the pooled measurements - even when the option paired = True is set.\nHowever, we’re not interested in the overall difference, we’re interested in the differences on a patient-by-patient basis.\nSo, we need to calculate these differences and then calculate the effect size by hand:\n\ndiff = cortisol_wide_py[\"evening\"] - cortisol_wide_py[\"morning\"]\n\ndz = diff.mean() / diff.std(ddof = 1)\n\ndz\n\nnp.float64(-1.159018898796262)\n\n\nHere we have the following:\n\ndiff is the vector of within-person differences (e.g., evening − morning for each participant).\ndiff.mean() gives the average change across individuals.\ndiff.std(ddof = 1) gives the standard deviation of those changes, i.e., how much individuals vary in their change.\n\n\n\n\nThis value is a massive effect size. It’s quite likely that we actually have more participants in this study than we actually need given such a large effect. Let calculate how many individuals we would actually need:\n\nRPython\n\n\n\npwr.t.test(d = -1.159019, sig.level = 0.01, power = 0.8,\n           type = \"paired\")\n\n\n     Paired t test power calculation \n\n              n = 12.10628\n              d = 1.159019\n      sig.level = 0.01\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\n\n\n\nfrom statsmodels.stats.power import TTestPower\n\npwr = TTestPower()\n\npwr.solve_power(\n    effect_size = abs(-1.159019),   # magnitude of dz\n    alpha = 0.01,\n    power = 0.80,\n    alternative=\"two-sided\"\n)\n\n12.106256219386971\n\n\n\n\n\nSo we would have only needed 13 pairs of participants in this study given the size of effect we were trying to detect.",
    "crumbs": [
      "CS6: Statistical power",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#answer-2",
    "href": "materials/cs6_practical_power-analysis.html#answer-2",
    "title": "15  Power analysis",
    "section": "15.10 Answer",
    "text": "15.10 Answer\nLet’s first load in the data and have a look at them.\n\nRPython\n\n\n\nmussels &lt;- read_csv(\"data/CS6-shelllength.csv\")\n\n\nggplot(mussels,\n       aes(x = location,\n           y = length)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nmussels_py = pd.read_csv(\"data/CS6-shelllength.csv\")\n\n\np = (ggplot(mussels_py,\n        aes(x = \"location\",\n            y = \"length\")) +\n    geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nSo we are effectively looking at a one-way ANOVA with five groups. This will be useful to know later.\nNow we fit a linear model, and perform our calculations:\n\nRPython\n\n\n\n# define the model\nlm_mussels &lt;- lm(length ~ location,\n                data = mussels)\n\n# summarise the model\nsummary(lm_mussels)\n\n\nCall:\nlm(formula = length ~ location, data = mussels)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.025400 -0.007956  0.000100  0.007000  0.031757 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         0.078012   0.004454  17.517  &lt; 2e-16 ***\nlocationNewport    -0.003213   0.006298  -0.510  0.61331    \nlocationPetersburg  0.025430   0.006519   3.901  0.00043 ***\nlocationTillamook   0.002187   0.005975   0.366  0.71656    \nlocationTvarminne   0.017687   0.006803   2.600  0.01370 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0126 on 34 degrees of freedom\nMultiple R-squared:  0.4559,    Adjusted R-squared:  0.3918 \nF-statistic: 7.121 on 4 and 34 DF,  p-value: 0.0002812\n\n\nFrom this we can see that the R-squared value is 0.4559. We can extract that as follows and then use it to calculate Cohen’s \\(f^2\\):\n\n# get the effect size for ANOVA\nR2 &lt;- summary(lm_mussels) %&gt;%\n    glance() %&gt;% \n    pull(r.squared)\n\n# calculate Cohen's f2\nf2 &lt;- R2 / (1 - R2)\n\n\nf2\n\n[1] 0.837767\n\n\nNow, our model has 5 parameters (because we have 5 groups) and so the numerator degrees of freedom will be 4 \\((5 - 1 = 4)\\). This means that we can now carry our our power analysis:\n\npwr.f2.test(u = 4, f2 = 0.837767,\n            sig.level = 0.05 , power = 0.8)\n\n\n     Multiple regression power calculation \n\n              u = 4\n              v = 14.62396\n             f2 = 0.837767\n      sig.level = 0.05\n          power = 0.8\n\n\nThis tells us that the denominator degrees of freedom should be 15 (14.62 rounded up), and this means that we would only need 20 observations in total across all five groups to detect this effect size (Remember: number of observations = numerator d.f. + denominator d.f. + 1)\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"length ~ C(location)\", data = mussels_py)\n# and get the fitted parameters of the model\nlm_mussels_py = model.fit()\n\n\n# perform the anova on the fitted model\nsm.stats.anova_lm(lm_mussels_py)\n\n               df    sum_sq   mean_sq         F    PR(&gt;F)\nC(location)   4.0  0.004520  0.001130  7.121019  0.000281\nResidual     34.0  0.005395  0.000159       NaN       NaN\n\n\nSince we only have one model term here (location), the \\(R^2\\) and \\(\\eta^2\\) values are the same.\nWe can get the \\(R^2\\) (0.4559) value from the model as follows:\n\n# get the R2 value\nR2 = lm_mussels_py.rsquared\n\nWe can now calculate the number of required samples.\nWe use the following values:\n\neta_squared = R2\nk = 5 (we have five groups)\npower = 0.80\nalpha = 0.05 (our significance threshold)\n\n\npg.power_anova(eta_squared = R2,\n               k = 5, power = 0.80, alpha = 0.05)\n\n3.924794633956814\n\n\nThis means we need 4 samples per group, so 20 in total (\\(4 \\times 5\\)).",
    "crumbs": [
      "CS6: Statistical power",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#answer-3",
    "href": "materials/cs6_practical_power-analysis.html#answer-3",
    "title": "15  Power analysis",
    "section": "15.11 Answer",
    "text": "15.11 Answer\nLet’s first load in the data and have a look at them.\n\nRPython\n\n\nLet’s load in the data:\n\nepilepsy &lt;- read_csv(\"data/CS6-epilepsy1.csv\")\n\nRows: 236 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): age, seizure\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s just have a quick look at the data to see what we’re dealing with:\n\nggplot(data = epilepsy,\n       mapping = aes(x = age,\n                     y = seizure)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nLet’s load in the data:\n\nepilepsy_py = pd.read_csv(\"data/CS6-epilepsy1.csv\")\n\nLet’s just have a quick look at the data to see what we’re dealing with:\n\np = (ggplot(epilepsy_py, aes(x = \"age\",\n                         y = \"seizure\")) +\n  geom_point())\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nSo we are effectively looking at a simple linear regression here.\nNow we fit a linear model and determine the number of samples for the observed effect size at 90% power:\n\nRPython\n\n\n\n# define the model\nlm_epilepsy &lt;- lm(seizure ~ age,\n                  data = epilepsy)\n\n# summarise the model\nsummary(lm_epilepsy)\n\n\nCall:\nlm(formula = seizure ~ age, data = epilepsy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.77513 -0.19585 -0.04333  0.22288  1.24168 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.814935   0.124857   6.527 4.12e-10 ***\nage         -0.001990   0.004303  -0.463    0.644    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.413 on 234 degrees of freedom\nMultiple R-squared:  0.0009134, Adjusted R-squared:  -0.003356 \nF-statistic: 0.2139 on 1 and 234 DF,  p-value: 0.6441\n\n\nFrom this we get that the \\(R^2\\) value is 9.134^{-4} (which is tiny!) and we can use this to calculate Cohen’s \\(f^2\\) value using the formula in the notes:\n\nR2 &lt;- lm_epilepsy %&gt;% \n    glance() %&gt;% \n    pull(r.squared)\n\nf2 &lt;- R2 / (1 - R2)\nf2\n\n[1] 0.0009142378\n\n\nThis effect size is absolutely tiny. If we really wanted to design an experiment to pick up an effect size this small then we would expect that we’ll need 1000s of participants.\nNow, our model has 2 parameters (an intercept and a slope) and so the numerator degrees of freedom (u) will be 1 (2 - 1 = 1!). This means that we can now carry our our power analysis:\n\npwr.f2.test(u = 1, f2 = f2,\n            sig.level = 0.05 , power = 0.9)\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 11493.01\n             f2 = 0.0009142378\n      sig.level = 0.05\n          power = 0.9\n\n\nThis tells us that the denominator degrees of freedom (v) should be 1.1494^{4} (rounded up to the nearest number), and this means that we would need 11496 participants to detect this effect size (Remember: number of observations = numerator d.f. (u) + denominator d.f. (v) + 1).\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"seizure ~ age\", data = epilepsy_py)\n# and get the fitted parameters of the model\nlm_epilepsy_py = model.fit()\n\nSince we only have one model term here (age), the \\(R^2\\) and \\(\\eta^2\\) values are the same.\nWe can get the \\(R^2\\) value from the model as follows:\n\n# get the R2 value\nR2 = lm_epilepsy_py.rsquared\n\nR2\n\nnp.float64(0.0009134027561266134)\n\n\nThis is a tiny effect size! In order to detect this, we would need a hell of a lot of samples.\nThere is no straightforward way (that I currently know of) to calculate sample sizes in Python for a linear model with a continuous predictor. We can use an external script that is based on the pwr.f2.test() function in R - this uses a couple of different metrics to calculate the sample size (or effect size, power or significance threshold - depending on which values are given).\nThis takes the following arguments:\n\nf2, Cohen’s \\(f^2\\) which is calculated as \\(f^2 = \\frac{R^2}{1-R^2}\\)\nu, numerator degrees of freedom. This is the number of parameters in the model minus 1\nv, denominator degrees of freedom. This is the number of observations - number of parameters\nsig_level, significance level\npower, desired power of the test\n\nFirst, we load the pwr_f2_test() function:\n\nfrom pwr_f2_test import *\n\nNext, we calculated f2:\n\nf2 = R2 / (1 - R2)\n\n\npwr_f2_test(u = 1, f2 = f2, sig_level = 0.05 , power = 0.9)\n\nPower analysis results:\n u is: 1\n v is: 11493.013062478689\n f2 is: 0.00091423782347433\n sig_level is: 0.05\n power is: 0.9\n num_obs is: 11496\n\n\nSo we see that the number of observations we need is 11496!",
    "crumbs": [
      "CS6: Statistical power",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#answer-4",
    "href": "materials/cs6_practical_power-analysis.html#answer-4",
    "title": "15  Power analysis",
    "section": "15.12 Answer",
    "text": "15.12 Answer\nHere we have a system with a single response variable, and three predictor variables. One of them is gender, a categorical predictor with two possible levels (M, F). One of them is treatment, again a categorical predictor with two possible levels (Drug, placebo) and one of them is continuous (age). The last one could even be viewed as a categorical predictor, where each year is a category. If we would like to model age as such, then we’d have to define it as a factor. We’re not doing this here.\nA linear model with all possible two-way interactions would look something like this:\nresponse ~ treatment + gender + age + treatment:gender + treatment:age + age:gender\nIn order to do a power calculation for this set up, we’ll need four things:\n\nthe effect size. Here we’re told it’s a medium effect size according to Cohen so we can use his default values. Here the value is 0.15, see table further above. Alternatively, we could have looked this up online (which may give us different values, or values that are relevant to a specific discipline).\nThe desired power. Here we’re told it’s 90%\nThe significance level to work to. Again we’re told this is going to be 0.05.\nThe numerator degrees of freedom. This is the tricky bit. We can do this by adding up the degrees of freedom for each term separately.\n\nThe numerator degrees of freedom is best calculated by working out the degrees of freedom of each of the six terms separately and then adding these up.\nThere are three simple ideas here that you need:\n\nThe degrees of freedom for a categorical variable is just the number of groups - 1\nThe degrees of freedom for a continuous variable is always 1\nthe degrees of freedom for any interaction is simple the product of the degrees of the main effects involved in the interaction.\n\nSo this means:\n\nThe df for gender is 1 (2 groups - 1)\nThe df for treatment is 1 (2 groups -1)\nThe df for age is 1 (continuous predictor)\nThe df for gender:treatment is 1 (1 x 1)\nThe df for gender:age is 1 (1 x 1)\nThe df for age:treatment is 1 (1 x 1)\n\nRather boring that all of them were 1 to be honest. Anyway, given that the denominator degrees of freedom is just the sum of all of these, we can see that \\(u = 6\\).\nWe now have all of the information to carry out the power analysis.\n\nRPython\n\n\n\npwr.f2.test(u = 6, f2 = 0.15,\n            sig.level = 0.05, power = 0.9)\n\n\n     Multiple regression power calculation \n\n              u = 6\n              v = 115.5826\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.9\n\n\n\n\n\npwr_f2_test(u = 6, f2 = 0.15,\n            sig_level = 0.05, power = 0.9)\n\nPower analysis results:\n u is: 6\n v is: 115.58256817005983\n f2 is: 0.15\n sig_level is: 0.05\n power is: 0.9\n num_obs is: 123\n\n\n\n\n\nWe get a denominator df of 116, which means that we would need at least 123 participants in our study (Remember: number of observations = numerator d.f. (u) + denominator d.f. (v) + 1). Given that we have four unique combinations of gender and treatment, it would be practically sensible to round this up to 124 participants so that we could have an equal number (31) in each combination of sex and treatment. It would also be sensible to aim for a similar distribution of age ranges in each group as well.",
    "crumbs": [
      "CS6: Statistical power",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#summary",
    "href": "materials/cs6_practical_power-analysis.html#summary",
    "title": "15  Power analysis",
    "section": "15.13 Summary",
    "text": "15.13 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nPower is the capacity of a test to detect significant results and is affected by\n\nthe effect size\nsample size\nthe significance level\n\nPower analysis optimises the trade-off between power, significance level and the desired effect size that we would like to detect",
    "crumbs": [
      "CS6: Statistical power",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  }
]